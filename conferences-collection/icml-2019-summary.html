<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="ICML-2019 accepted paper list已经放出来一个月了，知乎上有关ICML-2019的问题下还一片火热，让周末摸鱼的我感受到了强烈的peer pressure，开始后知地后觉滚去公司刷paper 扫了一眼accepted list，感觉今年很多文章都很有趣，值得深入研究一下">
<meta property="og:type" content="website">
<meta property="og:title" content="ICML-2019 论文整理">
<meta property="og:url" content="http://yoursite.com/conferences-collection/icml-2019-summary.html">
<meta property="og:site_name" content="Chen Shawn&#39;s Blogs">
<meta property="og:description" content="ICML-2019 accepted paper list已经放出来一个月了，知乎上有关ICML-2019的问题下还一片火热，让周末摸鱼的我感受到了强烈的peer pressure，开始后知地后觉滚去公司刷paper 扫了一眼accepted list，感觉今年很多文章都很有趣，值得深入研究一下">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/conferences-collection/icml-2019-summary/off-policy-1.png">
<meta property="og:image" content="http://yoursite.com/conferences-collection/icml-2019-summary/off-policy-2.png">
<meta property="og:image" content="http://yoursite.com/conferences-collection/icml-2019-summary/off-policy-3.png">
<meta property="og:image" content="http://yoursite.com/conferences-collection/icml-2019-summary/action-robust.png">
<meta property="og:image" content="http://yoursite.com/conferences-collection/icml-2019-summary/drsac.png">
<meta property="article:published_time" content="2019-06-23T05:57:59.000Z">
<meta property="article:modified_time" content="2020-04-19T07:46:27.031Z">
<meta property="article:author" content="Chen Shawn">
<meta property="article:tag" content="Paper reading">
<meta property="article:tag" content="Research">
<meta property="article:tag" content="Resource collection">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/conferences-collection/icml-2019-summary/off-policy-1.png">

<link rel="canonical" href="http://yoursite.com/conferences-collection/icml-2019-summary">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>ICML-2019 论文整理 | Chen Shawn's Blogs
</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Chen Shawn's Blogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">╭(●｀∀´●)╯ ╰(●’◡’●)╮</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
        <li class="menu-item menu-item-conferences">

    <a href="/conferences-collection" rel="section"><i class="fa fa-sitemap fa-fw"></i>Conferences</a>

  </li>
        <li class="menu-item menu-item-hello-worlds">

    <a href="/hello-world" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Hello Worlds</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/ChenShawn" class="github-corner" title="Read some fucking stupid codes here..." aria-label="Read some fucking stupid codes here..." rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="en">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">ICML-2019 论文整理
</h1>

<div class="post-meta">
  

</div>

</header>

      
      
      
      <div class="post-body">
          <p>ICML-2019 accepted paper list已经放出来一个月了，知乎上<a href="https://www.zhihu.com/question/321210381/answer/724436093" target="_blank" rel="noopener">有关ICML-2019的问题</a>下还一片火热，让周末摸鱼的我感受到了强烈的peer pressure，开始后知地后觉滚去公司刷paper</p>
<p>扫了一眼accepted list，感觉今年很多文章都很有趣，值得深入研究一下</p>
<a id="more"></a>
<h2 id="Machine-learning"><a href="#Machine-learning" class="headerlink" title="Machine learning"></a>Machine learning</h2><h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><h3 id="Off-Policy-Deep-Reinforcement-Learning-without-Exploration"><a href="#Off-Policy-Deep-Reinforcement-Learning-without-Exploration" class="headerlink" title="Off-Policy Deep Reinforcement Learning without Exploration"></a><a href="https://arxiv.org/abs/1812.02900" target="_blank" rel="noopener">Off-Policy Deep Reinforcement Learning without Exploration</a></h3><p>问题背景和之前看过的[Tencent AILab 那篇 imitation learning]的文章基本一致：你有一批<strong>固定的</strong>数据，可以用这一堆数据去进行off-policy的学习，但是训练过程中不允许有更多的exploration，也不允许收集更多的数据</p>
<p>这种场景在定价系统或推荐系统中较为常见，前者explore的代价太大，一不小心explore到很糟糕的地方就血本无归；而在大部分做推荐搜索的公司内部，万恶的PM是绝对不会允许你拿用户做实验的，万一一个explore不好就流失了一个用户；因此这类场合下的RL往往是先收集一波数据，然后拿这批固定的数据做off-policy的训练，这样在训练期线上只需跑一个安全且保守的策略即可</p>
<p>作者首先提出了一个很惊人的结论——在以上问题设定的大背景下，由于 <strong>extrapolation error</strong> 的存在，一些传统的RL算法，包括DQN、DDPG等，是不会收敛的</p>
<blockquote>
<p>In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting.</p>
</blockquote>
<p>根本原因在于，如此设定下的off-policy RL优化的MDP实际上是batch data中的数据所代表的MDP，而不是真实环境的MDP，这其中会产生一个不能被bound的误差</p>
<p>对于state s和acton a，我们首先定义这个误差函数 $\epsilon(s,a)$ 为off-policy收集到的数据数据所代表的MDP与真实环境MDP之间的误差，而 $\epsilon$ 在state和action下的积分为extrapolation error：</p>
<script type="math/tex; mode=display">
\epsilon(s,a)=Q^{\pi}(s,a)-Q^{\mathcal{B}}(s,a) \\
e_{MDP}=\int_{\mathcal{S}}d^{\pi}(s)\int_{\mathcal{A}}\pi(a|s)\epsilon(s,a)\ dads</script><p>把两个Q function用Bellman equation展开，可以发现$\epsilon(s,a)$也可以写成类似Bellman equation的递推式形式：</p>
<p><img src="./icml-2019-summary/off-policy-1.png" width="100%"></p>
<p>以加号为界将两部分分隔开，这个递推式由两部分组成，左边这部分的核心是 $p_{M}(s’|s,a)-p_{\mathcal{B}}(s’|s,a)$，可以理解为MDP的model bias，右半部分是一个Bellman operator。已知Bellman operator一定是contraction mapping，那么问题就在于前半部分的model bias是不是可以被bound</p>
<p>文章指出至少有三种情况下，这个model bias可以是非常大的，这里用自己的语言总结一下</p>
<ul>
<li><strong>Model bias:</strong> batch size比较小，那么batch中的数据对真实MDP的表达方差会非常大，实际上相当于在学一个non-stationary的MDP</li>
<li><strong>Absent data:</strong> 这点在连续state-action space的MDP中比较常见，连续空间中，用神经网络拟合从 $\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ 本质是用数据中已有的state和action学习到整个state-action space空间的插值，那么对于某个特定的state，如果target policy访问到这个state的概率很高，但由behavior policy收集到的数据中恰好缺少接近这个state的样本，那么对于这个state的value estimation error就会任意大</li>
<li><strong>Training mismatch:</strong> 即使数据量足够，由于从dataset中的采样是均匀的，如果behavior policy收集到的数据分布和target policy对应的数据分布不一样，那么误差也可以很大（<em>这是不是反过来又证明了TRPO/PPO类算法的先进性？</em>）</li>
</ul>
<p>作者在实验中对比了三种场景的off-policy训练</p>
<ol>
<li><strong>Final buffer:</strong> 从零开始训练一个DDPG，将这个DDPG从开始训练到收敛所有用到的样本收集起来训练off-policy DDPG</li>
<li><strong>Concurrent:</strong> 训练一个DDPG，将其训练数据同时feed给另一个完全异步同时启动的off-policy DDPG</li>
<li><strong>Imitation:</strong> 拿一个已经训练好的DDPG来跑expert demonstration给off-policy DDPG来训练</li>
</ol>
<p>实验结果发现1和2的off-policy DDPG比普通DDPG差很多，3则完全不收敛，且value出现了爆炸现象</p>
<blockquote>
<p>这个value爆炸的现象，我做实验也遇到过，当时以为是哪里写错把policy loss的梯度传到Q-function里面了，后来分析了一下，其实是因为DDPG训练的loss是一个正反馈系统，如果有某些访问概率很大的state的value估计很差的话，这个误差会随着Bellman递推越来越大。虽然提高起来蛮不可思议，但确实即使没有gradient指导，value也会出现爆炸的现象</p>
</blockquote>
<p><img src="./icml-2019-summary/off-policy-2.png" width="80%"></p>
<p>作者提出的解决方案是，对于上面的递推式，只要保证MDP是deterministic的，即可使得左半部分的model bias为0，作者还证明了对于任意state-action pair， $e_{MDP}=0$ 的充要条件是model-bias为0</p>
<p>deterministic MDP其实是个很强的条件，绝大多数RL问题都不满足，因此作者假设对于任意state-action pair，它与batch data中的数据相似度可以用一个conditional marginal likelihood $p_{\mathcal{B}}(a|s)$ 学出来。实际作者是用VAE来学的，这样就可以直接从这个分布中采样，采样后在所有action中选择value最大的那个action作为Q value的估计即可。可以想到随着训练迭代，VAE采样出的数据分布会asymptotically follow真实MDP的数据分布</p>
<p><em>这里还需要check作者前作 <a href="https://arxiv.org/abs/1802.09477" target="_blank" rel="noopener">Clipped Double Q learning</a>，还是有点复杂的</em></p>
<p><img src="./icml-2019-summary/off-policy-3.png" width="85%"></p>
<h3 id="Actor-Attention-Critic-for-Multi-Agent-Reinforcement-Learning"><a href="#Actor-Attention-Critic-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Actor-Attention-Critic for Multi-Agent Reinforcement Learning"></a><a href="https://arxiv.org/abs/1810.02912" target="_blank" rel="noopener">Actor-Attention-Critic for Multi-Agent Reinforcement Learning</a></h3><p>见<a href="https://chenshawn.github.io/2019/09/09/2019-fall-review/#toc-heading-6" target="_blank" rel="noopener">Multi-Agent问题汇总Actor-Attention-Critic</a></p>
<h3 id="Open-ended-Learning-in-Symmetric-Zero-sum-Games"><a href="#Open-ended-Learning-in-Symmetric-Zero-sum-Games" class="headerlink" title="Open-ended Learning in Symmetric Zero-sum Games"></a><a href="http://proceedings.mlr.press/v97/balduzzi19a/balduzzi19a.pdf" target="_blank" rel="noopener">Open-ended Learning in Symmetric Zero-sum Games</a></h3><p>见<a href="https://chenshawn.github.io/2019/09/09/2019-fall-review/#toc-heading-8" target="_blank" rel="noopener">Multi-Agent问题汇总#Actor-Attention-Critic</a></p>
<h3 id="Action-Robust-Reinforcement-Learning-and-Applications-in-Continuous-Control"><a href="#Action-Robust-Reinforcement-Learning-and-Applications-in-Continuous-Control" class="headerlink" title="Action Robust Reinforcement Learning and Applications in Continuous Control"></a><a href="https://arxiv.org/abs/1901.09184" target="_blank" rel="noopener">Action Robust Reinforcement Learning and Applications in Continuous Control</a></h3><p>如题，本文研究的是连续空间中的robust RL控制问题</p>
<p>在此之前，虽然已经有大量文献对robust MDP问题进行了充分的研究，但他们分析的场景的大多都是tabular case或者linear function approximator，这些研究结果很难拓展到非线性的连续state-action space中；<a href="https://arxiv.org/pdf/1703.02702.pdf" target="_blank" rel="noopener">2017年Pinto等人发表于ICML2017的文章</a>中，作者曾提出了一种基于对抗博弈优化RL模型robustness的方法，该方法得到了很好的实验结果，但其背后缺乏理论保障</p>
<p>本文中作者研究的主要是对于action空间的鲁棒性，文章以minimax对抗博弈作为基础，设定了两种博弈情况</p>
<ul>
<li>模型预测得到的action与实际执行的action不同，有一个adversary试图通过修改实际执行的action来让agent拿到尽可能低的reward，即 $a=\delta a^{+}+(1-\delta)a^{-}$，这种设定叫做NR-MDP</li>
<li>存在$\alpha$的概率使得模型与环境交互的时候，实际执行的是另一个完全不同的动作；否则执行模型的输出，这种设定叫做PR-MDP</li>
</ul>
<p>针对两种设定，本文在Bellman equation的层面上提出了统一的minimax鲁棒训练解决方案，算法分两步</p>
<ul>
<li>给定一个adversarial strategy，计算optimal counter strategy</li>
<li>基于当前的value做一步Bellman propagation</li>
</ul>
<p>文章还指出非soft的收敛速度会比non-soft得快，但soft面对错误更robust</p>
<p><img src="./icml-2019-summary/action-robust.png" width="98%"></p>
<p>实验中作者采用的是类似DDPG的形式</p>
<ul>
<li>PR-MDP: $\pi^{mix}(u|s;\theta,\bar{\theta})=(1-\alpha)(u-\mu_{\theta}(s))+\alpha(u-\bar{\mu_{\theta}}(s))$</li>
<li>NR_MDP: $\pi^{mix}(u|s;\theta,\bar{\theta})=u-[(1-\alpha)\mu_{\theta}(s)+\alpha\bar{\mu_{\theta}}(s)]$</li>
</ul>
<h3 id="Distributionally-Robust-Reinforcement-Learning"><a href="#Distributionally-Robust-Reinforcement-Learning" class="headerlink" title="Distributionally Robust Reinforcement Learning"></a><a href="https://arxiv.org/abs/1902.08708" target="_blank" rel="noopener">Distributionally Robust Reinforcement Learning</a></h3><p>本文出自今年ICML-2019的workshop RL4RealLife，主要解决的问题是agent探索过程中的safety问题，作者在MDP的层面提出了一种新的Bellman operator，并证明了该operator</p>
<ul>
<li>is a valid Bellman operator, i.e., a monotone $\ell_{\infty}$-norm $\gamma$ contraction on value functions</li>
<li>has polynomial convergence rate intead of exponential</li>
<li>is distributionally robust, i.e., has safety guarantee</li>
</ul>
<p>从intuition的层面来讲，本文的核心思想是这样的：做policy iteration时，对于当前agent的policy $\pi$，如果我们希望$\pi$在explore的时候可以避免探索可能会带来非常差value的state，我们需要使用一个在能够带来最差value的adversarial policy $\tilde{\pi}$来做policy evaluation，即估计value的值；同时在policy improving时让$\pi$沿着用$\tilde{\pi}$估计出的value的greedy方向前进</p>
<p>如何定义这个adversarial policy $\tilde{\pi}$呢？我们可以定义它在一个以$\pi$为球心的$\epsilon$-ball中；距离定义的话文中用的是KL divergence（KL作为metric不满足对称性，它的Hessian倒是可以作为一个合法的Riemannian metric tensor，不过本文中并没有严格地追求这一点上的严谨）</p>
<script type="math/tex; mode=display">
\tilde{\pi}=\arg\min_{D_{KL}(\pi,\tilde{\pi})\leq{\epsilon}}\mathbb{E}_{\tilde{\pi}}[R_{t}+\gamma V(s_{t+1})]</script><p>比较容易想到的一个问题在于，这种用worst case adversarial policy来做policy evaluation的思路会导致agent在短期内采取很保守的探索策略，每步都这样explore的话势必会影响收敛速度，因此作者又提出这种方法可以和SAC结合，因为SAC的设定中每步都是包含有entropy正则的，模型可以自己去学习长期的exploration entropy取舍问题</p>
<p>本文实验部分对比的主要baseline算法是SAC，实验环境为PyBullet（原RobotSchool环境，免费下载使用，不像MuJoCo需要licence），Figure 1中可以看到，虽然在average return指标上作者提出的方法对比SAC提升不是很显著，但从降低方差的层面来说，DRSAC提升现还是比较明显的</p>
<p><img src="./icml-2019-summary/drsac.png" width="98%"></p>
<p>由于本文是OpenReview公开审稿，审稿人#2给了一些建设性的意见：在更多的环境下做实验，也可以分析简单实验环境下agent的行为特点，这两点经验非常值得学习</p>
<h3 id="On-the-Generalization-Gap-in-Reparameterizable-Reinforcement-Learning"><a href="#On-the-Generalization-Gap-in-Reparameterizable-Reinforcement-Learning" class="headerlink" title="On the Generalization Gap in Reparameterizable Reinforcement Learning"></a><a href="https://arxiv.org/abs/1905.12654" target="_blank" rel="noopener">On the Generalization Gap in Reparameterizable Reinforcement Learning</a></h3><p>To the best of my knowledge, this is the first paper comprehensively discussing and analyzing the <strong>generalization issues</strong> of reinforcement learning from a theoretic approach.</p>
<p>作者指出，要对on-policy RL做具体的理论分析有两个方面的难点</p>
<ul>
<li>第一，训练过程中episode的distribution一直在随policy的变化而变化，这为finite sample analysis带来了难度</li>
<li>第二，当前的RL研究普遍混淆了两种不同意义的generalization，作者称这两种generalization gap实际上是有本质区别的<ul>
<li>由训练环境本身带来的randomness，例如很多RL模型无法在不同的random seed下得到consistent performance，对于这种generalization gap作者称之为intrinsic error，与supervised learning中的generalization意义是类似的</li>
<li>由于环境变化所带来的distribution shift，例如RARL中在默认MuJoCo环境下训练模型，在不同的relative mass下测试模型性能，得到模型缺乏鲁棒性的结论，作者将这种generalization gap称为external error，并认为这种error比较类似于transfer learning中的误差</li>
</ul>
</li>
</ul>

      </div>
      
      
      
    </div>
    

    
    
    


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Machine-learning"><span class="nav-number">1.</span> <span class="nav-text">Machine learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reinforcement-learning"><span class="nav-number">2.</span> <span class="nav-text">Reinforcement learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Off-Policy-Deep-Reinforcement-Learning-without-Exploration"><span class="nav-number">2.1.</span> <span class="nav-text">Off-Policy Deep Reinforcement Learning without Exploration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Actor-Attention-Critic-for-Multi-Agent-Reinforcement-Learning"><span class="nav-number">2.2.</span> <span class="nav-text">Actor-Attention-Critic for Multi-Agent Reinforcement Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Open-ended-Learning-in-Symmetric-Zero-sum-Games"><span class="nav-number">2.3.</span> <span class="nav-text">Open-ended Learning in Symmetric Zero-sum Games</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Action-Robust-Reinforcement-Learning-and-Applications-in-Continuous-Control"><span class="nav-number">2.4.</span> <span class="nav-text">Action Robust Reinforcement Learning and Applications in Continuous Control</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Distributionally-Robust-Reinforcement-Learning"><span class="nav-number">2.5.</span> <span class="nav-text">Distributionally Robust Reinforcement Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#On-the-Generalization-Gap-in-Reparameterizable-Reinforcement-Learning"><span class="nav-number">2.6.</span> <span class="nav-text">On the Generalization Gap in Reparameterizable Reinforcement Learning</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Chen Shawn"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Chen Shawn</p>
  <div class="site-description" itemprop="description">Daily life of eating, sleeping, and messing around</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ChenShawn" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ChenShawn" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:c.x.zhao@hotmail.com" title="E-Mail → mailto:c.x.zhao@hotmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/yourname" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/yukio-2" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;yukio-2" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Zhihu</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen Shawn</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.3" zIndex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'http//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":200,"height":400},"mobile":{"show":false},"react":{"opacity":1}});</script></body>
</html>
