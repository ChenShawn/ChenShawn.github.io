<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="今年RL的论文怎么这么多。。。 Key words">
<meta property="og:type" content="website">
<meta property="og:title" content="ICLR-2019论文整理">
<meta property="og:url" content="http://yoursite.com/conferences-collection/iclr-papers.html">
<meta property="og:site_name" content="Chen Shawn&#39;s Blogs">
<meta property="og:description" content="今年RL的论文怎么这么多。。。 Key words">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/conferences-collection/iclr-papers/paper1.png">
<meta property="article:published_time" content="2019-02-28T13:34:33.000Z">
<meta property="article:modified_time" content="2020-04-19T07:38:16.072Z">
<meta property="article:author" content="Chen Shawn">
<meta property="article:tag" content="Research">
<meta property="article:tag" content="Resource collection">
<meta property="article:tag" content="Paper reading">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/conferences-collection/iclr-papers/paper1.png">

<link rel="canonical" href="http://yoursite.com/conferences-collection/iclr-papers">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>ICLR-2019论文整理 | Chen Shawn's Blogs
</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Chen Shawn's Blogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">╭(●｀∀´●)╯ ╰(●’◡’●)╮</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
        <li class="menu-item menu-item-conferences">

    <a href="/conferences-collection" rel="section"><i class="fa fa-sitemap fa-fw"></i>Conferences</a>

  </li>
        <li class="menu-item menu-item-hello-worlds">

    <a href="/hello-world" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Hello Worlds</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/ChenShawn" class="github-corner" title="Read some fucking stupid codes here..." aria-label="Read some fucking stupid codes here..." rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="en">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">ICLR-2019论文整理
</h1>

<div class="post-meta">
  

</div>

</header>

      
      
      
      <div class="post-body">
          <p><del>今年RL的论文怎么这么多。。。</del></p>
<h2 id="Key-words"><a href="#Key-words" class="headerlink" title="Key words"></a>Key words</h2><a id="more"></a>
<ul>
<li>RL $\times$ Adersarial learning</li>
<li>Guided-policy search (Model-based RL)</li>
<li>GAIL (IRL)</li>
<li>Multi-agent RL</li>
<li>Soft Q-learning (PGM)</li>
</ul>
<h2 id="Adversarial-learning"><a href="#Adversarial-learning" class="headerlink" title="Adversarial learning"></a>Adversarial learning</h2><h6 id="Excessive-Invariance-Causes-Adversarial-Vulnerability"><a href="#Excessive-Invariance-Causes-Adversarial-Vulnerability" class="headerlink" title="Excessive Invariance Causes Adversarial Vulnerability"></a><a href="https://openreview.net/forum?id=BkfbpsAcF7" target="_blank" rel="noopener">Excessive Invariance Causes Adversarial Vulnerability</a></h6><p>只看标题觉得结论还是比较有趣的，先mark一下</p>
<h6 id="The-Limitations-of-Adversarial-Training-and-the-Blind-Spot-Attack"><a href="#The-Limitations-of-Adversarial-Training-and-the-Blind-Spot-Attack" class="headerlink" title="The Limitations of Adversarial Training and the Blind-Spot Attack"></a><a href="https://openreview.net/forum?id=HylTBhA5tQ" target="_blank" rel="noopener">The Limitations of Adversarial Training and the Blind-Spot Attack</a></h6><p>文章研究了adversarial training中存在的blind-spot现象，作者在abstract中如此解释blind-spot attack：</p>
<blockquote>
<p>Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the “blind-spot attack”, where the input images reside in “blind-spots” (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold.</p>
<p>The existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data.</p>
</blockquote>
<p>此外作者还发现了2018年Kolter &amp; Wong以及Sinha <em>et al.</em>的adversarial training方法都存在blind-spot问题</p>
<h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><h6 id="Variational-Discriminator-Bottleneck-Improving-Imitation-Learning-Inverse-RL-and-GANs-by-Constraining-Information-Flow"><a href="#Variational-Discriminator-Bottleneck-Improving-Imitation-Learning-Inverse-RL-and-GANs-by-Constraining-Information-Flow" class="headerlink" title="Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow"></a><a href="https://openreview.net/forum?id=HyxPx3R9tm" target="_blank" rel="noopener">Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow</a></h6><ul>
<li>UCB大佬集团出品，Pieter Abbeel和Sergey Levine都在作者名单里，</li>
<li>Variational information bottleneck并非首创，早在2016年<a href="https://arxiv.org/abs/1612.00410" target="_blank" rel="noopener">Alemi <em>et al.</em>的工作</a>就已经将其用于supervised learning任务，以此来reduce model variance；相比之下本文的创新在于将这种思路拓展至几个经典的需要使用discriminator/critic的场景：<ul>
<li>GAN生成图像，unsupervised learning，文章后面的实验中放了几张生成的$1024\times{}1024$的人脸大图</li>
<li>GAIL，第一篇为基于GAN做imitation learning提供理论基础的文章</li>
<li><a href="https://arxiv.org/abs/1710.11248" target="_blank" rel="noopener">AIRL</a>，将GAIL拓展至inverse RL的工作</li>
</ul>
</li>
<li>直接constraint mutual information的upper bound，训练时用了截断的dual gradient descent，此外实验一节也提出了若干trick (加GP等)，目测复现比较困难</li>
</ul>
<h6 id="Temporal-Difference-Variational-Auto-Encoder"><a href="#Temporal-Difference-Variational-Auto-Encoder" class="headerlink" title="Temporal Difference Variational Auto-Encoder"></a><a href="https://openreview.net/forum?id=S1x4ghC9tQ" target="_blank" rel="noopener">Temporal Difference Variational Auto-Encoder</a></h6><p>Motivation很有说服力，作者认为一个agent的experience modeling应该具有三个性质：</p>
<ul>
<li>model应当学习到MDP中state空间的抽象表示，而不仅仅局限于根据observation做判断</li>
<li>model应当学习belief state</li>
<li>mode应当能够学习到<strong>时域上的抽象</strong>，从而使得agent的planning可以更长远</li>
</ul>
<h6 id="Rigorous-Agent-Evaluation-An-Adversarial-Approach-to-Uncover-Catastrophic-Failures"><a href="#Rigorous-Agent-Evaluation-An-Adversarial-Approach-to-Uncover-Catastrophic-Failures" class="headerlink" title="Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures"></a><a href="https://openreview.net/forum?id=B1xhQhRcK7" target="_blank" rel="noopener">Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures</a></h6><p>选题很新潮，Motivation是在safety-critical场景下evaluate agent，审稿人的summary简洁而准确：</p>
<blockquote>
<p>Proposes an importance sampling approach to sampling failure cases for RL algorithms. The proposal distribution is based on a function learned via a neural network on failures that occur during agent training. The method is compared to random sampling on two problems where the “true” failure probability can be approximated through random sampling. The IS method requires substantially fewer samples to produce failure cases and to estimate the failure probability.</p>
</blockquote>
<p>此外也是用adversarial learning的思路来做RL，可以看出这种思路将成为一种大的趋势</p>
<h6 id="Woulda-Coulda-Shoulda-Counterfactually-Guided-Policy-Search"><a href="#Woulda-Coulda-Shoulda-Counterfactually-Guided-Policy-Search" class="headerlink" title="Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search"></a><a href="https://openreview.net/forum?id=BJG0voC9YQ" target="_blank" rel="noopener">Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search</a></h6><p>Guided policy search系列的工作，需要稍微查一下之前的文献才能看懂</p>
<h6 id="CEM-RL-Combining-evolutionary-and-gradient-based-methods-for-policy-search"><a href="#CEM-RL-Combining-evolutionary-and-gradient-based-methods-for-policy-search" class="headerlink" title="CEM-RL: Combining evolutionary and gradient-based methods for policy search"></a><a href="https://openreview.net/forum?id=BkeU5j0ctQ" target="_blank" rel="noopener">CEM-RL: Combining evolutionary and gradient-based methods for policy search</a></h6><blockquote>
<p>In this paper, we propose a different combination scheme using the simple cross-entropy<br>method (<strong>CEM</strong>) and Twin Delayed Deep Deterministic policy gradient (<strong>TD3</strong>), another off-policy deep RL algorithm which improves over DDPG.</p>
</blockquote>
<p>审稿人1的评论</p>
<blockquote>
<p>This paper combines two different types of existing optimization methods, CEM/CMA-ES and DDPG/TD3, for policy optimization. The approach resembles ERL but demonstrates good better performance on a variety of continuous control benchmarks.  Although I feel the novelty of the paper is limited, the provided promising results may justify the acceptance of the paper.</p>
</blockquote>
<h6 id="Directed-Info-GAIL-Learning-Hierarchical-Policies-from-Unsegmented-Demonstrations-using-Directed-Information"><a href="#Directed-Info-GAIL-Learning-Hierarchical-Policies-from-Unsegmented-Demonstrations-using-Directed-Information" class="headerlink" title="Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information"></a><a href="https://openreview.net/forum?id=BJeWUs05KQ" target="_blank" rel="noopener">Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information</a></h6><p>Imitation learning领域的新鲜血液，2017年NIPS上<a href="http://papers.nips.cc/paper/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations.pdf" target="_blank" rel="noopener">info-GAIL</a>的延伸</p>
<div align="center"><img src="./iclr-papers/paper1.png" width="90%"></div>

<ul>
<li>解决的问题：<ul>
<li>Learning a macro-policy from unsegmented expert demonstrations</li>
<li>Unsupervised inference of subtask-specific latent variables</li>
</ul>
</li>
<li>提出的方法<ul>
<li>将info-GAIL的PGM推广为sequence of latent variable的形式，优化latent variable与state-action pair的directed mutual information</li>
<li>采用Gumbel trick来解决latent variable离散的问题</li>
<li>用expert demonstrations预训练了一个VAE用于从latent variable的先验分布中采样</li>
</ul>
</li>
<li>作者分析了提出的方法与options framework的关联</li>
<li>实验中有若干离散latent variable可视化的例子，作者以此来证明他们方法中latent variable的interpretability，审稿人认为实验比较弱，没有大型continuous control任务上的evaluation</li>
<li>个人认为与infoGAN的inference相似，由于复杂任务的subtask表示本身具有某种程度的复杂性，这种inference很难在复杂任务上学习到对人类有意义的latent variable representation</li>
</ul>
<h6 id="Hindsight-policy-gradients"><a href="#Hindsight-policy-gradients" class="headerlink" title="Hindsight policy gradients"></a><a href="https://openreview.net/forum?id=Bkg2viA5FQ" target="_blank" rel="noopener">Hindsight policy gradients</a></h6><p><em>Jürgen Schmidhuber出现在了通讯作者的位置上，审稿人认为contribution有限，但AC力排众议给了这篇文章很高的评价</em></p>
<p>Metareview:</p>
<blockquote>
<p>The paper generalizes the concept of “hindsight”, i.e. the recycling of data from trajectories in a goal-based system based on the goal state actually achieved, to policy gradient methods.</p>
</blockquote>
<p>Review 1:</p>
<blockquote>
<p>The authors present HPG, which applies the hindsight formulation already applied to off-policy RL algorithms (hindsight experience replay, HER, Andrychowicz et al., 2017) to policy gradients.</p>
</blockquote>
<h6 id="Probabilistic-Planning-with-Sequential-Monte-Carlo-methods"><a href="#Probabilistic-Planning-with-Sequential-Monte-Carlo-methods" class="headerlink" title="Probabilistic Planning with Sequential Monte Carlo methods"></a><a href="https://openreview.net/forum?id=ByetGn0cYX" target="_blank" rel="noopener">Probabilistic Planning with Sequential Monte Carlo methods</a></h6><blockquote>
<p>Sequential Monte Carlo (SMC) has since its inception some 25 years ago proved to be a powerful and generally applicable tool. The authors of this paper continue this development in a very interesting and natural way by showing how SMC can be used to solve challenging planning problems. This is a enabled by reformulating the planning problem as an inference problem via the recent trend referred to as “control as inference”. </p>
</blockquote>
<h6 id="Learning-to-Understand-Goal-Specifications-by-Modelling-Reward"><a href="#Learning-to-Understand-Goal-Specifications-by-Modelling-Reward" class="headerlink" title="Learning to Understand Goal Specifications by Modelling Reward"></a><a href="https://openreview.net/forum?id=H1xsSjC9Ym" target="_blank" rel="noopener">Learning to Understand Goal Specifications by Modelling Reward</a></h6><p>文章讨论了NLP任务中的RL应用，当reward没有良好定义的时候，如何用一个discriminator D来生成pseudo rewards。感觉motivation蛮有趣的，可以一读。</p>
<h6 id="Adversarial-Imitation-via-Variational-Inverse-Reinforcement-Learning"><a href="#Adversarial-Imitation-via-Variational-Inverse-Reinforcement-Learning" class="headerlink" title="Adversarial Imitation via Variational Inverse Reinforcement Learning"></a><a href="https://openreview.net/forum?id=HJlmHoR5tQ" target="_blank" rel="noopener">Adversarial Imitation via Variational Inverse Reinforcement Learning</a></h6><p>Metareview给了这篇文章很高的评价</p>
<p>作者在abstract中claim的contribution：</p>
<blockquote>
<p>Our proposed method builds on the framework of generative adversarial networks and introduces the empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies. Empowerment-based regularization prevents the policy from overfitting to expert demonstrations, which advantageously leads to more generalized behaviors that result in learning near-optimal rewards.</p>
</blockquote>
<p>Reviewer在评论区claim的contribution：</p>
<blockquote>
<p>This paper builds on the AIRL framework (Fu et al., 2017) by combining the empowerment maximization objective for optimizing both the policy and reward function. Algorithmically, the main difference is that this introduces the need to optimize a inverse model (q), an empowerment function (Phi) and alters the AIRL updates to the reward function and policy. This paper presents experiments on the original set of AIRL tasks, and shows improved performance on some tasks.</p>
</blockquote>
<h6 id="The-Laplacian-in-RL-Learning-Representations-with-Efficient-Approximations"><a href="#The-Laplacian-in-RL-Learning-Representations-with-Efficient-Approximations" class="headerlink" title="The Laplacian in RL: Learning Representations with Efficient Approximations"></a><a href="https://openreview.net/forum?id=HJlNpoA5YQ" target="_blank" rel="noopener">The Laplacian in RL: Learning Representations with Efficient Approximations</a></h6><p>Abstract非常吸引我，感觉讨论的问题很有趣，且和我之前的工作似乎有一点点的关联，需要仔细研究下</p>
<blockquote>
<p>The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning.</p>
</blockquote>
<p>所有审稿人中，审稿人2起初的意见相对比较negative，质疑主要围绕两点展开:</p>
<ul>
<li>作者只使用random policy来学习state representation，这在比较大的MDP上显然不能对state space进行有效的探索</li>
<li>作者在文中指出，Laplacian representation的一个应用在于reward-shaping，但审稿人对sample efficiency提出了质疑，认为文章中的实验并没有公正地反映出sample efficiency</li>
</ul>
<p>经过rebuttal审稿人接受了作者的说法</p>
<h6 id="Marginal-Policy-Gradients-A-Unified-Family-of-Estimators-for-Bounded-Action-Spaces-with-Applications"><a href="#Marginal-Policy-Gradients-A-Unified-Family-of-Estimators-for-Bounded-Action-Spaces-with-Applications" class="headerlink" title="Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications"></a><a href="https://openreview.net/forum?id=HkgqFiAcFm" target="_blank" rel="noopener">Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications</a></h6><blockquote>
<p>With the marginal policy gradients family of estimators we present a unified analysis of the variance reduction properties of APG and CAPG; our results provide a stronger guarantee than existing analyses for CAPG.</p>
</blockquote>
<p>又是来自Tencent AILab大佬Han Liu组的paper，研究的是在RTS游戏背景下的policy gradient方差问题，RTS游戏中action space往往由几个连续的变量来表示，连续变量的值往往表示英雄移动的方向或施放技能的方向等。此前RL方面还没有类似的研究RTS特定背景的的工作，值得一读。</p>
<h6 id="Soft-Q-Learning-with-Mutual-Information-Regularization"><a href="#Soft-Q-Learning-with-Mutual-Information-Regularization" class="headerlink" title="Soft Q-Learning with Mutual-Information Regularization"></a><a href="https://openreview.net/forum?id=HyEtjoCqFX" target="_blank" rel="noopener">Soft Q-Learning with Mutual-Information Regularization</a></h6><p>Soft Q-learning的思路来源于概率图模型，之前的研究表明soft Q-learning中的entropy regularizer可以很有效地提高exploration的效率与policy的robustness，因此被广泛应用在很多RL任务中。而根据作者在abstract中的claim：</p>
<blockquote>
<p>However, entropy regularization might be undesirable when actions have significantly different importance.</p>
<p>We propose a theoretically motivated framework that dynamically weights the importance of actions by using the mutual-information.</p>
<p>This regularizer encourages the policy to be close to a non-uniform distribution that assigns higher probability mass to more important actions.</p>
</blockquote>
<p>文章的formulation看起来还是很漂亮的，可以仔细研究下。</p>
<h6 id="Deep-reinforcement-learning-with-relational-inductive-biases"><a href="#Deep-reinforcement-learning-with-relational-inductive-biases" class="headerlink" title="Deep reinforcement learning with relational inductive biases"></a><a href="https://openreview.net/forum?id=HkxaFoC9KQ" target="_blank" rel="noopener">Deep reinforcement learning with relational inductive biases</a></h6><p>划重点，文章在星际2的几个小型任务上刷到了SOTA</p>
<blockquote>
<p>We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene.</p>
</blockquote>
<h6 id="Preferences-Implicit-in-the-State-of-the-World"><a href="#Preferences-Implicit-in-the-State-of-the-World" class="headerlink" title="Preferences Implicit in the State of the World"></a><a href="https://openreview.net/forum?id=rkevMnRqYQ" target="_blank" rel="noopener">Preferences Implicit in the State of the World</a></h6><p>Pieter Abbeel组的第二篇文章，文章的主旨可以用一句话概括：</p>
<blockquote>
<p>Inferring preferences from the initial state of an environment.</p>
</blockquote>
<p>文章中放出的代码地址<a href="https://github.com/HumanCompatibleAI/rlsp" target="_blank" rel="noopener">https://github.com/HumanCompatibleAI/rlsp</a></p>
<h2 id="Multi-agent-RL"><a href="#Multi-agent-RL" class="headerlink" title="Multi-agent RL"></a>Multi-agent RL</h2><h6 id="M-3RL-Mind-aware-Multi-agent-Management-Reinforcement-Learning"><a href="#M-3RL-Mind-aware-Multi-agent-Management-Reinforcement-Learning" class="headerlink" title="M^3RL: Mind-aware Multi-agent Management Reinforcement Learning"></a><a href="https://openreview.net/forum?id=BkzeUiRcY7" target="_blank" rel="noopener">M^3RL: Mind-aware Multi-agent Management Reinforcement Learning</a></h6><p>Multi-agent的工作，mark一下，有时间可以仔细看看</p>
<h6 id="Learning-to-Schedule-Communication-in-Multi-agent-Reinforcement-Learning"><a href="#Learning-to-Schedule-Communication-in-Multi-agent-Reinforcement-Learning" class="headerlink" title="Learning to Schedule Communication in Multi-agent Reinforcement Learning"></a><a href="https://openreview.net/forum?id=SJxu5iR9KQ" target="_blank" rel="noopener">Learning to Schedule Communication in Multi-agent Reinforcement Learning</a></h6><blockquote>
<p>In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards.</p>
</blockquote>
<h6 id="Multi-Agent-Dual-Learning"><a href="#Multi-Agent-Dual-Learning" class="headerlink" title="Multi-Agent Dual Learning"></a><a href="https://openreview.net/forum?id=HyGhN2A5tm" target="_blank" rel="noopener">Multi-Agent Dual Learning</a></h6><p>将dual learning的思路与multi-agent结合，传统的dual learning一般是两个模型互相利用对方的对偶性质来进行学习，这篇文章将idea拓展至multi-agent环境下的多个目标之间的互相交互，并且在machine translation任务上刷到了SOTA。</p>
<p>Metareview:</p>
<blockquote>
<p>A paper that studies two tasks: machine translation and image translation. The authors propose a new multi-agent dual learning technique that takes advantage of the symmetry of the problem. The empirical gains over a competitive baseline are quite solid.</p>
</blockquote>
<h2 id="Deep-learning"><a href="#Deep-learning" class="headerlink" title="Deep learning"></a>Deep learning</h2>
      </div>
      
      
      
    </div>
    

    
    
    


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Key-words"><span class="nav-number">1.</span> <span class="nav-text">Key words</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adversarial-learning"><span class="nav-number">2.</span> <span class="nav-text">Adversarial learning</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Excessive-Invariance-Causes-Adversarial-Vulnerability"><span class="nav-number">2.0.0.0.1.</span> <span class="nav-text">Excessive Invariance Causes Adversarial Vulnerability</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#The-Limitations-of-Adversarial-Training-and-the-Blind-Spot-Attack"><span class="nav-number">2.0.0.0.2.</span> <span class="nav-text">The Limitations of Adversarial Training and the Blind-Spot Attack</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reinforcement-learning"><span class="nav-number">3.</span> <span class="nav-text">Reinforcement learning</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Variational-Discriminator-Bottleneck-Improving-Imitation-Learning-Inverse-RL-and-GANs-by-Constraining-Information-Flow"><span class="nav-number">3.0.0.0.1.</span> <span class="nav-text">Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Temporal-Difference-Variational-Auto-Encoder"><span class="nav-number">3.0.0.0.2.</span> <span class="nav-text">Temporal Difference Variational Auto-Encoder</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Rigorous-Agent-Evaluation-An-Adversarial-Approach-to-Uncover-Catastrophic-Failures"><span class="nav-number">3.0.0.0.3.</span> <span class="nav-text">Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Woulda-Coulda-Shoulda-Counterfactually-Guided-Policy-Search"><span class="nav-number">3.0.0.0.4.</span> <span class="nav-text">Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#CEM-RL-Combining-evolutionary-and-gradient-based-methods-for-policy-search"><span class="nav-number">3.0.0.0.5.</span> <span class="nav-text">CEM-RL: Combining evolutionary and gradient-based methods for policy search</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Directed-Info-GAIL-Learning-Hierarchical-Policies-from-Unsegmented-Demonstrations-using-Directed-Information"><span class="nav-number">3.0.0.0.6.</span> <span class="nav-text">Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Hindsight-policy-gradients"><span class="nav-number">3.0.0.0.7.</span> <span class="nav-text">Hindsight policy gradients</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Probabilistic-Planning-with-Sequential-Monte-Carlo-methods"><span class="nav-number">3.0.0.0.8.</span> <span class="nav-text">Probabilistic Planning with Sequential Monte Carlo methods</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Learning-to-Understand-Goal-Specifications-by-Modelling-Reward"><span class="nav-number">3.0.0.0.9.</span> <span class="nav-text">Learning to Understand Goal Specifications by Modelling Reward</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Adversarial-Imitation-via-Variational-Inverse-Reinforcement-Learning"><span class="nav-number">3.0.0.0.10.</span> <span class="nav-text">Adversarial Imitation via Variational Inverse Reinforcement Learning</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#The-Laplacian-in-RL-Learning-Representations-with-Efficient-Approximations"><span class="nav-number">3.0.0.0.11.</span> <span class="nav-text">The Laplacian in RL: Learning Representations with Efficient Approximations</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Marginal-Policy-Gradients-A-Unified-Family-of-Estimators-for-Bounded-Action-Spaces-with-Applications"><span class="nav-number">3.0.0.0.12.</span> <span class="nav-text">Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Soft-Q-Learning-with-Mutual-Information-Regularization"><span class="nav-number">3.0.0.0.13.</span> <span class="nav-text">Soft Q-Learning with Mutual-Information Regularization</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Deep-reinforcement-learning-with-relational-inductive-biases"><span class="nav-number">3.0.0.0.14.</span> <span class="nav-text">Deep reinforcement learning with relational inductive biases</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Preferences-Implicit-in-the-State-of-the-World"><span class="nav-number">3.0.0.0.15.</span> <span class="nav-text">Preferences Implicit in the State of the World</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-agent-RL"><span class="nav-number">4.</span> <span class="nav-text">Multi-agent RL</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#M-3RL-Mind-aware-Multi-agent-Management-Reinforcement-Learning"><span class="nav-number">4.0.0.0.1.</span> <span class="nav-text">M^3RL: Mind-aware Multi-agent Management Reinforcement Learning</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Learning-to-Schedule-Communication-in-Multi-agent-Reinforcement-Learning"><span class="nav-number">4.0.0.0.2.</span> <span class="nav-text">Learning to Schedule Communication in Multi-agent Reinforcement Learning</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Multi-Agent-Dual-Learning"><span class="nav-number">4.0.0.0.3.</span> <span class="nav-text">Multi-Agent Dual Learning</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-learning"><span class="nav-number">5.</span> <span class="nav-text">Deep learning</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Chen Shawn"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Chen Shawn</p>
  <div class="site-description" itemprop="description">Daily life of eating, sleeping, and messing around</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ChenShawn" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ChenShawn" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:c.x.zhao@hotmail.com" title="E-Mail → mailto:c.x.zhao@hotmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/yourname" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/yukio-2" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;yukio-2" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Zhihu</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen Shawn</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.3" zIndex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":200,"height":400},"mobile":{"show":false},"react":{"opacity":1}});</script></body>
</html>
