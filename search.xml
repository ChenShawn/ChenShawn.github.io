<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Collection of Engineering Tricks</title>
      <link href="/2020/04/20/engineering-tricks-collection/"/>
      <url>/2020/04/20/engineering-tricks-collection/</url>
      
        <content type="html"><![CDATA[<p>各种machine learning工程上会遇到的问题及解决方案收集</p><a id="more"></a><p>提纲</p><ul><li>正负样本不均衡：Focal loss</li><li>时序数据/NLP文本数据</li><li>多机多卡分布式训练</li></ul><h2 id="1-Focal-loss"><a href="#1-Focal-loss" class="headerlink" title="1. Focal loss"></a>1. Focal loss</h2><p>出自CVPR 2017何凯明大佬的<a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">Focal Loss for Dense Object Detection</a>，虽然原论文是做object detection任务的，但由于其适用性极强效果也很好，被广泛应用在包括NLP/推荐等各种领域中</p><p><img src="./focal-1.png" alt=""></p><p>何凯明大佬认为object detection数据集中各类之间的正负样本数量是不均衡的，在梯度反向传播的过程中，正样本提供的有效梯度信息会被淹没在大量无意义的负样本中，focal loss可以定义为</p><script type="math/tex; mode=display">FL(p)=-\alpha_{t}(1-p_{t})^{\gamma}\log{p_{t}}</script><p>直观上理解，一般如果是用交叉熵损失的话，负样本比例越大，模型越会倾向于输出接近0的概率值，而数据集中的负样本比例越多，则整体focal loss的数值也就越大；focal loss可以放大hard positive samples，而negative samples的损失会被衰减</p><ul><li>如何理解$\gamma$：$\gamma$被称为focusing factor，$\gamma=0$时该loss等价于交叉熵损失；字面意义上，$\gamma$越大，则损失函数会越倾向于衰减易分类样本的权重，从而使得模型在分类中更加倾向于难分类的样本；$\gamma$值不宜过大，如果模型在训练初期的输出概率普遍接近0.5，那么$(1-p_{t})$会很快被衰减到接近于0</li><li>如何理解$\alpha$：论文里被称作weighting factor，其实给正负样本的loss分别加权重应该是一个很容易想到的idea，以前也确实有很多人这样做过，仅仅对正负样本乘以不同weight的方法在这种文章中作为对比的baseline</li></ul><p>实验结果表明，在object detection任务上效果最好的超参组合是$\gamma=2, \alpha=0.25$，一般其他任务会设置$\alpha=0.5$</p><p>tf2.x时代的代码实现见<a href="https://github.com/tensorflow/addons/blob/v0.7.1/tensorflow_addons/losses/focal_loss.py#L100-L151" target="_blank" rel="noopener">tf-addon的github代码库</a>，写的还是挺清爽的，可以直接copy过来用</p><h2 id="2-Sequential-data-processing"><a href="#2-Sequential-data-processing" class="headerlink" title="2. Sequential data processing"></a>2. Sequential data processing</h2><p><strong>Everyone knows the facts:</strong> Sequential data最常见的是自然语言和时间序列，DL时代一般处理sequential data的常规工具包括LSTM与GRU，GRU可以看做是LSTM的简化版本，现有实验研究的数据表明绝大多数LSTM与GRU的效果是差不多的</p><h2 id="3-Distributed-Training-with-horovod"><a href="#3-Distributed-Training-with-horovod" class="headerlink" title="3. Distributed Training with horovod"></a>3. Distributed Training with horovod</h2><h3 id="3-1-Introduction"><a href="#3-1-Introduction" class="headerlink" title="3.1. Introduction"></a>3.1. Introduction</h3><p>horovod是uber发布的一个分布式深度学习训练框架（<a href="https://github.com/horovod/horovod" target="_blank" rel="noopener">github repository link here!</a>），其motivation来源于两篇文章：</p><ul><li>Facebook: <a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf" target="_blank" rel="noopener">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</a>，主要思路是多GPU输入不同的数据batch，各个GPU梯度求平均，最后broadcast到各个GPU显存中更新模型参数</li><li>Baidu: <a href="https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/" target="_blank" rel="noopener">Bringing HPC Techniques to Deep Learning</a>，这篇文章重点考虑了多机多卡之间broadcast梯度的方式，即后来大名鼎鼎的ring-allreduce，后来ByteDance AILab貌似也是对broadcasting这一点做了一些改进</li></ul><blockquote><p>插两句废话，其实horovod不算是什么新技术，遥想本渣渣研二的时候还研究过一番tensorflow 1.x的分布式训练框架，当时最大的感受就是概念极其繁多，从<code>tf.Server</code>到<code>tf.train.SyncReplicasOptimizer()</code>再到<code>tf.Supervisor</code>，每一个类都会让人莫名其妙这又是什么鬼东西，作为最早将mapreduce落地应用到商业领域的公司，Google在分布式计算方面拥有领跑全球的技术不假，但他们似乎并没有兴趣仔细打磨分布式训练应用层的用户体验</p></blockquote><p><img src="./horovod-1.png" alt="horovod性能benchmark"></p><p>horovod的官方文档还是比较友好的：</p><ul><li><a href="https://horovod.readthedocs.io/en/latest/tensorflow.html" target="_blank" rel="noopener">horovod官网文档</a></li><li><a href="">horovod github文档</a></li><li><a href="https://www.jianshu.com/p/383ae89de556" target="_blank" rel="noopener">一篇hvd+keras的博客</a></li></ul><h3 id="3-2-tf-2-x-hvd应用案例"><a href="#3-2-tf-2-x-hvd应用案例" class="headerlink" title="3.2. tf-2.x+hvd应用案例"></a>3.2. tf-2.x+hvd应用案例</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> horovod.tensorflow.keras <span class="keyword">as</span> hvd</span><br><span class="line"><span class="comment"># import horovod.tensorflow.keras as hvd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize Horovod</span></span><br><span class="line">hvd.init()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Implement one process per GPU device</span></span><br><span class="line">gpus = tf.config.experimental.list_physical_devices(<span class="string">'GPU'</span>)</span><br><span class="line"><span class="keyword">for</span> gpu <span class="keyword">in</span> gpus:</span><br><span class="line">    tf.config.experimental.set_memory_growth(gpu, <span class="literal">True</span>)</span><br><span class="line"><span class="keyword">if</span> gpus:</span><br><span class="line">    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], <span class="string">'GPU'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build model and dataset (omit)</span></span><br><span class="line">dataset = ...</span><br><span class="line">model = ...</span><br><span class="line">loss = ...</span><br><span class="line">opt = tf.optimizers.Adam(<span class="number">0.001</span> * hvd.size())</span><br><span class="line">opt = hvd.DistributedOptimizer(opt)</span><br><span class="line"></span><br><span class="line">callbacks = [</span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> broadcast initial variable states from rank 0 to all other processes.</span></span><br><span class="line">    <span class="comment"># This is necessary to ensure consistent initialization of all workers when</span></span><br><span class="line">    <span class="comment"># training is started with random weights or restored from a checkpoint.</span></span><br><span class="line">    hvd.callbacks.BroadcastGlobalVariablesCallback(<span class="number">0</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> This callback must be in the list before the ReduceLROnPlateau,</span></span><br><span class="line">    <span class="comment"># TensorBoard or other metrics-based callbacks.</span></span><br><span class="line">    hvd.callbacks.MetricAverageCallback(),</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> using `lr = 0.001 * hvd.size()` from the very beginning leads to worse final</span></span><br><span class="line">    <span class="comment"># accuracy. Scale the learning rate `lr = 1.0` ---&gt; `lr = 1.0 * hvd.size()` during</span></span><br><span class="line">    <span class="comment"># the first five epochs. See https://arxiv.org/abs/1706.02677 for details.</span></span><br><span class="line">    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=<span class="number">5</span>, verbose=<span class="number">1</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># only save checkpoints using </span></span><br><span class="line"><span class="keyword">if</span> hvd.rank() == <span class="number">0</span>:</span><br><span class="line">    callbacks.append(TensorBoard(log_dir=<span class="string">'./tensorboard/'</span>)</span><br><span class="line">    callbacks.append(keras.callbacks.ModelCheckpoint(<span class="string">'./history-&#123;epoch:02d&#125;.hdf5'</span>))</span><br></pre></td></tr></table></figure><p>几个需要注意的细节：</p><ul><li>采用N个GPU进行分布式训练时，所有设备上的梯度最终要allreduce去求平均，实际上的<code>batch_size</code>相当于变成了每个device上<code>batch_size</code>的N倍，相对应地，理论上梯度的方差，<span style="color: red">a.k.a Fisher information</span>，会变成原来的1/N，所以设定learning rate的时候可以把单设备上的lr乘以<code>hvd.size()</code>；</li><li>然而，facebook那篇文章中的实验表示这样做的效果并不好，原因也很容易想到，SGD中梯度的噪声分布本来是各向异性的，当你reduce了噪声，也就加大了模型收敛到bad optima的可能性，因此需要在callback中加入<code>LearningRateWarmupCallback</code>；</li><li>我在公司实习期间曾多次遇到程序在运行tensorboard callback时挂起的问题，github上也有很多人提了类似的issue，比如<a href="https://github.com/horovod/horovod/issues/742" target="_blank" rel="noopener">horovod issues #742</a>、<a href="https://github.com/horovod/horovod/issues/938" target="_blank" rel="noopener">horovod issues #938</a>以及<a href="https://github.com/horovod/horovod/issues/297" target="_blank" rel="noopener">horovod hang on with TensorBoard #297</a>，这里暗坑很多，建议自己把<code>train_step</code>写成<code>tf.function</code>，在函数自己定义tensorboard记录</li></ul><h3 id="3-3-运行"><a href="#3-3-运行" class="headerlink" title="3.3. 运行"></a>3.3. 运行</h3><p><code>horovod</code>的程序可以用<code>openmpi</code>或<code>horovodrun</code>二者之一运行，前者能够达到fine-grained control，而后者是前者的简单封装，能让你少写几个意味不明的参数；虽然官网文档中推荐使用后者，实际在公司还是用<code>mpirun</code>更多一些，下面的例子参考了<a href="https://github.com/horovod/horovod/blob/master/docs/mpirun.rst" target="_blank" rel="noopener">官网文档: Run Horovod with Open MPI</a></p><p>单机四张卡运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The following two ways are equivalent</span></span><br><span class="line">horovodrun -np 4 python train.py</span><br><span class="line"></span><br><span class="line">mpirun -np 4 \</span><br><span class="line">    -<span class="built_in">bind</span>-to none -map-by slot \</span><br><span class="line">    -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH \</span><br><span class="line">    -mca pml ob1 -mca btl ^openib \</span><br><span class="line">    python train.py</span><br></pre></td></tr></table></figure><p>四台机器，每台机器四张卡</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The following two ways are equivalent</span></span><br><span class="line">horovodrun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py</span><br><span class="line"></span><br><span class="line">mpirun -np 16 \</span><br><span class="line">    -H server1:4,server2:4,server3:4,server4:4 \</span><br><span class="line">    -<span class="built_in">bind</span>-to none -map-by slot \</span><br><span class="line">    -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH \</span><br><span class="line">    -mca pml ob1 -mca btl ^openib \</span><br><span class="line">    python train.py</span><br></pre></td></tr></table></figure><p><b style="color: red">NOTE:</b> <code>${server}:${num_gpus}</code> 这里填ip地址和对应那台机器上的GPU数量，更详细的参数设置见上面的链接</p>]]></content>
      
      
      
        <tags>
            
            <tag> Resource collection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Distributed Machine Learning Collection</title>
      <link href="/2020/03/18/federated-learning/"/>
      <url>/2020/03/18/federated-learning/</url>
      
        <content type="html"><![CDATA[<p>收集、学习、分析近年来优秀的分布式机器学习工作</p><p><em>本文不讨论单机环境上开多线程共享stack这种“虚假”的分布式：任何一个分布式算法，如果没有办法部署到真正的集群/客户端/云平台，并对scalability有较好的解决方案，即可被定义为“虚假”的分布式算法</em></p><a id="more"></a><h2 id="1-Federated-learning"><a href="#1-Federated-learning" class="headerlink" title="1. Federated learning"></a>1. Federated learning</h2><h3 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1. Background"></a>1.1. Background</h3><p>2018年5月28日，欧盟发布了<a href="https://gdpr-info.eu/" target="_blank" rel="noopener">the General Data Protection Regulation (GDPR)</a>，该法律主要面向用户数据隐私问题，对互联网公司对用户隐私数据的使用出台了严格的法律限制，违反者将被处以高额罚金，媒体称其为“史上最严厉最翔实”的保护用户数据安全的法律。该法律一经出台，以Facebook为首的一众公司股票连日下跌，扎克伯格连续两天现身美国国会参议院的听证会，民间拍手称快者有之，嗟乎欧洲AI发展穷途末路者有之，暗喜中国AI可弯道超车者有之，借机推销商业概念或自己组工作者亦有之</p><p>为什么很多互联网公司的AI团队如丧考妣？按照GDPR的规定，互联网公司要想使用用户数据做模型训练，必须取得数据集中的每一名用户的同意，对于Google和Facebook这些用户过亿的公司来说，这样的规定简直是要了他们的老命</p><p>这样严格的法律规定下也催生了很多新的思考：如果我们没有办法拿到用户数据（或者如果我们没有办法拿到<strong>大量的</strong>用户数据），是不是机器学习这条路就完全走不通了呢？就没有一种办法，可以让互联网公司在完全不接触任何实际用户数据的情况下训练模型吗？</p><h3 id="1-2-FL-Framework"><a href="#1-2-FL-Framework" class="headerlink" title="1.2. FL Framework"></a>1.2. FL Framework</h3><p>FL是Google给出的一个解决方案，最早出自Google AI的blog文章：<a href="https://blog.ml.cmu.edu/2019/11/12/federated-learning-challenges-methods-and-future-directions/" target="_blank" rel="noopener">Federated Learning: Collaborative Machine Learning without Centralized Training Data</a>，整体思路很清晰而且很fancy：要使用FL方法训练一个模型，大概可以分为以下几步：</p><ol><li>server端初始化模型；</li><li>按照某种随机方式选取选取部分用户，经过用户移动端的app，server将模型发送到每个用户的手机/PC上；</li><li>每个用户的手机上，当满足一定条件时（比如用户客户端本地的数据达到一定数量、已连接wifi且正在充电中），拿<strong>客户端本地</strong>的用户数据训练模型；</li><li>客户端向server上传模型，server段对收集到的N多用户模型参数进行聚合（Google的博客和paper里一般叫<strong>aggregation</strong>），比如最简单也使用最广泛的一种聚合方式就是所有用户的模型参数求平均，叫做FedAvg；</li><li>重复步骤2-4，直至模型收敛；</li></ol><h3 id="1-3-Challenges-and-technical-details"><a href="#1-3-Challenges-and-technical-details" class="headerlink" title="1.3. Challenges and technical details"></a>1.3. Challenges and technical details</h3><p>整个idea看起来很fancy，给人一种“这tm都可以”的感觉，实际上具体到整个系统的technical details上都是问题，FL可以看做是整合了很多不同领域的思想，实现整个系统需要涉及到很多方面的领域知识：</p><ul><li>Machine learning, distributed machine learning</li><li>Privacy, information security</li><li>Distributed systems</li><li>Numerical optimization for deep learning on mobile devices or CPU machines</li></ul><p>涉及到具体的technical details，会有很多细节问题值得商榷：</p><ul><li><strong>Model training on mobile devices:</strong> 既然要在移动端设备上做本地训练，那么网络一定不能大，不能指望在移动端设备上跑BERT或者ResNet-152</li><li><strong>Convergence on non-IID data:</strong> 看到FL的idea就觉得这居然都能收敛实在是非常反直觉的事情。知乎也有此方向的大佬指出，目前ArXiv上已有两篇有关converence on non-iid data的工作，两篇文章都说明模型在non-iid数据上的收敛性是成问题的，详见 <a href="https://arxiv.org/abs/1907.02189" target="_blank" rel="noopener">On the Convergence of FedAvg on Non-IID Data</a> 和 <a href="https://arxiv.org/abs/1806.00582" target="_blank" rel="noopener">Federated Learning with Non-IID Data</a></li><li><strong>Security:</strong> 这方面我不是很懂，Google称为了避免客户端上传模型时泄露隐私，每个用户上传的模型都使用了连server都没有的秘钥加密，通过secure aggregation技术，可以使得这些用户模型聚合之后得到所有用户模型的均值。问题在于这里的secure aggregation貌似不是100%无损准确地恢复，而是给了一个数学期望意义上的保证，那么用了这个secure aggregation以后，模型训练有多大程度上受到影响就变成了一个值得商榷的问题（毕竟之前model quantization的研究中即使只是把<code>float_32t</code>换成<code>int_16t</code>都会让模型出不少问题）</li><li><strong>Robustness:</strong> 之前Robustnesｓ ML的研究中就有一类攻击方法叫做data poisoning，指的是训练数据中混入的少量恶意样本足以使得模型不收敛或收敛至完全错误的决策边界。对于新兴的FL工业应用来说，谈adversarial examples攻防或许有点为时尚早，但FL的分布式训练系统对outlier用户的robustness绝对是一个值得讨论的问题，如果存在一两个outlier用户就足以毁掉整个模型的训练过程，那么FL系统的应用就比较困难了</li></ul><p><img src="./fl-1.png" width="98%"></p><h2 id="2-Reinforcement-learning"><a href="#2-Reinforcement-learning" class="headerlink" title="2. Reinforcement learning"></a>2. Reinforcement learning</h2><h3 id="2-1-A3C"><a href="#2-1-A3C" class="headerlink" title="2.1. A3C"></a>2.1. A3C</h3><p>出自ICML 2016的paper: <a href="https://arxiv.org/abs/1602.01783" target="_blank" rel="noopener">Asynchronous Methods for Deep Reinforcement Learning</a>，曾在相当一段时间里主宰了Atari任务上的state-of-the-art</p><blockquote><p>作者通过多线程的方式来实现并行：每个线程有一份独立的policy参数以及一个游戏环境，这个policy会和环境交互产生数据来计算梯度。在跑每个episode之前，这个policy都会从parameter server上拉取最新的模型参数，然后在跑完每个episode之后计算梯度，把梯度上传到parameter server做异步梯度更新。在当时看来，这个算法不仅能极大提升训练速度，而且效果也比当时的单机DQN版本好很多</p></blockquote><p><strong>Pros:</strong></p><ul><li>站在2020年当前distributed RL的视角来看，这篇工作在并行化与分布式处理的细节方面相对比较粗糙，但无疑为distributed RL中大力出奇迹的思想开了先河，现在大部分的distributed RL算法视线中都能看到A3C的影子。最关键的启发在于：A3C首创采用多个CPU机器做环境探索，各台机器之间可以异步独立工作，这很大程度上缓解了RL算法在CPU上采样与GPU上优化模型之间的延迟问题，并且加快了复杂环境中探索环境的速度</li></ul><p><strong>Cons:</strong></p><ul><li>异步梯度还是同步梯度，这是一个问题：一般来说异步梯度很不稳定，甚至CPU机器与GPU机器配比不合适也会影响到模型收敛；但使用同步梯度意味着GPU机器可能需要花费大量的时间忙等CPU机器传梯度，而且同步时还需要考虑万一某台CPU机器挂掉的可能性；</li><li>整个系统的io成本实际上是比较高的：第一，CPU机器在每个episode开始时都会向GPU机器请求更新模型文件；第二：CPU机器需要自己根据模型和样本算梯度，然后传给GPU机器；以上两点，如果一个模型的参数有100MB <span style="color: red">(which is common for industrial RL models)</span>，那么每一次io都是100MB的数据传输（作为对比参考，一般在rpc系统里超500KB就算比较大型的数据包了）</li><li>整个系统并没有充分发挥CPU与GPU的性能：对于GPU来讲，大量CPU机器异步请求模型文件，意味着GPU机器在迭代模型的同时必须不停地把模型落盘；对于CPU来讲，本身CPU其实并不适合干神经网络inference的活，A3C的系统却要求每台CPU机器自己去跑BP算梯度；这两点是比较明显的性能浪费</li></ul><h3 id="2-2-IMPALA"><a href="#2-2-IMPALA" class="headerlink" title="2.2. IMPALA"></a>2.2. IMPALA</h3><p>见之前的blog: <a href="https://chenshawn.github.io/2019/03/18/rl-up-to-date/#IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures" target="_blank" rel="noopener">DeepMind Paper Reading —— How is DeepMind Different from OpenAI: IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</a></p><p><strong>Pros:</strong></p><ul><li>Trajectory-based optimization</li><li>V-trace off-policy correction</li></ul><p><strong>Cons:</strong></p><ul><li>讲真IMPALA文章里放出来的结果非常漂亮，如果没亲自用过IMPALA的话大概不可能挑出什么真的毛病来；站在2020年的视角马后炮，从SEED反推，可以看出IMPALA在tensorflow模型静态图优化方面做的已经非常出色了，但针对集群和TPU尚存在不少可优化的空间</li></ul><h3 id="2-3-APE-X"><a href="#2-3-APE-X" class="headerlink" title="2.3. APE-X"></a>2.3. APE-X</h3><p>出自DeepMind的论文<a href="https://arxiv.org/abs/1803.00933" target="_blank" rel="noopener">Distributed Prioritized Experience Replay</a>，整篇文章比较偏工程，类似于Rainbow DQN，APE-X结合了之前若干重要的DQN文章的idea，包括</p><ul><li>Dueling DQN: 将整个网络结构设计成两个子网络的相加，语义上一个网络代表value函数，另一个网络代表advantage函数，为了避免歧义，定义advantage函数的输出均值为0</li><li>Double DQN：为了避免在Bellman迭代时产生value overestimation，采用两个Q网络并采用两个网络中预测Q值较小的作为target-Q value estimation</li><li>N-step Bellman return: 在Q-learning的框架下，采用N-step Bellman return优化模型在理论上是存在问题的（主要问题在于Q-learning中Q-function学习的是greedy policy的value estimation，而greedy policy是对于每一步而言的）；然而现实场景下N-step Bellman return往往能够得到比较好的效果，尤其是对于像Atari这种相邻frame差别可以非常小的场景而言更加如此</li><li>Distributed prioritized experience replay，<b color="red">待补充</b></li></ul><p><strong>Cons:</strong> Novelty is limited… A bit more like a combination of several previous works… And the fact that using distributed prioritized experience replay makes better performance is not surprising, since distributed PER provides stronger exploration ability, and alleviates non-iid-ness of the training data</p><h3 id="2-4-R2D2"><a href="#2-4-R2D2" class="headerlink" title="2.4. R2D2"></a>2.4. R2D2</h3><p>出自ICLR 2019 DeepMind的工作：<a href="https://openreview.net/forum?id=r1lyTjAqYX" target="_blank" rel="noopener">Recurrent Experience Replay in Distributed Reinforcement Learning</a>，可以看做是APE-X的改进版，主要的改进包括：</p><ul><li>Propose to use LSTM to learn a good representation from sequence of $(s_0, a_0, r_0, s_1, a_1, r_1, …, s_T, a_T, r_T)$ samples</li><li>Propose the <strong>burn-in</strong> heuristic to generate the initialization state for the LSTM, sufficient experimental results have demonstrated the effectiveness of the BURN-IN heuristic</li></ul><p>文章的一大亮点在于作者用详细的实验数据比较了使用<code>zero_state</code>或<code>saved_recurrent_state</code>作为LSTM的初始<code>state_initialization</code>的优缺点：</p><ul><li><code>zero_state</code>: 由于本文引入LSTM的目的在于从时序化的序列数据中学习state的representation，采用zero-state作为输入无疑会解耦合之前的state与当前sequence的联系，除非LSTM够长，否则肯定会影响效果（但是LSTM太长又容易导致梯度爆炸/消失之类的烦心事），文中的实验数据也表明zero-state的效果最差</li><li><code>saved_recurrent_state</code>: 采用时序意义上，上一个sequence输入LSTM得到的<code>final_state</code>作为初始state，由actor输出然后保存在replay memory中。作者指出，这样做的问题在于representation shift现象，off-policy训练时保存在replay buffer里的hidden state有可能是很久以前的，在被采样到之前，即使LSTM的参数只发生了微小的变化，只要LSTM够长，最终LSTM输出的hidden state就有可能相差很远，这意味着过去保存在replay buffer里的hidden state有可能已经不适用于现在的LSTM模型</li><li><code>burn_in_state</code>: 比如说sequence长度是80，可以用前40做burn-in，LSTM的第40个hidden state输出作为初始state，用BPTT参数更新时LSTM的前40个循环不参与求梯度（这个代码上应该怎么实现？）</li></ul><h3 id="2-5-Seed-RL"><a href="#2-5-Seed-RL" class="headerlink" title="2.5. Seed RL"></a>2.5. Seed RL</h3><p>出自ICLR 2020，Google大大联合DeepMind又一篇有钱任性的文章：<a href="https://openreview.net/forum?id=rkgvXlrKwH&amp;noteId=rkgvXlrKwH" target="_blank" rel="noopener">SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference</a>，abstract里是这样介绍SEED的</p><blockquote><p>SEED adopts two state of the art distributed algorithms, IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), and is evaluated on Atari-57, DeepMind Lab and Google Research Football. We improve the state of the art on Football and are able to reach state of the art on Atari-57 three times faster in wall-time. For the scenarios we consider, a 40% to 80% cost reduction for running experiments is achieved.</p></blockquote><p><img src="./seed-rl-1.png" width="98%"><br><img src="./seed-rl-2.png" width="98%"></p><h3 id="2-6-Dreamer"><a href="#2-6-Dreamer" class="headerlink" title="2.6. Dreamer"></a>2.6. Dreamer</h3><p>Yet another paper from Google Brain，出自ICLR-2020 <a href="https://arxiv.org/pdf/1912.01603.pdf" target="_blank" rel="noopener">Dream to Control: Learning Behaviors by Latent Imagination</a>,与上面介绍的若干方法最大的不同点在于dreamer是一个基于model-based RL方法的distributed RL系统，前身PlaNet出自 <a href="https://arxiv.org/pdf/1811.04551.pdf" target="_blank" rel="noopener">Learning latent dynamics for planning from pixels</a></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.zhihu.com/question/351886375/answer/868162709" target="_blank" rel="noopener">[zhihu] 联邦学习在机器学习领域有什么独立存在的价值?</a></li><li><a href="https://zhuanlan.zhihu.com/p/78920620" target="_blank" rel="noopener">[Ant AI] 共享学习：蚂蚁金服数据孤岛解决方案</a></li><li><a href="https://blog.ml.cmu.edu/2019/11/12/federated-learning-challenges-methods-and-future-directions/" target="_blank" rel="noopener">[CMU] Federated Learning: Challenges, Methods, and Future Directions</a></li><li><a href="https://blog.csdn.net/jILRvRTrc/article/details/90724873" target="_blank" rel="noopener">[Google AI] 联盟学习到底是什么？我们画了部漫画……</a></li><li><a href="https://zhuanlan.zhihu.com/p/77976582" target="_blank" rel="noopener">[zhihu] 强化学习并行训练论文合集</a></li></ul><h2 id="P-S-Shit，怎么全都是Google…"><a href="#P-S-Shit，怎么全都是Google…" class="headerlink" title="P.S. Shit，怎么全都是Google…"></a>P.S. Shit，怎么全都是Google…</h2>]]></content>
      
      
      
        <tags>
            
            <tag> Paper reading </tag>
            
            <tag> Research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Web Server Based Parallel RL Training Framework</title>
      <link href="/2020/02/18/web-based-parallel-rl/"/>
      <url>/2020/02/18/web-based-parallel-rl/</url>
      
        <content type="html"><![CDATA[<p>一个基于Web Server的的分布式训练架构，开发中，代码见 <a href="https://github.com/ChenShawn/webserver-based-parallel-rl-training" target="_blank" rel="noopener">webserver-based-parallel-rl-training</a></p><a id="more"></a><h2 id="1-TODO"><a href="#1-TODO" class="headerlink" title="1. TODO"></a>1. TODO</h2><ul><li>[x] 写数据用<code>atomic_uint64</code>来取样本序列下标，从而保证写事务的一致性</li><li>[ ] 数据读取速度太慢，关键的问题在于能不能避免读数据的时候发request</li></ul><h2 id="2-Architecture-Overview"><a href="#2-Architecture-Overview" class="headerlink" title="2. Architecture Overview"></a>2. Architecture Overview</h2><p>该架构由三个结构组成</p><ul><li>worker $\times$ N: maintain the policy network and the target-Q network, use the models to roll out samples</li><li>mempool server $\times$ N: save the samples from workers to the replay memory buffer, deliver samples to the trainer in demand</li><li>trainer: read samples from mempool server and train models, use soft update to maintain the target networks, send target network models to workers frequently to replace the olds</li></ul><p>三个server之间互相沟通的关系如下图</p><p><img src="./architecture.png" width="100%"></p><h2 id="3-Mempool-Server"><a href="#3-Mempool-Server" class="headerlink" title="3. Mempool Server"></a>3. Mempool Server</h2><p>mempool应该是这个架构中技术最难的部分，主要存在两个问题</p><ul><li>由上图可以明显看出mempool server必须要异步地处理worker发来的样本，同时还要将样本发送给trainer，这是一个典型的读者-写者问题，写者是worker，读者是trainer，同一个mempool server可以有多个worker发样本，这意味着无锁化会很难搞</li><li>样本从worker到trainer的传输效率问题<ul><li>首先mempool server作为一个中间介质的存在是必要的，不然没有办法做到让模型训练时从一个样本集合中均匀地采样batch</li><li>样本量比较大的时候，考虑用pb编码传输效率比json更快，可以让worker直接把样本编码成pb发送，问题在于pb的反序列化是CPU密集型的，所以反序列化肯定不能放在trainer里面做，但如果在mempool server中反序列化再通过http发送未经压缩的样本，那么worker用pb编码样本的意义就不存在了，最终性能的瓶颈会受限于mempool server到trainer的传输效率，所以也不能用http发送样本，这个意义上讲mempool与Trainer之间通过shared memory传数据是最优选择</li></ul></li></ul><h3 id="3-1-读者-写者问题"><a href="#3-1-读者-写者问题" class="headerlink" title="3.1. 读者-写者问题"></a>3.1. 读者-写者问题</h3><p>考虑到trainer的读取请求应该会比worker的写入请求更多，本repo采用写者优先的策略，伪代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayMemory</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, pbdata)</span>:</span></span><br><span class="line">        self.writelock.acquire()</span><br><span class="line">        self.rwlock.acquire()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># do some samples saving</span></span><br><span class="line">        do_some_samples_saving()</span><br><span class="line"></span><br><span class="line">        self.rwlock.release()</span><br><span class="line">        self.writelock.release()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        self.writelock.acquire()</span><br><span class="line">        self.readcntlock.acquire()</span><br><span class="line">        <span class="keyword">if</span> self.readcnt == <span class="number">0</span>:</span><br><span class="line">            self.rwlock.acquire()</span><br><span class="line">        self.readcnt += <span class="number">1</span></span><br><span class="line">        self.readcntlock.release()</span><br><span class="line">        self.writelock.release()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># do some reading</span></span><br><span class="line">        do_some_reading()</span><br><span class="line"></span><br><span class="line">        self.readcntlock.acquire()</span><br><span class="line">        self.readcnt -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> self.readcnt == <span class="number">0</span>:</span><br><span class="line">            self.rwlock.release()</span><br><span class="line">        self.readcntlock.release()</span><br></pre></td></tr></table></figure><p>是否存在无锁化方案仍在调研中。。。</p><h3 id="3-2-骚操作之一：在Flask中用protobuf"><a href="#3-2-骚操作之一：在Flask中用protobuf" class="headerlink" title="3.2. 骚操作之一：在Flask中用protobuf"></a>3.2. 骚操作之一：在Flask中用protobuf</h3><p>安装过程比较简单，按照官方给的文档来就不会踩坑，首先到 <a href="https://github.com/protocolbuffers/protobuf/releases" target="_blank" rel="noopener">https://github.com/protocolbuffers/protobuf/releases</a> 去下载需要用到的语言对应的源码压缩包，下载好以后需要先编译cpp的版本才能编译其他语言，按照 <a href="https://github.com/protocolbuffers/protobuf/tree/master/src" target="_blank" rel="noopener">https://github.com/protocolbuffers/protobuf/tree/master/src</a> 页面的readme文档编译安装即可</p><p>cpp版本编译完成后，Python版本编译方法如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">python setup.py build</span><br><span class="line">python setup.py install</span><br><span class="line">python setup.py <span class="built_in">test</span></span><br><span class="line"><span class="comment"># test cases (if neccesary)</span></span><br><span class="line"><span class="comment"># python </span></span><br><span class="line"><span class="comment"># &gt;&gt;&gt; import google.protobuf</span></span><br></pre></td></tr></table></figure></p><h3 id="3-3-骚操作之二：将numpy-array的内存空间映射到shared-memory"><a href="#3-3-骚操作之二：将numpy-array的内存空间映射到shared-memory" class="headerlink" title="3.3. 骚操作之二：将numpy array的内存空间映射到shared memory"></a>3.3. 骚操作之二：将numpy array的内存空间映射到shared memory</h3><blockquote><p>经验教训写在最前面：以后用Linux系统层的接口一定要认真看官方的mannual，网上的大部分blog都是复制来复制去，一个<code>man xxx</code>比十篇blog都管用</p></blockquote><p>本项目中对shared memory的处理方式是：在Python中，用numpy.ndarray的格式开一个足够放下一整个batch数据的buffer，将numpy的指针传入cpp中，cpp将numpy.ndarray内存空间映射为shared memory，然后Python就可以直接通过操作numpy格式的array来操作shared memory了</p><p><img src="./shared_memory.png" width="90%"></p><p>shared memory是Linux系统接口提供的功能，可以让不同进程进行高效的数据交互，其原理本质是页表映射：操作系统首先开辟一片物理空间作为进程间共享的shared memory，然后在两个进程内的一片连续的空间内，将逻辑地址通过页表映射到一片相同的物理地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看系统中的共享内存</span></span><br><span class="line">ipcs -m</span><br><span class="line"><span class="comment"># 通过shmid删除某片共享内存</span></span><br><span class="line">ipcrm -m <span class="variable">$&#123;shmid&#125;</span></span><br></pre></td></tr></table></figure><p><code>shmget</code>，在物理内存空间中创建共享内存，成功返回shmid，错误返回-1<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">shmget</span><span class="params">(<span class="keyword">key_t</span> key, <span class="keyword">size_t</span> <span class="built_in">size</span>, <span class="keyword">int</span> shmflg)</span></span>;</span><br></pre></td></tr></table></figure></p><ul><li><code>key</code>：大于零的整数即可，一般可以用<code>ftok</code>来生成，<code>ftok</code>中的参数可以随便定，系统会保证给你返回一个不冲突的shmid；本项目中考虑到从Python向cpp传<code>ftok</code>的参数字符串很麻烦，选择直接手动指定</li><li><code>size</code>：需要注意这个函数中会将<code>size</code>变量自动round up，推荐直接传<code>getpagesize()</code>整数倍大小的<code>size</code>值</li><li><code>shmflg</code>：<code>IPC_CREAT</code>就代表创建，可以通过或操作符设置权限，比如本项目中用到的是 <code>IPC_CREATE|0666</code>，代表所有用户都有读写权限；如果是想找到一个已经存在的共享内存段，该参数直接传0即可；其他一些flag可以直接用 <code>man shmget</code> 查看</li></ul><p><code>shmat</code>，将进程内的一篇连续空间使用页表映射到物理内存中的共享内存，成功返回进程内被关联到的共享内存逻辑地址指针，错误返回-1<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">shmat</span><span class="params">(<span class="keyword">int</span> shmid, <span class="keyword">const</span> <span class="keyword">void</span> *shmaddr, <span class="keyword">int</span> shmflg)</span></span>;</span><br></pre></td></tr></table></figure></p><ul><li><code>shmid</code>：shmget的返回值，唯一标定一块具体的shared memory物理空间</li><li><code>shmaddr</code>：这个参数名堂不小，网上几乎所有的blog都会告诉你这个参数直接填0，这样操作系统就会自己去找一篇合适的内存空间，但这显然不符合我们的需求，我们想要的是让操作系统将shared memory映射到指定的numpy.ndarray空间中。如果该参数非空，那么系统要求<code>shmaddr</code>满足以下二者之一<ul><li><code>shmaddr</code>必须得是页对齐的，然而Python中开numpy的array才不会管你有没有页对齐，我搜了好半天的api也没搜到有什么办法能开出来页对齐的numpy.ndarray，所以此方案pass</li><li><code>shmaddr</code>没有页对齐的情况下，<code>shmflg</code>必须指定<code>SHM_RND</code>，指定之后该函数会自动将<code>shmaddr</code>的地址<strong>round down</strong>到页对齐的地址去——这个round down真的是很坑，无论是栈空间上的数组还是堆空间开辟的内存，用户肯定都是用头指针进行操作的，如果你传进去numpy.ndarray的头指针，那么有可能round down的过程中就会访问到越界的内存——目前想到的最优解决方案是传numpy.ndarray的头指针加上<code>getpagesize()</code>，这样向下做页对齐的时候就肯定不会越界</li></ul></li><li><code>shmflg</code>：除了上面说的<code>SHM_RND</code>以外，还有一个网上 所有blog都忽视的一点：如果用户通过<code>shmaddr</code>传进去了一个已经被页表映射过的内存空间，那么这里必须指定<code>SHM_REMAP</code>，这个flag会强制操作系统对<code>shmaddr</code>对应的内存进行重新映射（如果没有这个标志的话会报错，errno=22，打开shm的头文件你会发现错误码里甚至没有22，可能需要查很久才会发现问题所在）</li></ul><p><strong>TODO:</strong> 加了<code>SHM_REMAP</code>之后，对<code>shmaddr</code>对应的内存地址做remap之后会不会原来的内存空间就泄露了？</p><blockquote><p>目前还没有结论，直觉上来讲是很有可能的，因为一般应用层分配堆空间内存都是用<code>malloc</code>之类的，而页表内存映射是更底层的东西。个人认为即使这里出现内存泄漏也是可以接受的，因为它只发生在<code>ReplayMemory</code>的构造函数中，也就是说整个程序运行时只会泄露一次，不会随着程序运行越漏越多</p></blockquote><p>相比之下，释放shm就比较简单了，直接放代码<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">close_shm</span><span class="params">(<span class="keyword">float</span>* shmbuf, <span class="keyword">int</span>* addinfo)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> shmid = addinfo[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> offset = addinfo[<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">if</span> (shmdt((<span class="keyword">char</span>*)shmbuf + offset) == <span class="number">-1</span>) &#123;</span><br><span class="line">        perror(<span class="string">"shmdt failed\n"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (shmctl(shmid, IPC_RMID, <span class="number">0</span>) == <span class="number">-1</span>) &#123;</span><br><span class="line">        perror(<span class="string">"remove shm error\n"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="3-4-numpy与cpp接口的一个小细节"><a href="#3-4-numpy与cpp接口的一个小细节" class="headerlink" title="3.4. numpy与cpp接口的一个小细节"></a>3.4. numpy与cpp接口的一个小细节</h3><p>已经不是第一次踩这个坑了：numpy.ndarray在内存空间中的存储<strong>可能</strong>不是连续的，如果不加处理直接将不连续的numpy指针传进cpp，处理的时候就会出莫名其妙的segmentation fault</p><p>解决方案：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayMemory</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        self.shmbuf = np.zeros((G.MAX_SHM_BYTES_ROUNDED // <span class="number">4</span>), dtype=np.float32)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.shmbuf.flags[<span class="string">'C_CONTIGUOUS'</span>]:</span><br><span class="line">            self.shmbuf = np.ascontiguousarray(self.shmbuf, dtype=np.float32)</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure></p><h2 id="4-Workers"><a href="#4-Workers" class="headerlink" title="4. Workers"></a>4. Workers</h2><p>Workers的主要任务包括以下几个内容</p><ul><li>维持trainer发来的网络模型，并定期更新，其中定期更新可以考虑两种思路<ul><li>指定固定的时长，每次发送完一整个episode就进行时间检查，超过指定时间不管模型文件是否更新都reload模型文件</li><li>每次发送完一整个episode都检查一遍文件的md5sum，从而判断模型文件是否已经更新，若已被更新则马上reload模型文件</li></ul></li><li>采样数据样本，计算<code>reward_sum</code>封装成pb格式发送，这里计算<code>reward_sum</code>也是要分情况讨论<ul><li>对于Q-Learning类的算法，$ R_t = r_t + \gamma * V(s_{t+1}) $，近似等价于GAE中$\lambda=0$的情况</li><li>对于policy gradient类算法，如PPO，最好是用GAE计算</li><li>冷启动：动作可以直接通过<code>env.action_space.sample()</code>采样得到，这种情况<code>reward_sum</code>可以通过backtracking反加得到，等价于GAE中$\lambda=1$</li></ul></li></ul><p>Worker最大的问题在于需要从trainer下载新的模型文件，本项目中这部分功能由mempool server代替trainer实现，本项目的代码中对这部分做了两个方面的优化：</p><p>第一，当模型文件比较大的时候，对worker而言，使用requests进行读取，需要一次性将大文件load到内存中，然后再写入硬盘文件，torch导入时会再次重新从硬盘上读取模型文件。在内存上一次性开辟等同于文件大小的buffer是巨大的浪费，且中间多了一次经过磁盘的操作，如果模型文件比较大，这部分的性能消耗也会比较严重</p><p>当然requests早就考虑到了大文件下载的问题，解决方案是在参数列表中加入<code>stream=True</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ret = requests.get(url + <span class="string">'/sac_critic_target.pth'</span>, stream=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">if</span> ret.status_code == <span class="number">200</span>:</span><br><span class="line">    <span class="keyword">with</span> open(G.CRITIC_FILENAME, <span class="string">'wb'</span>) <span class="keyword">as</span> fd:</span><br><span class="line">        <span class="keyword">for</span> chunk <span class="keyword">in</span> ret.iter_content(chunk_size=<span class="number">128</span>):</span><br><span class="line">            <span class="keyword">if</span> chunk:</span><br><span class="line">                fd.write(chunk)</span><br></pre></td></tr></table></figure></p><p>第二，下载文件是IO密集型的作业，而worker的采样是CPU密集型的作业，因此可以将文件下载封装成单独的一个线程<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DownloadThread</span><span class="params">(threading.Thread)</span>:</span></span><br><span class="line">    flag_success = <span class="literal">False</span></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># download files...</span></span><br><span class="line">        self.flag_success = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_reload</span><span class="params">(self)</span>:</span></span><br><span class="line">        download_thread = DownloadThread()</span><br><span class="line">        download_thread.start()</span><br><span class="line">        <span class="keyword">return</span> download_thread.flag_success</span><br></pre></td></tr></table></figure></p><h2 id="5-Trainer"><a href="#5-Trainer" class="headerlink" title="5. Trainer"></a>5. Trainer</h2><h3 id="5-1-grequests"><a href="#5-1-grequests" class="headerlink" title="5.1. grequests"></a>5.1. grequests</h3><p>由于架构中存在多个mempool server，模型训练的过程中，我们希望trainer可以从每个mempool server中请求数据，这样实际上模型训练时的batch size就是每个mempool server中设定的batch size大小之和，较大的batch size可以很好地降低policy gradient的方差</p><p>那么如何实现trainer从每个mempool中请求数据的过程呢？首先可以确定的是，在一个for循环中去依次请求每个mempool server是非常不scalable的做法，如果有一个mempool server返回延时较高，那么整体请求的响应速度就会被严重拖慢。稍微进阶一点的思路是开线程并行，但众所周知Python有个坑爹的GIL，且线程的代价也比较高，所以考虑使用grequests库来满足我们的要求</p><p>用法比较简单，放代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># use grequests to support large number of mempool server requests</span></span><br><span class="line">req_list = [grequests.get(url) <span class="keyword">for</span> url <span class="keyword">in</span> self.remote_url_list]</span><br><span class="line">ret_list = grequests.map(req_list)</span><br><span class="line"><span class="comment"># ret_list里面每个元素都是一个requests库中定义的HttpResponse类型</span></span><br></pre></td></tr></table></figure></p><h3 id="5-2-send-model"><a href="#5-2-send-model" class="headerlink" title="5.2. send_model"></a>5.2. send_model</h3><p>send_model的功能，是让Trainer每当更新后的模型落盘后，都将模型文件打包发送给workers</p><p>这部分的实现策略思前想后比较纠结，一开始打算用pb发文件，后来想想为了做这件事还得在trainer这里单独开一个http server，颇有杀鸡用牛刀之感；后来还想在Trainer进程中fork出来一个完全异步的Python子进程，让子进程来管理发送模型文件，但是这样子进程做的事情和父进程就没有任何关联了，还不如直接单开进程</p><p>思前想后，既然mempool server和trainer是在同一台物理机器上运行的进程，干脆让mempool server越俎代庖去trainer那边读模型文件，然后worker这边在单开一个进程定期用requests去下载模型就好了</p><h2 id="6-logging"><a href="#6-logging" class="headerlink" title="6. logging"></a>6. logging</h2><p>本项目中所有模块的日志管理都用到了Python中的logging模块，用法非常简单，代码开头注册logger即可，这里记录下本项目采用的log格式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">logfmt = <span class="string">'[%(levelname)s][%(asctime)s][%(filename)s][%(funcName)s][%(lineno)d] %(message)s'</span></span><br><span class="line">logging.basicConfig(filename=<span class="string">'./logs/xxx.log'</span>, level=logging.DEBUG, format=logfmt)</span><br><span class="line">logger = logging.getLogger(__name__)</span><br></pre></td></tr></table></figure><h2 id="7-Potential-references"><a href="#7-Potential-references" class="headerlink" title="7. Potential references"></a>7. Potential references</h2><ul><li><a href="https://github.com/lns/dapo" target="_blank" rel="noopener">Distributed training of RL implemented in Tencent AILab papers</a></li><li><a href="https://github.com/lns/memoire" target="_blank" rel="noopener">Distributed replay memory for RL training</a></li></ul><h2 id="8-tf-2-x-documentation"><a href="#8-tf-2-x-documentation" class="headerlink" title="8. tf-2.x documentation"></a>8. tf-2.x documentation</h2><ul><li><a href="https://tensorflow.google.cn/api_docs/python/tf/data/experimental/make_csv_dataset?hl=zh-cn" target="_blank" rel="noopener">tf.data.experimental.make_csv_dataset</a></li><li><a href="https://tensorflow.google.cn/tutorials/load_data/csv?hl=zh-cn" target="_blank" rel="noopener">用 tf.data 加载 CSV 数据</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
            <tag> Coding </tag>
            
            <tag> Manual </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes for Stein Variational Gradient Descent</title>
      <link href="/2019/11/12/svgd-notes/"/>
      <url>/2019/11/12/svgd-notes/</url>
      
        <content type="html"><![CDATA[<p>Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</p><a id="more"></a><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://arxiv.org/pdf/1608.04471.pdf" target="_blank" rel="noopener">[Paper by Liu] Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</a></li><li><a href="https://arxiv.org/pdf/1602.03253.pdf" target="_blank" rel="noopener">[Paper by Jordan] A Kernelized Stein Discrepancy for Goodness-of-fit Tests</a></li><li><a href="https://www.sanyamkapoor.com/machine-learning/stein-gradient/" target="_blank" rel="noopener">[Blog] The Stein Gradient</a></li><li><a href="http://people.ee.duke.edu/~lcarin/Yunchen3.17.2017.pdf" target="_blank" rel="noopener">[Tutorial slides] Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</a></li><li><a href="http://jianpeng.cs.illinois.edu/SVPG.pdf" target="_blank" rel="noopener">[Paper] Stein Variational Policy Gradient</a></li><li><a href="https://arxiv.org/pdf/1702.08165.pdf" target="_blank" rel="noopener">[Paper] Reinforcement Learning with Deep Energy-Based Policies</a></li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>贝叶斯机器学习常常提到的一个概念是inference，所谓inference即通过已知的先验与数据得到posterior的方法，其具体过程为</p><script type="math/tex; mode=display">p(y|x)=\frac{p(x|y)p(y)}{\int_{\mathcal{Y}}p(x|y)p(y)dy}</script><p>可以说，贝叶斯及其学习的一切都围绕inference展开，上式在实践应用中最大的问题就是分母上的积分不一定可以算出closed-form solution，为此Bayesian们想了很多办法，包括但不限于</p><ul><li>Sampling: MCMC, HMC, etc.</li><li>Variational inference: approximate $p(y|x)$ with $q(y)$, and then<ul><li>Mean field: $q(y)=\prod_{i}q_{i}(y)$</li><li>Black box variational inference: VAE. etc.</li></ul></li><li>Expectation propagation…</li></ul><p>这篇论文中提出的SVGD也是一种概率近似的方法，其核心思想是通过density transformation来最小化 $q(y)$ 与 $p(y|x)$ 的KL</p><script type="math/tex; mode=display">q(y)=q_{T} \circ q_{T-1} \circ q_{T-2} \dots \circ q_{1} \circ q_{0}</script><h2 id="Stein’s-identity"><a href="#Stein’s-identity" class="headerlink" title="Stein’s identity"></a>Stein’s identity</h2><p>Define operator $\mathcal{A}_{p}\phi(x):=\phi(x)\nabla_{x}\log p(x)^{T}+\nabla_{x}\phi(x)$, then</p><script type="math/tex; mode=display">\mathbb{E}_{x\sim{p}}[\mathcal{A}_{p}\phi(x)]=0</script><p>The identity holds true if either</p><ul><li>$p(x)\phi(x)=0\qquad$   $\forall x\in\partial\mathcal{X}$</li><li>$\lim_{|x|\rightarrow\infty}p(x)\phi(x)=0$ when $x\in{\mathbb{R}^{d}}$</li></ul><h2 id="Stein-discrepancy"><a href="#Stein-discrepancy" class="headerlink" title="Stein discrepancy"></a>Stein discrepancy</h2>]]></content>
      
      
      
        <tags>
            
            <tag> Paper reading </tag>
            
            <tag> Research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MARL调研汇总</title>
      <link href="/2019/09/09/2019-fall-review/"/>
      <url>/2019/09/09/2019-fall-review/</url>
      
        <content type="html"><![CDATA[<p>调研汇总multi-agent方向的方法</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>MARL的问题设定早在上个世纪就已经被提出，在深度学习与强化学习兴起之前，在POMDP、game theory、evolutionary strategy等领域都已经有大量研究工作围绕该问题展开</p><p>MARL一般采用stochastic game作为其数学定义，相比于MDP，stochastic game可以看作是MDP在多智能体方面的延伸，其state transition同时受到所有agent的action影响，where each agent can only get local observation instead of global state from the environment，reward由global state给出，每个agent的reward定义可以不同</p><p><img src="./stochastic-game.png" width="98%"></p><p>除了single-agent RL的常规问题以外，MARL会带来的的新问题包括：</p><ul><li><strong>Non-stationary:</strong> 假设每个agent的observation都包含有其他agent的行为与状态，当所有agent同时从环境中学习时，每个agent的policy都会发生变化，使得environment的state transition probability随之变化，这违背了RL对MDP中state distribution为stationary distribution的基本假设</li><li><strong>Non-Markovian:</strong> 在2011年的文章 <a href="http://www.glaurent.free.fr/papers/Laurent2011world.pdf" target="_blank" rel="noopener">The World of Independent Learners is not Markovian</a> 中，作者指出对于independent learners所构成的世界环境，如果各个agent之间不存在沟通，那么该环境不满足Markov性质，这也打破可MDP有关Markov性质的基础假设</li><li><strong>Overfitting to the other agents’ policies:</strong> DeepMind发表于NIPS-2017的文章 <a href="https://arxiv.org/pdf/1711.00832.pdf" target="_blank" rel="noopener">A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning</a> 指出，用single-agent RL训练得到的policy往往容易过拟合对手策略，因而无法在面对不同策略的对手时保持足够的泛化能力。这种现象在游戏AL任务中非常普遍，但截至目前为止，学术界很少有对强化学习模型泛化能力的系统性研究</li><li><strong>Action space too big:</strong> 一种将MARL转化为普通RL的方法是将所有合作博弈的agent认为是一个整体，所有agent的observation集合构成了整体的global state，所有agent的action构成了整体的action，这样的做法对agent的数量增加不具有可扩展性，假设环境中一共有n个agent，每个agent可选择的action有k种，那么整体policy的action就有 $n^{k}$ 种action，指数级大小的action space对于需要scalable的环境来说是无法接受的。截止目前为止，学术界很少有研究聚焦如何在拥有大量agent的环境中训练MARL模型</li><li><strong>Communication:</strong> 尽管agent的observation只包含了global state的局部信息，agent可以通过交流来获取更多的信息，假设所有agent都可以互相交流，且交流是没有成本的，那么实际上每个agent都相当于拥有了整个global state的信息。近年来很多MARL的研究假设agent之间互相交流是有成本的，核心问题在于如何使用尽可能少的交流次数使得模型达到令人满意的performance。需要指出，这种假设对于一些特定领域的MARL应用落地是很有价值的，例如network routing问题中每个agent可能对应的是一个路由器，不同路由器之间每次交流对应数据报文互传，交流过多可能会严重降低网络的传输效率</li></ul><h2 id="OpenAI系列"><a href="#OpenAI系列" class="headerlink" title="OpenAI系列"></a>OpenAI系列</h2><p>OpenAI做MARL的主要落脚点有二，一是连续state-action space的机器人控制问题，二是如Dota这样复杂环境下的游戏AI。前者在现实环境中模拟成本较高，后者的trajectory极长且state space为partial observable，由此在本身就已经足够巨大的解空间中为inference带来不确定性，以上两类任务对强化学习算法共同的要求就是高效的exploration，让模型收敛所需的样本量尽可能少，也正是在这种背景下，OpenAI从一开始的着眼点就是算法的迭代效率</p><h3 id="Multi-Agent-Actor-Critic-for-Mixed-Cooperative-Competitive-Environments"><a href="#Multi-Agent-Actor-Critic-for-Mixed-Cooperative-Competitive-Environments" class="headerlink" title="Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"></a><a href="https://arxiv.org/pdf/1706.02275.pdf" target="_blank" rel="noopener">Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments</a></h3><p>MARL比较具有开创意义的工作，2018年BAIR的MADDPG，学术界绝大多数的MARL研究要从这里说起</p><p>作者首先考虑一个很简单的场景：假设所有agent的policy都是概率为0.5的Bernoulli distribution，假设损失函数梯度 $\nabla_{\theta}J(\theta)=\mathbb{E}_{s\sim{d^{\pi},a\sim\pi}}[Q^{\pi}(s,a)\nabla_{\theta}\log\pi_{\theta}(a|s)]$ ，则单个样本估计出的随机梯度与真实梯度夹角为锐角的概率随着agent数量增加而指数级递减</p><p><img src="./proposition.png" width="96%"></p><p>方法也比较简单，就是用centralized critic</p><p>文章放出了一个叫做 <a href="https://github.com/openai/multiagent-particle-envs" target="_blank" rel="noopener">multiagent-particle-envs</a> 的实验环境，相比之下</p><ul><li>MuJoCo环境的主要难点在于high dimensional continuous state and action space，如何在无穷大空间内explore，至于reward反而由于state space是连续而变得differentiable，众所周知differentiable的reward是最容易学习的</li><li>Atari的主要难度在于如何在discrete action space下合理地解决credit assignment问题，如何应该long-term delayed reward，难度以Montezuma’s revenge为最</li><li>而MPE的主要难点还是在于multi-agent，其他问题都很大程度上简化了，MPE的reward定义一般是距离某个目标的距离，single agent的环境下很容易学习的目标，在multi-agent的环境下有可能变得复杂一些，而且有些环境涉及到communication，有些环境是zero-sum game有些是collaborative game</li></ul><h3 id="Robust-Multi-Agent-Reinforcement-Learning-via-Minimax-Deep-Deterministic-Policy-Gradient"><a href="#Robust-Multi-Agent-Reinforcement-Learning-via-Minimax-Deep-Deterministic-Policy-Gradient" class="headerlink" title="Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient"></a><a href="http://aima.eecs.berkeley.edu/~russell/papers/aaai19-marl.pdf" target="_blank" rel="noopener">Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient</a></h3><p>2019年AAAI有一篇follow的工作M3DDPG，主要解决的问题是MADDPG框架下MARL训练的robustness：作者想要在优化每一个agent时，优化的场景都是对于这个agent当前state下的worst case opponent strategy。<strong>在连续动作空间下</strong>，可以借用SL中的adversarial training的思路，采用一步gradient descent来调整centralized Q-function的其他agent的action</p><p>问题在于为什么我们希望在某个agent的worst case opponent strategy做优化？作者提出的解释是这样可以学习到更robustness的policy，我对此表示怀疑</p><p>首先，文章中没有实验验证单步gradient对Q的影响是怎么样的，在SL中适用的piecewise-linearity vulnerability，是否在RL中仍然显著？如果in practice单步梯度对Q影响甚微，那么这个方法基本就可以看做是MADDPG的approximation了</p><p>我们退一步先假设单步梯度确实可以显著影响到Q-value，那么每次用梯度更新对手策略时，为了得到一个较低的Q，往往得到的对手policy都较为激进。例如在MOBA类游戏中，激进的策略往往包含越塔强杀，强推高地，偷Roshan或者是开雾野区抓人，越是激进的策略就越容易被针对，那么每次都在worst case处优化policy实际上反而过于乐观，当前agent很容易在激进的对方策略下找到一个可以使自己Q-value很高的点，但这个位置的高Q-value并不能体现出robustness</p><p>真正的robustness应该是当前agent可以对<strong>任意的对方策略</strong>下都保证Q不太差，难点在于优化的目标是$\pi$，而实际样本中只有$a, a^{-}$</p><script type="math/tex; mode=display">\max_{\pi}\min_{\pi^{-}}\mathbb{E}_{a\sim\pi,a^{-}\sim\pi^{-}}[Q(s,a,a^{-})]</script><p>这里可以考虑MARL中一个比较老的方法minimax-Q，minimax-Q在tabular case下是保证收敛的，Q-function每次更新都会稳定地沿着Nash equilibrium移动，且每步更新保证单调性，作者也在Conclusion一节中指出，连续空间下如何解决在Q-function中找Nash equilibrium，是个很有意义的问题</p><h3 id="Multiagent-Soft-Q-Learning"><a href="#Multiagent-Soft-Q-Learning" class="headerlink" title="Multiagent Soft Q-Learning"></a><a href="https://arxiv.org/pdf/1804.09817.pdf" target="_blank" rel="noopener">Multiagent Soft Q-Learning</a></h3><p>以上方法的基础框架都是DDPG，DDPG作为deterministic的RL算法，最显著的特点是给定一个state，其输出的action是deterministic的，用game theory的语言来说就是DDPG学的是一个pure strategy</p><p>问题在于我们知道，multi-agent设定下pure strategy的Nash equilibrium不一定存在，更常见的形式是mixed-strategy，例如两个人玩石头剪刀布，游戏的Nash equilibrium一定是三种动作上的均匀分布，任何一方如果选择pure strategy则一定会被对手发现并找到针对性策略打败</p><p>因此我们可以得到的一个结论是：MARL问题中stochastic actor相比deterministic的actor具有天然的优势，因为stochastic actor可以直接学习到mixed-strategy</p><p>目前stochastic actor的两类主流方法被Berkeley/OpenAI垄断，一类是包括TRPO、PPO在内的利用二阶信息进行优化保证策略迭代稳定性的方法，另一类是包括SQL、SAC在内的最大熵RL方法，作者本文研究的内容就是用SQL来直接学习mixed strategy</p><p>本文的实验内容比较水，最主要的贡献在于如何将SQL引入MARL问题上，作者的方法是让每个agent输出的action概率分布都包含全局所有agent的action，只有自己的action真正用于与环境交互，用强化学习进行训练，而其他agent的action可以用监督学习直接回归得到</p><h3 id="Actor-Attention-Critic-for-Multi-Agent-Reinforcement-Learning"><a href="#Actor-Attention-Critic-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Actor-Attention-Critic for Multi-Agent Reinforcement Learning"></a><a href="https://arxiv.org/abs/1810.02912" target="_blank" rel="noopener">Actor-Attention-Critic for Multi-Agent Reinforcement Learning</a></h3><p>主要的贡献是用self-attention去学习critic应当关注哪些agent，发表于ICML-2019，NIPS-2019的MARL文章还没整理过，目前看到的sota。与上文类似，作者使用SAC这种最大熵正则的方法来直接学习mixed strategy</p><p>Section 3整体写的比较混乱，自己的观点，自己提出的方法，别人方法的问题，以及别人提出的观点与方法混在一起，幸而这篇文章理论性没有很强，想知道作者到底干了什么还是直接去github看代码吧</p><h2 id="DeepMind系列"><a href="#DeepMind系列" class="headerlink" title="DeepMind系列"></a>DeepMind系列</h2><h3 id="A-Unified-Game-Theoretic-Approach-to-Multiagent-Reinforcement-Learning"><a href="#A-Unified-Game-Theoretic-Approach-to-Multiagent-Reinforcement-Learning" class="headerlink" title="A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning"></a><a href="https://arxiv.org/pdf/1711.00832.pdf" target="_blank" rel="noopener">A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning</a></h3><p><img src="./oracle.png" width="98%"></p><h3 id="Open-ended-Learning-in-Symmetric-Zero-sum-Games"><a href="#Open-ended-Learning-in-Symmetric-Zero-sum-Games" class="headerlink" title="Open-ended Learning in Symmetric Zero-sum Games"></a><a href="http://proceedings.mlr.press/v97/balduzzi19a/balduzzi19a.pdf" target="_blank" rel="noopener">Open-ended Learning in Symmetric Zero-sum Games</a></h3><p>DeepMind的力作，发表于ICML-2019，文章很好地从game theory的角度解释了几个重要问题：</p><ul><li>为什么DeepMind做StarCraft要用AlphaLeague和Nash distribution这么麻烦的事情？</li><li>传统的同一模型加载两边阵营的self-play训练方法存在什么问题？</li><li>如何训练出能够打赢不同policy对手的agent？</li></ul><h3 id="Reinforcement-Learning-for-Multi-Objective-and-Constrained-Markov-Decision-Processes"><a href="#Reinforcement-Learning-for-Multi-Objective-and-Constrained-Markov-Decision-Processes" class="headerlink" title="Reinforcement Learning for Multi-Objective and Constrained Markov Decision Processes"></a><a href="http://export.arxiv.org/pdf/1901.08978" target="_blank" rel="noopener">Reinforcement Learning for Multi-Objective and Constrained Markov Decision Processes</a></h3><p>2019年年初挂在Arxiv的文章，technical detail还没看，但abstract提出的问题非常有趣：作者想要解决zero-sum game场景下，一方以RL最大化total discounted reward为目标，另一方以最大化一个bandit问题为目标，作者在Q-learning的基础上拓展，得到了一个可以保证收敛到最优的方法</p><h3 id="UNDERSTANDING-amp-GENERALIZING-ALPHAGO-ZERO"><a href="#UNDERSTANDING-amp-GENERALIZING-ALPHAGO-ZERO" class="headerlink" title="UNDERSTANDING &amp; GENERALIZING ALPHAGO ZERO"></a><a href="https://openreview.net/forum?id=rkxtl3C5YX" target="_blank" rel="noopener">UNDERSTANDING &amp; GENERALIZING ALPHAGO ZERO</a></h3><p>还没来得及看，mark</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper reading </tag>
            
            <tag> Research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes for Multi-Arm Bandit Problems</title>
      <link href="/2019/09/03/multi-arm-bandit/"/>
      <url>/2019/09/03/multi-arm-bandit/</url>
      
        <content type="html"><![CDATA[<p>搬运一些国外讲bandit比较好的博客文章</p><p><em>Note: 与原文相比，数学符号定义会做一些小的修改，使其与RL设定通用</em></p><a id="more"></a><p>bandit类问题可以大致分为三类：</p><ul><li>stochastic：每台老虎机吐钱的概率固定，要找到得到reward最大的那台机器</li><li>adversarial：想象你在和赌场老板玩bandit游戏，赌场老板会在每一轮中调整每台老虎机吐钱的概率，使得你的尽可能少拿钱</li><li>Markovian：每台老虎机吐钱的概率模型是Markovian的</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a><em>References</em></h2><ul><li><a href="https://www.chrisstucchio.com/blog/2014/adversarial_bandit_is_not_statistics_problem.html" target="_blank" rel="noopener">The Adversarial Bandit is not a Statistics Problem</a></li><li><a href="https://jeremykun.com/2013/11/08/adversarial-bandits-and-the-exp3-algorithm/" target="_blank" rel="noopener">Adversarial Bandits and the Exp3 Algorithm</a></li><li><a href="https://banditalgs.com/" target="_blank" rel="noopener">A blog of bandit algorithms</a></li><li><a href="http://web.eecs.umich.edu/~jabernet/eecs598course/fall2015/web/notes/lec22_120315.pdf" target="_blank" rel="noopener">Lecture 22: Adversarial Multi_Armed Bandits</a></li></ul><h2 id="Stochastic"><a href="#Stochastic" class="headerlink" title="Stochastic"></a>Stochastic</h2><h2 id="Adversarial"><a href="#Adversarial" class="headerlink" title="Adversarial"></a>Adversarial</h2><h4 id="1-Setting"><a href="#1-Setting" class="headerlink" title="1. Setting"></a>1. Setting</h4><p><img src="./bandit1.png" width="98%"></p><p>Adversarial bandit可以看做是赌徒与赌场老板玩的一个游戏，与stochastic设定相同，玩家要多次尝试去拉老虎机，以此验证自己对每台老虎机吐钱概率的belief，最小化的目标为自己玩游戏的regret</p><script type="math/tex; mode=display">G(T)=\sum_{t=0}^{T}R_{a_{i}}(t)</script><p>赌场老板的目标是尽可能少让赌徒拿钱，目标有两种设定，一种是最大化赌徒的weak regret，即假设赌徒从头到尾只去尝试吐钱概率最大的老虎机，最终可以得到的reward与实际赌徒得到reward的差值：</p><script type="math/tex; mode=display">\text{Weak Regret}=\max_{a_{t}}\sum_{t=0}^{T}R_{a_{t}}(t)-G(T)</script><p>与之相比，另一种设定是最大化赌徒的regret，即假设赌徒从头到尾只选取最优策略可以拿到的reward与实际赌徒拿到的reward差值</p><script type="math/tex; mode=display">\text{Regret}=\max_{\pi}\mathbb{E}_{a\sim{\pi}}[\sum_{t=0}^{T}R_{a_{t}}(t)]-G(T)</script><p><em>看起来weak regret和regret对于stochastic bandit问题是等价的，因为stochastic bandit中最优的老虎机不会发生变化，reward的概率值也不会发生变化，其optimal policy是greedy的</em></p><h4 id="2-Exp3"><a href="#2-Exp3" class="headerlink" title="2. Exp3"></a>2. Exp3</h4><p>Exp3的全称是 Exponential weight algorithm for exploration and exploitation</p><p>算法的核心思想是对于每个可选择的action维护一组weight，用这些weight来决定下一步中应当选择哪个action，若得到的reward是好的，就让相关的weight增长，反之则降低相关的weight，由此达到根据adversary动态调节policy的目的</p><p>算法还需要一个exploration参数 $\gamma \in [0,1]$， $\gamma$ 越大则Exp3算法会越倾向于用均匀分布来explore，反之则会更加依赖参数 $w$ 来决定探索策略</p><blockquote><p>Given $\gamma \in [0,1]$, initialize the weights $w_i(1) = 1$ for $i = 1$, $\dots, K$;</p><p>In each round t:</p><ol><li>Set $\displaystyle p_i(t) = (1-\gamma)\frac{w_i(t)}{\sum_{j=1}^K w_j(t)} + \frac{\gamma}{K}$ for each $i$;</li><li>Draw the next action $i_t$ randomly according to the distribution of $p_i(t)$;</li><li>Observe reward $R_{i_t}(t)$;</li><li>Define the estimated reward $\hat{R}_{i_t}(t)$ to be $\displaystyle \frac{R_{i_t}(t)}{p_{i_t}(t)}$;</li><li>Set $\displaystyle w_{i_t}(t+1) = w_{i_t}(t) \exp(\frac{\gamma \hat{R}_{i_t}(t)}{K})$;</li><li>Set all other $w_j(t+1) = w_j(t)$;</li></ol></blockquote><h4 id="3-Theoretical-analysis"><a href="#3-Theoretical-analysis" class="headerlink" title="3. Theoretical analysis"></a>3. Theoretical analysis</h4><p>首先分析Exp3的理论期望regret</p><blockquote><p><strong>Theorem 1</strong>: For any $K &gt; 0$, $\gamma \in (0, 1]$, and any stopping time $T \in \mathbb{N}$</p><script type="math/tex; mode=display">G_{\textup{max}}(T) - \mathbb{E}(G_{\textup{Exp3}}(T)) \leq (e-1)\gamma G_{\textup{max}}(T) + \frac{K \log K}{\gamma}.</script></blockquote><p>这个定理给出了Exp3的regret upper bound，我们可以看出</p><ul><li>过大的 $\gamma$ 会导致这个upper bound第一项过大，失去其本来的意义，因为过大的 $\gamma$ 实际代表的是对exploration的倾向性</li><li>当action数量很多时，过小的 $\gamma$ 会导致第二项很大，过小的 $\gamma$ 代表的是对exploitation的倾向性</li></ul><p>综上，从该upper bound的形式即可看出exploration与exploitation之间的矛盾，如何挑选 $\gamma$ 的值是一个重要的问题，因此这里引出第二个引理</p><blockquote><p><strong>Corollary 2</strong>: Assume that $G_{\textup{max}}(T)$ is bounded by $g$, and that Exp3 is run with</p><script type="math/tex; mode=display">\displaystyle \gamma = \min \left ( 1, \sqrt{\frac{K \log K}{(e-1)g}} \right )</script><p>Then the weak regret of Exp3 is bounded by $2\sqrt{(e-1)gK\log K}$ for any reward.</p></blockquote><p>这里注意当reward分配是服从Bernoulli distribution时，这个 $G_{max}(T)$ 的upper bound $g$ 绝大多数情况下都等于 $n$ ，即玩bandit游戏的步数，也就是说Exp3得到的regret关于步数是 $O(\sqrt{(n)})$ 的关系（可以看出adversarial bandit场景下的regret还是要比stochastic场景下 $O(\log(n))$ 级别大很多</p><h2 id="Markovian"><a href="#Markovian" class="headerlink" title="Markovian"></a>Markovian</h2>]]></content>
      
      
      
        <tags>
            
            <tag> Paper reading </tag>
            
            <tag> Research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu 18.04各种折腾</title>
      <link href="/2019/06/29/deepin-wechat/"/>
      <url>/2019/06/29/deepin-wechat/</url>
      
        <content type="html"><![CDATA[<h2 id="在ubuntu-18-04上安装微信的问题"><a href="#在ubuntu-18-04上安装微信的问题" class="headerlink" title="在ubuntu 18.04上安装微信的问题"></a>在ubuntu 18.04上安装微信的问题</h2><a id="more"></a><p>首先安装deepin环境<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://gitee.com/wszqkzqk/deepin-wine-for-ubuntu.git</span><br><span class="line"><span class="built_in">cd</span> deepin-wine-for-ubuntu</span><br><span class="line">sudo sh install.sh</span><br></pre></td></tr></table></figure></p><p>然后去<a href="http://mirrors.aliyun.com/deepin/pool/non-free/d/" target="_blank" rel="noopener">阿里云镜像</a>下载打包的deepin.com.wechat客户端，这里一定要下载最新版，否则会提示版本太低要求升级</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirrors.aliyun.com/deepin/pool/non-free/d/deepin.com.wechat/deepin.com.wechat_2.6.8.65deepin0_i386.deb</span><br><span class="line">sudo dpkg -i deepin.com.wechat_2.6.8.65deepin0_i386.deb</span><br></pre></td></tr></table></figure><p>本来到这里就应该结束了，结果报依赖关系的错误</p><blockquote><p>正在解包 deepin.com.wechat:i386 (2.6.8.65deepin0) …<br>dpkg: 依赖关系问题使得 deepin.com.wechat:i386 的配置工作不能继续：<br> deepin.com.wechat:i386 依赖于 deepin-wine (&gt;= 2.18-19)；然而：<br>系统中 deepin-wine 的版本为 2.18-12。<br> deepin.com.wechat:i386 依赖于 deepin-wine-helper (&gt;= 1.2deepin8)；然而：<br>系统中 deepin-wine-helper:i386 的版本为 1.2deepin0。</p><p>dpkg: 处理软件包 deepin.com.wechat:i386 (—install)时出错：<br> 依赖关系问题 - 仍未被配置<br>正在处理用于 gnome-menus (3.13.3-11ubuntu1.1) 的触发器 …<br>正在处理用于 desktop-file-utils (0.23-1ubuntu3.18.04.2) 的触发器 …<br>正在处理用于 mime-support (3.60ubuntu1) 的触发器 …<br>正在处理用于 hicolor-icon-theme (0.17-2) 的触发器 …<br>在处理时有错误发生：<br> deepin.com.wechat:i386</p></blockquote><p>从报错信息上来看，问题出在最新版wechat需要的deepin最低版本为2.18-19，然而如果我们去<a href="https://gitee.com/wszqkzqk/deepin-wine-for-ubuntu" target="_blank" rel="noopener">社区deepin-wine的gitee地址</a>看的话会发现目前发布的最新版只有2.18-12</p><p>最后参考了<a href="https://github.com/wszqkzqk/deepin-wine-ubuntu/issues/173" target="_blank" rel="noopener">github issue#173</a>的回复</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install deepin.com.wechat</span><br><span class="line"><span class="comment"># apt-get报错信息若干balabala</span></span><br><span class="line">sudo apt --fix-broken install</span><br><span class="line"><span class="comment"># 这一步可以看到apt会将deepin升级到2.18-19版本</span></span><br><span class="line">sudo dpkg -i deepin.com.wechat_2.6.8.65deepin0_i386.deb</span><br><span class="line"><span class="comment"># 安装完成</span></span><br></pre></td></tr></table></figure><p>主要参考 <a href="https://blog.csdn.net/qq_36285997/article/details/89046191" target="_blank" rel="noopener">https://blog.csdn.net/qq_36285997/article/details/89046191</a></p><h2 id="在Ubuntu18-04上安装WPS-office的问题"><a href="#在Ubuntu18-04上安装WPS-office的问题" class="headerlink" title="在Ubuntu18.04上安装WPS office的问题"></a>在Ubuntu18.04上安装WPS office的问题</h2><p>以下两步非必须，目的是卸载系统自带的无用软件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt remove libreoffice-common</span><br><span class="line">sudo apt remove ubuntu-web-launchers</span><br></pre></td></tr></table></figure><p>安装WPS过程比较简单，首先去官网上下载Linux版本的安装包，下载下来应该会是一个 <code>*.deb</code> 文件，直接按照流程安装即可</p><p>问题在于，官网上提供的最新版的安装包安装之后，WPS是打不开的，例如点了docx文件之后，文件并不会被打开，且用 <code>ps -ef | grep wps</code> 发现后台没有应用</p><p>主要的问题在于版本，试过几次以后实测<strong>WPS-Office 10.1.0</strong>版本是没问题可以正常使用的，步骤如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://kdl.cc.ksosoft.com/wps-community/download/6758/wps-office_10.1.0.6758_amd64.deb</span><br><span class="line">sudo dpkg -i wps-office_10.1.0.6758_amd64.deb</span><br></pre></td></tr></table></figure><p>然后发现WPS可以正常打开了，但打开之后会弹窗warning说缺失字体，所以这里需要去<a href="https://pan.baidu.com/s/1mh0lcbY" target="_blank" rel="noopener">网上下载</a>下来WPS需要的几种字体，把压缩包单独放在某个文件夹下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">unzip wps_symbol_fonts.zip</span><br><span class="line">sudo cp *.ttf /usr/share/fonts/</span><br><span class="line">sudo cp *.TTF /usr/share/fonts/</span><br><span class="line">sudo mkfontscale</span><br><span class="line">sudo mkfontdir</span><br><span class="line">sudo <span class="built_in">fc</span>-cache</span><br></pre></td></tr></table></figure><p>再打开WPS发现不在弹窗，问题解决</p>]]></content>
      
      
      
        <tags>
            
            <tag> Manual </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>游戏AI基础知识梳理</title>
      <link href="/2019/06/09/knowledge-distillation-from-openai-deepmind/"/>
      <url>/2019/06/09/knowledge-distillation-from-openai-deepmind/</url>
      
        <content type="html"><![CDATA[<p>世界范围内，做游戏AI的公司主要就是DeepMind和OpenAI，前者有AlphaZero、IMPALA和AlphaStar，后者有OpenAI Five，本文对这两个公司做游戏的主要知识结构做简单梳理</p><a id="more"></a><h2 id="1-PPO的目标函数"><a href="#1-PPO的目标函数" class="headerlink" title="1. PPO的目标函数"></a>1. PPO的目标函数</h2><p>PPO有两种目标函数形式，第一种一般简称adaptive KL</p><script type="math/tex; mode=display">\theta_{k+1}=\arg\max_{\theta}\mathbb{E}_{\pi'}[\sum_{t=0}^{\infty}\gamma^{t}\frac{\pi'(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})}A^{\pi}(s_{t},a_{t})-\beta_{k}D_{KL}(\pi'||\pi_{\theta})]</script><p>第二种一般被称作clipped surrogate</p><script type="math/tex; mode=display">\theta_{k+1}=\arg\max_{\theta}\mathbb{E}_{\pi'}[\sum_{t=0}^{\infty}[\min(\frac{\pi'(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})}A^{\pi}(s_{t},a_{t}),\ \text{clip}(\frac{\pi'(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})},1-\epsilon,1+\epsilon)A^{\pi}(s_{t},a_{t}))]]</script><p>其中</p><ul><li>$\theta$ 是policy模型的参数，$\pi_{\theta}$ 是我们要训练迭代的模型</li><li>$\pi’$ 是迭代之前旧的policy模型，一般做法是初始化两个结构相同的网络，使用$\pi’$ 与环境交互得到的训练数据 (trajectory) 更新 $\pi$ ，若干步后把 $\pi$ 的参数全部copy给 $\pi’$ </li><li>$A^{\pi}(s_{t},a_{t})=Q^{\pi}(s_{t},a_{t})-V^{\pi}(s_{t})=R_{t}+\gamma{V^{\pi}(s_{t+1})}-V^{\pi}(s_{t})$ 是advantage function</li><li>$\epsilon$ 一般取0.1之类的</li><li>$D_{KL}(\pi’||\pi)=\mathbb{E}_{s_{t}\sim{d^{\pi’}(s)}}\mathbb{E}_{a_{t}\sim\pi’}[\log(\frac{\pi’(a_{t}|s_{t})}{\pi(a_{t}|s_{t})})]$  ，就是常说的KL divergence。对于离散空间直接两个交叉熵除一下即可；对于连续空间一般会采用reparameterization-trick将网络参数化成一个Gaussian distribution （就是让网络输出两个向量一个代表 $\mu$ 一个代表 $\sigma$ 然后从中采样），两个Gaussian之间的KL有闭式解<script type="math/tex; mode=display">  D_{KL}(\mathcal{N}(\mu_{1},\sigma_{1},\mathcal{N}(\mu_{2},\sigma_{2})))=\log(\frac{\sigma_{2}}{\sigma_{1}})+\frac{\sigma_{1}^{2}+\mu_{2}-\mu_{1}}{2\sigma_{2}^{2}}-\frac{1}{2}</script></li><li>$V^{\pi}(s)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}\gamma^{t}R_{t}]$ ，等号右边这个东西叫做 total discounted reward，是所有强化学习算法优化的最终目标，一般参数化成价值网络的形式，直接用监督学习训练，近几年的强化学习算法普遍采用GAE来估计 total discounted reward</li></ul><h3 id="1-1-设计这种目标函数的目的"><a href="#1-1-设计这种目标函数的目的" class="headerlink" title="1.1. 设计这种目标函数的目的"></a>1.1. 设计这种目标函数的目的</h3><ul><li>这两种目标函数的目的都是为了近似自然梯度 $\tilde{g}=F^{-1}g=(\nabla^{2}_{\theta}KL(\pi’||\pi_{\theta}))^{-1}g$ ， 式中的 $F$ 是 Fisher information matrix， 由 $F$ 可以确定一个在概率空间中具有不变性的黎曼度量，使得 $F^{-1}g$ 是逆变向量，i.e., 由 $F^{-1}g$ 所确定的自然梯度与 $\pi_{\theta}$的参数化形式无关，因而拥有较小的训练方差</li><li>在bounded KL范围内迭代可以有单调提升的（弱）bound：<script type="math/tex; mode=display">      J(\pi_{\theta})-J(\pi')\geq \mathbb{E}_{\pi'}[\sum_{t=0}^{\infty}\gamma^{t}\frac{\pi'(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})}A^{\pi}(s_{t},a_{t})]-\frac{4\gamma\max_{s,a}A^{\pi}(s,a)}{(1-\gamma)^{2}}\mathbb{E}_{s\sim{d^{\pi'}}}[D_{KL}(\pi'||\pi_{\theta})]</script>  i.e., 在 bounded KL ball 中迭代policy可以保证收敛的稳定</li><li>off-policyness：可以用从 $\pi’$ 中采样得到的trajectory优化 $\pi_{\theta}$，这样做有利于实现分布式计算框架，但其实可以想到这个迭代速度不会很快，因为PPO的目标函数形式限制了每步迭代中 $D_{KL}(\pi’||\pi_{\theta})$ 的大小，且每次更新完以后都要把 $\pi_{\theta}$ 的参数copy回去给 $\pi’$</li></ul><p>个人经验，训练时要尤其关注KL的变化，KL bound得比较好的话，policy的improvement是有理论保障的；反之如果bound的不好，有时会出现 policy 的退化现象，越训练越差</p><h3 id="1-2-References"><a href="#1-2-References" class="headerlink" title="1.2. References"></a>1.2. References</h3><ol><li><a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="noopener">Trust Region Policy Optimization</a></li><li><a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">Proximal Policy Optimization</a></li></ol><h2 id="2-GAE的简单解释"><a href="#2-GAE的简单解释" class="headerlink" title="2. GAE的简单解释"></a>2. GAE的简单解释</h2><p>全称 generalized advantage estimator，出自论文<a href="https://arxiv.org/abs/1506.02438" target="_blank" rel="noopener">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>，in a word，其最终目的是在 advantage function $A(s,a)=Q(s,a)-V(s)$ 的各种估计方式估计中找一个bias-variance tradeoff 的平衡点</p><p><img src="https://danieltakeshi.github.io/assets/gae_paper_pg_basics.png" width=95%></p><p>以上六种policy gradient的形式中，3拥有最小的理论方差，但实际计算中由于复杂度问题一般会采用5</p><p>一种估计 $A^{\pi}(s_{t},a_{t})$ 的方法是把整个trajectory的reward都考虑在内,这种估计方式有较小的bias和较大的variance：</p><script type="math/tex; mode=display">    \hat{A}^{\pi}(s_{t},a_{t})=\sum_{t=0}^{T}\gamma_{t}R_{t}+\gamma^{T+1}V(s_{T})</script><p>另一种方式是利用已有的 $V(s)$ 函数进行辅助估计，这种方法有较大的bias与较小的variance：</p><script type="math/tex; mode=display">    \hat{A}(s_{t},a_{t})=R_{t}+\gamma V(s_{t+1})-V(s_{t})</script><p>也可以构造出介于两者之间的形式，总结为：</p><script type="math/tex; mode=display">    \hat{A}_t^{(1)} = R_t + \gamma V(s_{t+1}) - V(s_t) \\    \hat{A}_t^{(2)} = R_t + \gamma R_{t+1} + \gamma^2 V(s_{t+2}) - V(s_t) \\    \hat{A}_{t}^{(n)} = \sum_{k=0}^{n-1}\gamma^{k-1}R_{t+k}+\gamma^{n}V(s_{t+n})-V(s_{t}) \\     \hat{A}_t^{(\infty)} = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots - V(s_t)</script><p>在以上所有形式的advantage estimator中取bias-variance平衡点的方法，就是使用从 $\hat{A}_{t}^{(1)}$ 到 $\hat{A}_{t}^{(\infty)}$ 的几何平均：</p><script type="math/tex; mode=display">\hat{A}_t^{GAE(\gamma,\lambda)} = (1-\lambda)\Big(\hat{A}_{t}^{(1)} + \lambda \hat{A}_{t}^{(2)} + \lambda^2 \hat{A}_{t}^{(3)} + \cdots \Big) = \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l}^{V}</script><h2 id="3-AlphaStar-Mastering-the-Real-Time-Strategy-Game-StarCraft-II"><a href="#3-AlphaStar-Mastering-the-Real-Time-Strategy-Game-StarCraft-II" class="headerlink" title="3. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II"></a>3. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II</h2><p><img src="https://storage.googleapis.com/deepmind-live-cms-alt/images/SCII-BlogPost-Fig03.width-1500.png" width="95%"></p><p>DeepMind用来做星际2的框架，里面包含的内容非常多</p><ul><li>模型采用off-policy的actor critic，加experience replay、self-imitation learning以及policy distillation</li><li>为保证策略的多样性，先用SL训练一个baseline的模型（上图001），然后在每段iteration开始时,从前一轮iteration的模型中copy几个相同的出来进行自对弈，每个模型的超参都不一样，甚至reward定义都不一样，用PBT训练。上一轮迭代的模型不再更新，称之为frozen competitor，采用和人类玩家天梯匹配系统类似的方式设计自对弈中的对手匹配系统</li><li>用了transformer结构输出每个unit的action，结合了pointer network以及centralized value baseline</li></ul><h3 id="3-1-Population-based-training-of-neural-networks"><a href="#3-1-Population-based-training-of-neural-networks" class="headerlink" title="3.1. Population based training of neural networks"></a>3.1. Population based training of neural networks</h3><p>简称PBT，出自DeepMind论文<a href="https://arxiv.org/pdf/1711.09846.pdf" target="_blank" rel="noopener">Population based training of neural networks</a>，见<a href="https://deepmind.com/blog/population-based-training-neural-networks/" target="_blank" rel="noopener">DeepMind博客地址</a></p><p>本质就是用genetic algorithm的思路来做hyper-parameter tuning：同时训练N个模型，每个模型有自己的一套超参，训练一段时间，取部分效果较好的模型，在其超参基础上做一些外延探索，并继续训练模型</p><p>这样的做法也可以防止模型陷入局部最优</p><h3 id="3-2-Policy-distillation"><a href="#3-2-Policy-distillation" class="headerlink" title="3.2. Policy distillation"></a>3.2. Policy distillation</h3><p><img src="https://pic3.zhimg.com/v2-215b36a5c9babb7fc2ee221d92583164_1200x500.jpg" width="95%"></p><p>Policy distillation的原始论文中，teacher是DQN，这一点和我们这边差别很大，我们一般用policy gradient类的方法可以直接蒸馏，不存在作者文章中讨论的各种不同的loss问题</p><p>对于student网络的蒸馏，作者在文章中试验了三种不同的loss</p><ol><li>将student参数化为 $\pi_{\theta}:\mathcal{S}\rightarrow{\mathcal{A}}$ 的形式，直接从DQN的replay memory中拿之前的数据出来当做action的标签</li><li>将student参数化为 $Q:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ 的形式，loss用均方误差</li><li>将student参数化为 $Q:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ 的形式，将student和teacher网络输出的Q值都过一个softmax，然后用Hinton论文里的KL散度作为loss</li></ol><p>实验结果表明第三种loss效果最好</p><h3 id="3-3-Centralized-value-baseline"><a href="#3-3-Centralized-value-baseline" class="headerlink" title="3.3. Centralized value baseline"></a>3.3. Centralized value baseline</h3><p>出自DeepMind发表在AAAI-2018的文章 <a href="https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/foersteraaai18.pdf" target="_blank" rel="noopener">Counterfactual Multi-Agent Policy Gradients</a>，idea很简单，就是对于multi-agent问题场景，<strong>所有的actor共享一个critic</strong></p><p>这个idea很像是OpenAI那篇 <a href="https://papers.nips.cc/paper/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.pdf" target="_blank" rel="noopener">Multi-agent actor critic</a> 的弱化版，OpenAI那篇好歹认真考虑了 multi-agent 设定下存在的 non-stationary MDP 问题，DeepMind这篇直接无视掉了这一点，侧面说明可能理论上存在的 non-stationary MDP 问题实际在工程上并不影响模型效果</p><h3 id="3-4-Transformer"><a href="#3-4-Transformer" class="headerlink" title="3.4. Transformer"></a>3.4. Transformer</h3><p>以下内容参考了博客 <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a> 与 <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Google Brain的原始paper “Attention is All You Need”</a></p><p>Transformer是BERT的基础，NLP任务从此开始走上了【去RNN】化的道路</p><p>简单来说，这篇文章只用attention和feed-forward network就在很多任务上取得了很好的效果，主要有几部分组成</p><ul><li>Self-attention, encoder-decoder attention, and multi-head attention</li><li>Feed-forward network</li><li>Positional encoding</li><li>Residual blocks</li></ul><p><strong>首先解释 self-attention</strong></p><p><img src="https://jalammar.github.io/images/t/Transformer_decoder.png" width="95%"></p><p>其中self-attention形式为</p><script type="math/tex; mode=display">    Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V \\    \text{where}\ Q=W_{Q}X, K=W_{K}X, V=W_{V}X</script><p>一图胜千言<br><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" width="95%"></p><p><strong>然后是encoder-decoder attention</strong></p><p>这个attention和之前seq2seq中的attention其实是一样的</p><blockquote><p>The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence</p></blockquote><p><strong>Multi-head attention:</strong> 说白了就是在同一层堆叠多个self-attention，作用主要有两点</p><blockquote><ul><li>It expands the model’s ability to focus on different positions</li><li>It gives the attention layer multiple “representation subspaces”</li></ul></blockquote><p><strong>Positional Encoding</strong></p><p>此外transformer网络中还用到了residual block来防止由于网络过深导致梯度退化</p><p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png" width="95%"></p><h2 id="4-MOBA游戏AI中一些难做的问题"><a href="#4-MOBA游戏AI中一些难做的问题" class="headerlink" title="4. MOBA游戏AI中一些难做的问题"></a>4. MOBA游戏AI中一些难做的问题</h2><p>提纲</p><ul><li>SL的问题<ul><li>distributional shift</li><li>multi-modal policy</li></ul></li><li>interaction with environment的成本问题</li><li>GAIL系列工作：高方差，训练不稳定<ul><li>高方差</li></ul></li></ul><h3 id="4-1-Imitation-learning"><a href="#4-1-Imitation-learning" class="headerlink" title="4.1. Imitation learning"></a>4.1. Imitation learning</h3><p>SL/behavior cloning对于实际应用场景而言是最简单快捷的方法，然而由于distributional shift问题的存在，这种方法在MDP上没有任何理论保障，即使你是在应用场景下不在乎理论性质，除非expert demonstration的state可以铺满整个MDP的state空间，否则学出来的agent大概率是没有任何泛化能力的</p><p>另一个问题是对于连续动作空间的multi-modal policy，Sergey Levine在上CS294-112课程时曾经给出一张非常形象生动的图片来说明这个问题</p><p><img src="./multi-modal.png" width="95%"></p><p>网上很多人吐槽过王者荣耀的AI会在塔前有规律地反复徘徊，凭借这点可以很轻松地判断出一个player是AI还是人类——这个问题很大程度上就来源于multi-modal policy。连续空间的regression会倾向于学习到训练数据中的均值，然而mutli-modal policy下均值可能并不是一个好的policy</p><p>那么是否可以通过把连续的动作空间强行切成离散的来避免这个问题呢？In theory，这样做会破坏掉MDP中action<br>space本来的性质，其次这样的做法可能会带来额外的高方差 <a href="https://chenshawn.github.io/2019/03/27/pg-variance/" target="_blank" rel="noopener">[参见另一篇blog]</a>，但是in practice这样的做法其实效果不差</p><p>此外一个根深蒂固但大部分时间里都被RL研究者们无视的问题是与environment交互的成本与效率问题，我们在gym或Atari上做实验的时候可以无限采样，效率低就开多线程或者多进程跑，然而现实中很多场景下，agent与environeent交互都是受制于各种成本的。目前为止几乎所有的方法都必须要和environment去做交互，才可以保障agent的稳定训练。在此基础上，2018年AILab发在NIPS的文章<a href="https://papers.nips.cc/paper/7866-exponentially-weighted-imitation-learning-for-batched-historical-data" target="_blank" rel="noopener">Exponentially Weighted Imitation Learning for Batched Historical Data</a>另辟蹊径地提出了一个问题：如果我们有很多的triplet形式的historical data $(s_{t},a_{t},r_{t})$，但无法与environment做交互，是否还可以做imitation learning呢？</p><p>如果我们用behavior cloning的方法直接训练，实际上很大程度上浪费掉了训练数据中reward signal中包含的信息，这篇文章的方法是用$\exp(\beta{A(s_{t},a_{t})})$来对每条训练数据加权，从直觉上来讲，这样的做法会鼓励模型去更多地学习带来带来高advantage值的动作。不可思议的是，这么简单一个方法居然有theoretical improvement guarantee</p><p>文章最后提到这种方法也可以用来做纯RL模式的学习，即没有人类数据的情况下，用agent与环境交互的数据训练agent，然后再把agent放到环境里交互，这种方法是纯off-policy的，我没有看出来这种off-policy的方法比SAC，IMPALA之类的方法好在哪里</p><p>Imitation learning中另一类不得不提的方法是GAIL，这个方法自从被提出之后一直在research中备受关注，其中一个原因在于其理论框架非常有美感：原始GAIL文章中，作者针对inverse RL需要在学习reward的内循环中训练RL agent问题，理论证明了$IRL(RL(\pi,R))$是occupancy measure matching的对偶问题，因而imitation learning可以直接套用GAN的框架，通过交替迭代policy与discriminator来优化；后续AIRL中又证明了当特定参数化形式的discriminator下（类似于reward shaping的形式），AIRL可以在训练中逐渐恢复出真正的reward+一个constant；2017年大热的WGAN和WGAN-GP也为GAIL类方法的训练不稳定问题提出了新的解决方案；继info-GAN之后的info-GAIL结合了PGM与GAIL，为unsupervised inference of expert demonstration提供了信息论基础</p><p>然而真正落地的application中，据我所知完全没人用这类看起来很高大上的方法，原因在于：</p><p>第一，这类Imitation learning方法几乎从未outperform hand-crafted reward，哪怕有时这种hand-crafted reward很粗糙。在ICLR-2019的variational information bottleneck文章中，作者总结</p><blockquote><p>One advantage of adversarial methods is that by leveraging a discriminator in<br>place of a reward function, they can be applied to imitate skills where reward functions can be<br>difficult to engineer. However, the performance of policies trained through adversarial methods still<br>falls short of those produced by manually designed reward functions.</p></blockquote><p>第二，GAN本身就是出了名的不稳定难训练，而这种不稳定再加上RL的高方差、高样本复杂度问题，可以说是把近年来机器学习领域内最令人头疼的问题汇集到了同一套算法中。在此基础上加时序套RNN是一个简单但通用的思路</p><p>第三，representation问题</p><h3 id="4-2-Hybrid-action-space"><a href="#4-2-Hybrid-action-space" class="headerlink" title="4.2. Hybrid action space"></a>4.2. Hybrid action space</h3><ul><li>Hierachical RL: unsupervised inference of macro-strategy from micro-policy</li><li>Representation of state space</li><li>Action embedding?</li></ul><p>这点上MOBA-games与mujoco有点类似：action space不是一个动作，而是很多个动作的组合。例如在王者荣耀中某一时刻下的action，有的维度代表的是放哪个技能，有的维度代表的是方向型技能的施法方向，有的维度代表的是当前英雄的移动方向</p><h3 id="4-3-Reward"><a href="#4-3-Reward" class="headerlink" title="4.3. Reward"></a>4.3. Reward</h3><h4 id="4-3-1-Long-term-delayed-reward"><a href="#4-3-1-Long-term-delayed-reward" class="headerlink" title="4.3.1. Long term delayed reward"></a>4.3.1. Long term delayed reward</h4><p>如果用类似AlphaGo的方式，只采用最后输赢作为binary reward，每个trajectory将会非常非常长，可以参考<a href="https://openai.com/blog/openai-five/" target="_blank" rel="noopener">OpenAI这篇blog</a>，里面有提到他们用这种binary reward来训练1v1的bot</p><blockquote><p>We ran an experiment where we only rewarded the agent for winning or losing, and it trained <strong>an order of magnitude</strong> slower and somewhat plateaued in the middle, in contrast to the smooth learning curves we usually see.</p></blockquote><p><img src="https://openai.com/content/images/2018/06/sparse-vs-dense-small@2x.png" width="90%"></p><p>与之对立，图中橙色dense的曲线对应的方法是人工定义reward。虽然我目前还没做过这方面的工作，想来应该是非常琐碎复杂的——因为reward是RL最终优化的目标，reward稍微改变一点点都可能会完全改变MDP优化的整体结构</p><h4 id="4-3-2-Reward-definition-in-OpenAI-Dota2-program"><a href="#4-3-2-Reward-definition-in-OpenAI-Dota2-program" class="headerlink" title="4.3.2. Reward definition in OpenAI Dota2 program"></a>4.3.2. Reward definition in OpenAI Dota2 program</h4><p>在OpenAI Dota2项目中，reward定义有几个trick</p><ul><li><strong>Team spriit:</strong> 每个英雄的reward都会加上其他英雄的reward平均值乘以一个系数  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">camp_hero_reward[i] = camp_hero_reward[i] + coeff * (sum(camp_hero_reward) - camp_hero_reward[i])</span><br></pre></td></tr></table></figure></li><li><strong>Zero-sum:</strong> 为了保证双方绝对不可能存在合作的可能性，设定双方的reward加起来恒等于0  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">camp_hero_reward[i] -= mean(enemy_hero_reward)</span><br></pre></td></tr></table></figure></li><li><strong>Time decay:</strong> 比赛后期的reward可能比比赛前期的大很多，因为英雄装备起来以后刷钱速度提升，团战频率也大了很多，如果不对比赛不同阶段的reward做特殊处理，可能模型会特别重视后期的reward，而后期的reward很大程度上源于前期的积累，因此将前期的reward和后期的reward做一个平衡</li></ul><p>为什么我们一定要在reward的层面保障双方绝对不能有合作的可能性呢？我们知道博弈分为合作博弈和零和博弈：对于zero-sum game，最终的优化目标就是双方收敛到Nash equilibiurm，在这个收敛处双方都达到了彼此的最优；而对于合作博弈来说情况要复杂很多，一个经典的问题就是“囚徒困境”：</p><blockquote><p>Two prisoners are accused of a crime. If one confesses and the other does not, the one who confesses will be released immediately and the other will spend 20 years in prison. If neither confesses, each will be held only a few months. If both confess, they will each be jailed 15 years. They cannot communicate with one another.</p></blockquote><p>这个例子的核心在于，合作博弈中，如果双方无法互相沟通（双方都看不到全局的信息），那么很可能最后双方都会掉进自己的suboptimality——明明两个囚徒的全局最优是都不认罪，然而由于他们互相无法沟通，最后他们都会选择他们自己的局部最优认罪，即使这个策略是次优的</p><p>这个例子与游戏场景是高度一致的，如果两个队伍存在合作关系却缺少全局信息的共享（如视野），那么可能两个队伍的policy都是suboptimal的</p><p>相比之下zero-sum game的性质就会好很多，Nash equilibrium existence theorem告诉我们：</p><blockquote><p>Nash equilibiurm exists for any mixed-strategy zero-sum game with a finite set of actions.</p></blockquote><p>我们很容易通过一些工程上的调整来使得我们的问题满足Nash equilibrium existence theorem的条件</p><h4 id="4-3-3-Understanding-self-play-reinforcement-learning-games"><a href="#4-3-3-Understanding-self-play-reinforcement-learning-games" class="headerlink" title="4.3.3. Understanding self-play reinforcement learning games"></a>4.3.3. Understanding self-play reinforcement learning games</h4><p>近年来，自对弈学习在强化学习中已经有非常广泛的应用，例如AlphaGo系列和OpenAI的Dota2项目，然而有关强化学习中self-play理论解释的工作仍然非常少，大部分人只是在多人游戏场景下比较empirical地用这种方法，从而避免采样过程需要人类参与</p><p>目前已知的一篇文章是<a href="https://openreview.net/forum?id=rkxtl3C5YX&amp;noteId=rkxtl3C5YX" target="_blank" rel="noopener">Understanding &amp; Generalizing AlphaGo Zero</a>，这篇文章之前投稿到ICLR-2019，尝试证明AlphaGo自对弈的过程最终可以收敛到Nash equilibrium，这个思路真的非常有趣，可惜被reject掉了。最后meta-review总结如下</p><blockquote><p>This work examines the AlphaGo Zero algorithm, a self-play reinforcement learning algorithm that has been shown to learn policies with superhuman performance on 2 player perfect information games.  The main result of the paper is that the policy learned by AGZ corresponds to a Nash equilibrium, that and that the cross-entropy minimization in the supervised learning-inspired part of the algorithm converges to this Nash equillibrium, proves a bound on the expected returns of two policies under the and introduces a “robust MDP” view of a 2 player zero-sum game played between the agent and nature.</p><p>R3 found the paper well-structured and the results presented therein interesting. R2 complained of overly heavy notation and questioned the applicability of the results, as well as the utility of the robust MDP perspective (though did raise their score following revisions).</p><p>The most detailed critique came from R1, who suggested that the bound on the convergence of returns of two policies as the KL divergence between their induced distributions decreases is unsurprising, that using it to argue for AGZ’s convergence to the optimal policy ignores the effects introduced by the suboptimality of the MCTS policy (while really interesting part being understanding how AGZ deals with, and whether or not it closes, this gap), and that the “robust MDP” view is less novel than the authors claim based on the known relationships between 2 player zero-sum games and minimax robust control. </p><p>I find R1’s complaints, in particular with respect to “robust MDPs” (a criticism which went completely unaddressed by the authors in their rebuttal), convincing enough that I would narrowly recommend rejection at this time, while also agreeing with R3 that this is an interesting subject and that the results within could serve as the bedrock for a stronger future paper.</p></blockquote><p>个人看法：reviewer2质疑的applicability纯属扯淡，本来self-play问题上的理论工作就非常少，这时出现了一篇文章告诉我们应该如何去理解self-play，你还要求这篇文章有applicability；相比之下，reviewer1提出的质疑在于，MCTS学到的policy是suboptimal的，文章中有关Nash equilibrium的证明无视了这一点，这个问题可谓一针见血，应该也是这篇文章被reject的最主要原因</p><p>更具体的审稿过程可以点开上面的链接看OpenReview，或许后面会更仔细地读下这篇文章写个blog</p><h3 id="4-4-Adversarial-robustness"><a href="#4-4-Adversarial-robustness" class="headerlink" title="4.4. Adversarial robustness"></a>4.4. Adversarial robustness</h3><p><a href="https://openai.com/blog/openai-five/" target="_blank" rel="noopener">OpenAI有关他们Dota2项目的blog里面</a>提到了若干他们的方法中用到的trick，其中值得一提的是他们的agent在1v1比赛中击败人类的过程：</p><blockquote><p>In March 2017, our first agent defeated bots but got confused against humans. </p></blockquote><p>很容易想到，传统的脚本AI会输给人类的主要原因就在于AI的行为模式是比较单一的，由于人类更擅长从AI的行为模式中总结与推理，一旦人类玩家摸清楚了AI的行为规律，接下来的时间里就可以轻松打败AI</p><blockquote><p>To force exploration in strategy space, during training (and only during training) we randomized the properties (health, speed, start level, etc.) of the units, and it began beating humans. Later on, when a test player was consistently beating our 1v1 bot, we increased our training randomizations and the test player started to lose. </p></blockquote><p>OpenAI还指出这种randomization的技巧也被他们的机器人团队广泛采用</p><p>经验上考虑的话，这种方法的有效性是值得商榷的，因为这意味着我们要在高维连续的空间中进行<strong>各向同性的</strong>随机搜索，意味着通过这样的randomization的得到state，势必有很多都是无效的，不属于问题本来所处的MDP空间里。对“各向同性”进行修正的话，一个思路是结合adversarial training，举个例子，原本的RL目标可以写成discounted future state distribution下对reward的数学期望：</p><script type="math/tex; mode=display">    \arg\max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}}[\sum_{t=0}^{\infty}\gamma^{t}R_{t}]=    \arg\max_{\pi_{\theta}}\frac{1}{1-\gamma}\mathbb{E}_{s\sim{d^{\pi_{\theta}}}}[\mathbb{E}_{a\sim\pi_{\theta}}[R(s,a)]]</script><p>那么distributional adversarial learning的思路是在一个probability metric space中允许discounted future state distribution在一个bounded subset中最小化reward</p><script type="math/tex; mode=display">    \max_{\pi_{\theta}}\min_{d'}\frac{1}{1-\gamma}\mathbb{E}_{s\sim{d'}}[\mathbb{E}_{a\sim\pi_{\theta}}[R(s,a)]]\quad \text{s.t.}\ D(d', d^{\pi_{\theta}})\leq{\delta}</script><p>其中 $D(.,.)$ 为任意符合距离定义的distance measure，因此我们会关心在什么样的distance measure下上面的minimax问题为tractable，可以考虑的两个思路</p><ul><li>Jensen Shannon divergence：或许需要引入FIM来解决一些问题，配合PPO类的方法或许会有新的insight出现</li><li>Wasserstein distance：这个确定是可以推出可行解的，我们可以比较容易地在Sinha et al. 给出的结果上引申：</li></ul><script type="math/tex; mode=display">    \sup_{d'}\mathbb{E}_{s\sim{d'}}[\mathbb{E}_{a\sim\pi_{\theta}}[-R(s,a)]]-\alpha D(d',d^{\pi_{\theta}}) \\    =\sup_{s'\sim{d'}}\mathbb{E}_{d^{\pi_{\theta}}}[\mathbb{E}_{a\sim\pi_{\theta}}[-R(s',a)] - \alpha{c(s, s')}] \\    \approx \inf_{s'}\mathbb{E}_{\pi_{old}}[\sum_{t=0}^{\infty}\gamma^{t}\frac{\pi_{old}(a_{t}|s_{t})}{\pi_{\theta}(a_{t}|s_{t})}A^{\pi_{\theta}}(s'_{t},a_{t})+\alpha{c(s,s')}]</script><p>其中 $c(.,.)$ 为Wasserstein distance中的cost function，约等号的形式可以由TRPO中的 monotonic improvement lower bound 推出</p><p>但这个优化在应用的时候存在一个问题：我们采用优化的方式而不是纯随机化，是为了让模型在explore MDP中的未知世界时，对那些会对自身造成很大影响的state做专门的优化；但问题在于RL问题中，我们实际上并没有direct access to the discounted future state distribution，实际的adversarial RL问题中，$d^{\pi_{\theta}}$并不会自己改变，真正会改变的因素包括</p><ul><li>Oppnent strategy: 如果对手是人类玩家，人类玩家会从你的policy中推理与总结，并根据你的policy动态地调整policy</li><li>The bias between the game simulator and reality: 常见于robotics问题中，做robotics的人有时会遇到的一个问题是模型在simulator上跑的很好，但在现实世界中效果却不尽人意</li><li>Non-stationary MDP: 常见于推荐系统场景，虽然现在用RL做推荐的组很多，也有很多模型已经上线，但其实真正应用场景下的transition与训练时的transition大部分情况下是不相同的，推荐场景下的state distribution一般是会动态变化的</li></ul><p>总结，以上三种情况的数学本质，是MDP中的transition kernel $P(s_{t+1}|s_{t},a_{t})$ 会变，而RL问题的基本假设决定了我们无法直接获取任何 transition kernel 的知识（不考虑mode-based RL）</p><p>这种情况下，通过优化discounted future state distribution的minimax对偶，以此期望模型可以更加鲁棒，这种做法类似于隔靴搔痒</p><p>那么怎样做可以使得Wasserstein distance中的 cost function更有意义呢？</p><h3 id="4-5-Model-Compression"><a href="#4-5-Model-Compression" class="headerlink" title="4.5. Model Compression"></a>4.5. Model Compression</h3><p>在学校写的模型代码往往比较简单，不需要考虑线上部署，但公司里会遇到需要部署的实际问题，一般来说有两种部署形式：</p><ul><li>后台部署：问题在于请求响应时需要用RPC的方式调用后台服务，而每台机器可能部署了不止一个模型，当每次请求用到的的模型不同时，系统需要不停地在不同的模型之间切换，内存换页触发次数过多，会严重影响效率，如果模型比较小的话这种现象会相对缓解一些，</li><li>前端部署：对于我实习的公司来说，前端部署绝大多数时间指的是直接部署在移动端设备上，e.g. 手机，iPad，考虑到移动端设备的计算能力，这对神经网络模型大小有比较苛刻的要求，网络不能太复杂，因此就需要模型压缩</li></ul><p>用到的两种方法，包括distillation和quantization，在<a href="https://pocketflow.github.io/distillation/" target="_blank" rel="noopener">PocketFlow的github主页</a>都有详细讲解</p><h3 id="Distillation"><a href="#Distillation" class="headerlink" title="Distillation"></a>Distillation</h3><p>Distillation出自Hinton老爷子2015年的文章<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>，目的是通过一个小的神经网络学习到大网络的输出分布，也就是所谓的soft-target</p><p><strong>Q1:</strong> What is the benefit for learning the soft-target given by larger network models instead of directly training a smaller network using hard-coded labels?</p><p>从AlexNet到VGG，从ResNet到DenseNet，过去大量的empirical evidence表明，大网络最终通过学习收敛到的optima要比小网络的optima有更好的generalization ability，小网络直接在大数据集上训练很容易很早地卡到一个local minimum中，这种情况下，让小网络学习soft-target可以使loss surface更稳定，容易学习</p><p>另一个empirical evidence是，大网络经过训练后，其soft-target中往往包含有比较多的信息，e.g. 在ImageNet的网络中，相近的几种label往往都可以获得相对比较高的probability</p><p><strong>Q2:</strong> What is the significance of using the temperature hyper-parameter?</p><p>考虑为什么这个方法会被叫做“蒸馏”？所谓蒸馏，先提高温度让我们想要的目标从混沌中分离，然后再降低温度让目标在目的地聚集，训练中实际也这样做的</p><p>问题：</p><ul><li>训练过程中发现，即使我们在测试的时候完全不加exploration，i.e., action永远取probability最大的哪一个， $T$ 的值却会改变模型输出的概率分布，模型输出的概率分布确实呈现了 $T$ 越大分布越soft的趋势，为什么？</li><li>实做的时候发现，teacher模型训练时需要对输入数据分布做一些人工的调整，从而避免训练数据带来的inductive bias，但student模型则完全不需要，i.e., Assume the data distribution is $p(x,y)$, the student model can learns to model $p(y|x)$ given by the teacher model even if $p(x)$ is completely different, why?</li></ul><h3 id="4-6-Quantization"><a href="#4-6-Quantization" class="headerlink" title="4.6. Quantization"></a>4.6. Quantization</h3><p>这方面我知之甚少，大意是将原本网络的float32类型参数替换成uint8之类的东西，从而极大地提高模型效率并降低模型大小</p><p>具体做的时候用到了tfLite</p>]]></content>
      
      
      
        <tags>
            
            <tag> Research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Details of Generalized Advantage Estimator</title>
      <link href="/2019/04/18/generalized-advantage-estimator/"/>
      <url>/2019/04/18/generalized-advantage-estimator/</url>
      
        <content type="html"><![CDATA[<p>记录一点点GAE的细节</p><a id="more"></a><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://arxiv.org/abs/1506.02438" target="_blank" rel="noopener">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a></li><li><a href="https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/" target="_blank" rel="noopener">Notes on the Generalized Advantage Estimation Paper</a></li></ul><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_gae</span><span class="params">(next_value, rewards, masks, values, gamma=<span class="number">0.99</span>, tau=<span class="number">0.95</span>)</span>:</span></span><br><span class="line">    <span class="string">"""compute_gae</span></span><br><span class="line"><span class="string">        next_value: type float, the value estimation for the last terminated state</span></span><br><span class="line"><span class="string">        rewards: type list, the rewards obtained via directly interacting with environment</span></span><br><span class="line"><span class="string">        masks: type list, with value 0 for terminated states and 1 for otherwise</span></span><br><span class="line"><span class="string">        values: type list, estimated using value function estimator</span></span><br><span class="line"><span class="string">        tau: corresponding to the hyper-parameter lambda in the paper</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    values = values + [next_value]</span><br><span class="line">    gae = <span class="number">0</span></span><br><span class="line">    returns = []</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> reversed(range(len(rewards))):</span><br><span class="line">        delta = rewards[step] + gamma * values[step + <span class="number">1</span>] * masks[step] - values[step]</span><br><span class="line">        gae = delta + gamma * tau * masks[step] * gae</span><br><span class="line">        returns.insert(<span class="number">0</span>, gae + values[step])</span><br><span class="line">    <span class="keyword">return</span> returns</span><br></pre></td></tr></table></figure><p>然而文章中推导的最终形式为</p><script type="math/tex; mode=display">\hat{A}_{t}^{GAE(\gamma,\lambda)}=\sum_{l=0}^{\infty}(\gamma\lambda)^{l}\delta_{t+l}^{V}</script><p>一眼看过去并不容易看出来这个implemention与GAE理论形式的关系，这里用的技巧是通过反向累加</p><script type="math/tex; mode=display">A_{t}^{GAE(\gamma,\lambda)}=\delta_{t}+\gamma\lambda A_{t+1}^{GAE(\gamma,\lambda)}</script><h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><p>Bias-variance tradeoff using hyperparameter $\lambda$</p><ul><li>$\lambda$ closed to 1 leads to high variance and low bias</li><li>$\lambda$ closed to 0 leads to low variance and high bias</li></ul><p>More specifically,</p><ul><li>when $\lambda=1$, advantage function == total gain $A_{t}^{GAE(\gamma,1)}=\sum_{l=t}^{\infty}[\gamma^{l-t}R_{l}]-V(s_{t})$</li><li>when $\lambda=0$, advantage function == td error $A_{t}^{GAE(\gamma,0)}=R_{t}+\gamma V_{s_{t+1}}-V(s_{t})$</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Paper reading </tag>
            
            <tag> Research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5行shell忙等抢占GPU——论如何与流氓公用GPU</title>
      <link href="/2019/04/17/fuck-gpu-users/"/>
      <url>/2019/04/17/fuck-gpu-users/</url>
      
        <content type="html"><![CDATA[<p>几行简单的shell代码，用来在实验室公用的服务器上忙等之前占用GPU的进程，一旦之前的进程退出，马上启动自己的进程来抢占GPU (<del>再也不用担心总是排不到空闲GPU了</del></p><a id="more"></a><h2 id="基础版"><a href="#基础版" class="headerlink" title="基础版"></a>基础版</h2><p>临时写的因而比较随意，大部分情况下，在这个代码基础上改一改就可以实现类似需求</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">res=`ps -ef | grep english_drop | wc -l`</span><br><span class="line"><span class="keyword">while</span> [ <span class="variable">$&#123;res&#125;</span> -gt 1 ] </span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    sleep 10</span><br><span class="line">    res=`ps -ef | grep english_drop | wc -l`                                    </span><br><span class="line">    time=$(date <span class="string">"+%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$time</span>"</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Start to run...\n"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Activate your own process here</span></span><br></pre></td></tr></table></figure><p>重点只有一个，就是灵活地用起来shell中的<code>wc</code>指令</p><ul><li><code>-l</code>: 返回行数</li><li><code>-L</code>: 返回字符最多的一行的长度</li><li><code>-m</code>: 返回characters数量</li><li><code>-c</code>: 返回bytes数量</li></ul><h2 id="流氓版"><a href="#流氓版" class="headerlink" title="流氓版"></a>流氓版</h2><p>同理想盯某一块显卡有没有进程在占用就可以这样写（只对用python启动的进程有效，用其他方式启动的进程只要改下grep后面的参数即可）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">res=`nvidia-smi | grep python | wc -l`</span><br><span class="line"><span class="keyword">while</span> [ <span class="variable">$&#123;res&#125;</span> -gt 0 ]</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    sleep 10</span><br><span class="line">    res=`nvidia-smi | grep english_drop | wc -l`</span><br><span class="line">    time=$(date <span class="string">"+%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$time</span>"</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Start to run...\n"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Activate your own process here</span></span><br></pre></td></tr></table></figure><p>单显卡的情况下更简单，没有进程占用GPU时nvidia-smi返回会显示</p><blockquote><p>|  No running processes found                                                 |</p></blockquote><p>可以grep其中任意一个词来查看是否有进程占用，例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res=`nvidia-smi | grep running | wc -l`</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Manual </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Soft Actor-Critic and MPO</title>
      <link href="/2019/04/01/soft-actor-critic/"/>
      <url>/2019/04/01/soft-actor-critic/</url>
      
        <content type="html"><![CDATA[<h2 id="Soft-Actor-Critic"><a href="#Soft-Actor-Critic" class="headerlink" title="Soft Actor-Critic"></a>Soft Actor-Critic</h2><a id="more"></a><p>准确来讲，这里要分析的是两篇发表时间不同但内容高度相关的文章，一篇是Soft Actor-Critic的开山之作<a href="https://arxiv.org/abs/1801.01290" target="_blank" rel="noopener">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>，另一篇是标题中的<a href="https://arxiv.org/abs/1812.05905" target="_blank" rel="noopener">Soft Actor-Critic Algorithms and Applications</a>，都来自于UCB大佬Pieter Abbeel和Sergey Levine实验室。</p><p>SAC系列工作发表之后在RL社区内广受好评，主要原因在于</p><ul><li>这套算法的稳定性非常好，可以在不同的random seed下达到相当的效果，且比较好调参，这一点足以拯救很多RL研究者们日益稀少的发量</li><li>之前同样可以在各种不同的任务环境下达到比较稳定效果的算法是PPO，根据SAC论文的实验效果来看，SAC在高维连续state space连续action space的控制任务上可以达到比PPO好很多的效果</li><li>SAC可以off-policy训练，这意味这SAC拥有比原始PPO低很多的样本复杂度</li><li>相比于DDPG为代表的actor-critic算法，SAC在exploitation-exploration balance上做的更好</li></ul><h3 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h3><p>在我目前看过的文章中，<a href="https://arxiv.org/abs/1812.05905" target="_blank" rel="noopener">Soft Actor-Critic这篇</a>的related works一节绝对称得上是最良心之一，篇幅占了整整一页，条理清晰，稍微扩展下可以出一篇review了</p><p>作者称他们的算法包含了三部分</p><ul><li>An actor-critic framework: 传统policy iteration优化一般分两步，分别是policy evaluation和policy improvement，二者互相在对方的最优点处迭代，最终收敛至全局最优；然而“必须要在对方的最优点迭代”的做法使得这种思路在大型MDP应用中效率很低，actor-critic架构的思路起源于希望这两步优化共同进行</li><li>An off-policy formulation: On-policy一般比off-policy稳定，但样本利用率要低很多，之后一个典型的off-policy尝试是DDPG，根据作者的说法<blockquote><p>The interplay between the deterministic actor network and the Q-function typically makes DDPG extremely difficult to stablize and brittle to hyper-parameter tuning.</p></blockquote></li><li>Entropy maximization term to enable stability and better exploration： 在此之前inverse RL和optimal control领域就已经有很多人利用policy的entropy搞事情了，包括<a href="https://openreview.net/forum?id=S1ANxQW0b" target="_blank" rel="noopener">Maximum a Posteriori Policy Optimization (MPO)</a>和Levine Sergey的成名作<a href="http://rll.berkeley.edu/gps/" target="_blank" rel="noopener">Guided Policy Search (GPS)</a>；此外Schuman大佬还在<a href="https://arxiv.org/abs/1704.06440" target="_blank" rel="noopener">2017年的一篇文章中</a>指出了Soft Q-learning与policy gradient的等价性</li></ul><h3 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h3><p>传统的RL任务的目标是在MDP上优化一个policy使得total discounted reward最大，而SAC的核心思想在于在优化的最终目标上加了一个policy的entropy项</p><script type="math/tex; mode=display">\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\rho_{\pi},\pi}[r(s_{t},a_{t})+\alpha\mathcal{H}(\pi)]</script><p>其中$\alpha$是temperature parameter，可以定为constant，也可以在训练中学习，理论部分的证明中$\alpha$被设定为constant</p><h4 id="Policy-evaluation"><a href="#Policy-evaluation" class="headerlink" title="Policy evaluation"></a>Policy evaluation</h4><p>作者在Lemma 1中证明了以下Bellman backup operator是一个contraction mapping，且$Q$最终会收敛到optimal soft Q-function for policy $\pi$</p><script type="math/tex; mode=display">\mathcal{T}^{\pi}Q(s_{t},a_{t})=r(s_{t},a_{t})+\gamma\mathbb{E}_{\pi}[Q(s_{t+1},a_{t+1}-\alpha\log\pi(a_{t}|s_{t}))]</script><p>证明过程用到一个assumption：action space是离散的，作者称这位为了保证entropy augmented reward有bound；实际上应该大多数连续action space任务上这个reward也是有界的，这个不影响算法实现</p><h4 id="Policy-improvement"><a href="#Policy-improvement" class="headerlink" title="Policy improvement"></a>Policy improvement</h4><p>因为soft Q-learning起源于PGM，这里做policy improvement理所当然地用到了Bolzmann policy distribution</p><script type="math/tex; mode=display">\pi_{new}=\arg\min_{\pi'}D_{KL}(\pi'||\frac{\exp(\frac{1}{\alpha}Q^{\pi_{old}}(s_{t},a_{t}))}{Z^{\pi_{old}(s_{t})}})</script><p>在Lemma 2中，作者进一步证明了由上式构成的policy improvement满足</p><script type="math/tex; mode=display">Q^{\pi_{new}}(s,a)\geq Q^{\pi_{old}}(s,a),\ \forall(s,a)\in{\mathcal{S}\times{\mathcal{A}}}</script><p>加上上面那个bounded augmented reward条件，单调有界两个条件凑齐了，可以立刻得到结论这一套policy iteration全局收敛</p><h3 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h3><p>由于计算复杂度太高，上面推的那一堆虽然很漂亮，但无法用于实际任务中（就说实际任务中不可能用PGM那一套带partition function的Bolzmann policy吧。。。</p><p><img src="http://wx3.sinaimg.cn/bmiddle/006m97Kgly1ffw8ogih9ij30yh0szwhc.jpg" width="20%"></p><p>更新Q-function时，文章也用到了DDPG的exponential moving average技巧，policy evaluation的目标函数只是把DQN的目标函数里面加上entropy，不表</p><p>Policy improvement最终的结果虽然看起来很简单，但中间的推导过程还是值得仔细琢磨下</p><p><img src="./policy-improve.png" width="95%"></p><p>划重点</p><ul><li>还是第一次在论文里见到说reparameterization trick可以得到lower variance estimator的，然而作者在这里没有给参考文献。。。<blockquote><p>It is thus convenient to apply the reparameterization trick instead, resulting in a <strong>lower variance estimator</strong>.</p></blockquote></li><li>看形式感觉是DDPG加了reparameterization trick和entropy regularizer，作者的说法是<blockquote><p>The unbiased gradient estimator extends the DDPG style policy gradients to any tractable stochastic policy</p></blockquote></li><li>在之前的工作中作者还加了一个value function approximator，后来发现这玩意并不需要，于是去掉了</li></ul><h2 id="Maximum-a-Posteriori-Policy-Optimization"><a href="#Maximum-a-Posteriori-Policy-Optimization" class="headerlink" title="Maximum a Posteriori Policy Optimization"></a>Maximum a Posteriori Policy Optimization</h2>]]></content>
      
      
      
        <tags>
            
            <tag> Paper reading </tag>
            
            <tag> Research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Two Papers about the Variance of Policy Gradient on ICLR-2019</title>
      <link href="/2019/03/27/pg-variance/"/>
      <url>/2019/03/27/pg-variance/</url>
      
        <content type="html"><![CDATA[<p><em>总结两篇ICLR上的硬核文章，内容都与policy gradient系列方法的variance estimation有关，一篇来自Pieter Abbeel组，2018年3月起挂在ArXiv上；另一篇出自腾讯AILab，发表于ICLR-2019（<del>仰望大佬</del></em></p><a id="more"></a><h2 id="Variance-Reduction-for-Policy-Gradient-with-Action-Dependent-Factorized-Baselines"><a href="#Variance-Reduction-for-Policy-Gradient-with-Action-Dependent-Factorized-Baselines" class="headerlink" title="Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines"></a>Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines</h2><p>文章研究的问题还是比较general的：当action可以按维度分成几个相互独立的factor的时候，作者提出可以通过引入与action相关的baseline来降低policy gradient的方差</p><blockquote><p>The key insight of this paper is that when the individual actions produced by the policy can be decomposed into multiple factors, we can incorporate this additional information into the baseline to further reduce variance.</p></blockquote><h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><p><em>解释几个关键的名词</em></p><h4 id="1-Baseline"><a href="#1-Baseline" class="headerlink" title="1. Baseline"></a>1. Baseline</h4><p>众所周知policy gradient的一般形式为</p><script type="math/tex; mode=display">\nabla_{\theta}J(\pi_{\theta})=\mathbb{E}_{\pi_{\theta}}[Q(s_{t},a_{t})\nabla_{\theta}\log{\pi_{\theta}(a_{t}|s_{t})}]</script><p>其中</p><script type="math/tex; mode=display">\mathbb{E}_{\pi_{\theta}}[Q(s_{t},a_{t})]=\mathbb{E}_{\pi_{\theta}}[\sum_{t=0}^{T}\gamma^{t}R(s_{t},a_{t})]</script><p>这就是最原始的REINFORCE形式，但REINFORCE梯度估计具有很高的方差，一种解决方案是在不影响原本梯度方向的前提下加一个<strong>只取决于state</strong>的baseline function</p><script type="math/tex; mode=display">\nabla_{\theta}J(\pi_{\theta})=\mathbb{E}_{\pi_{\theta}}[(Q(s_{t},a_{t})-b(s_{t}))\nabla_{\theta}\log{\pi_{\theta}(a_{t}|s_{t})}]</script><h4 id="2-Bias-free-for-baseline-functions"><a href="#2-Bias-free-for-baseline-functions" class="headerlink" title="2. Bias-free for baseline functions"></a>2. Bias-free for baseline functions</h4><p>当baseline只取决于state时，baseline无条件满足bias-free条件</p><script type="math/tex; mode=display">\mathbb{E}_{\pi_{\theta}}[b(s_{t})\nabla_{\theta}\log{\pi_{\theta}(a_{t}|s_{t})}]=0</script><p>本文中作者之所以反复提到bias-free，是因为文章最主要的改进是将$b(s_{t})$改成了$b(s_{t},a_{t}^{-i})$，即标题所说的action-dependent baseline</p><h4 id="3-Baselines-with-minimum-variance"><a href="#3-Baselines-with-minimum-variance" class="headerlink" title="3. Baselines with minimum variance"></a>3. Baselines with minimum variance</h4><p>什么样的$b(s_{t})$可以带来最小的方差？</p><p><img src="./optimal-baseline.png" width="95%"></p><p>一个简单的凸二次优化，最终的结果中，分母上就是FIM的trace，分子上是一个带有Q-function rescale的FIM trace</p><p>实际编程的时候，由于这个最优baseline的计算量太大，所以大多情况下人们采用备选方案——通过supervised training得到一个value function $v(s_{t})$ —— 这种方法被广泛应用在各种policy gradient变体算法中</p><h3 id="Action-Dependent-Baselines"><a href="#Action-Dependent-Baselines" class="headerlink" title="Action-Dependent Baselines"></a>Action-Dependent Baselines</h3><h4 id="Assumption"><a href="#Assumption" class="headerlink" title="Assumption"></a>Assumption</h4><p>Action $a_{t}$ 可以分解成几个互相不关联的factors $a_{t}^{i},a_{t}^{j},…$，不同的factors之间满足</p><script type="math/tex; mode=display">\nabla_{\theta}\log\pi_{\theta}(a_{t}^{i}|s_{t})^{T}\nabla_{\theta}\log\pi_{\theta}(a_{t}^{j}|s_{t}),\ \forall{i\neq{j}}</script><p>仔细想想这个假设应该还是比较强的，因为作者并不是在假设不同的action factors是条件独立的，而是假设不同action factors对于参数的梯度永远是正交的。作者举出了几个满足该假设的例子</p><ul><li>Multi-agent任务中不同的policy之间条件独立 =&gt; 不同policy之间条件独立等价于policy对于参数的梯度正交吗？</li><li>Partially-observable环境下，当每个agent观测到的state完全没有交集 =&gt; 同上</li><li>对不同的action factors采用不同的function approximators，且这些function approximators完全不存在参数共享 =&gt; 参数不共享说明参数不在一个空间里，不在同一个空间能做内积吗？</li></ul><h4 id="Optimal-baselines"><a href="#Optimal-baselines" class="headerlink" title="Optimal baselines"></a>Optimal baselines</h4><p>这里的推导用到了上面的假设，除此以外推导过程与上面的凸二次优化类似，得到结果</p><p><img src="./action-dep.png" width="95%"></p><p>满足假设条件时，定量描述，采用action-dependent baseline比原始的state-dependent baseline方差小多少？</p><p><img src="./improve.png" width="95%"></p><p>作者文中称当Q-function相对action维度很不平滑时，这个improvement会很大</p><p>个人觉得这个形式太复杂，意义不是很大，于是自己沿着这个结果推了个更加没什么卵用的upper bound（<del>如果是lower bound就好了</del></p><script type="math/tex; mode=display">I_{b^{*}(s)}\leq\sum_{i}\mathbb{E}_{\rho_{\pi},a_{t}^{-i}}[\frac{\sum_{j\neq{i}}Y_{j}}{\sum_{j\neq{i}}Z_{j}}]</script><h4 id="Marginalized-Q-baseline"><a href="#Marginalized-Q-baseline" class="headerlink" title="Marginalized Q baseline"></a>Marginalized Q baseline</h4><p>依然难算的baseline，作者在这一节里考虑如何将这个baseline用在实际算法中，提出两种方案</p><ul><li>Monte-Carlo marginalized baseline: 用Monte-Carlo算积分积掉action $a_{i}$，这个idea在我最早了解到multi-agent的时候就想到过，当时是从PGM的角度去想，觉得把其他agent的action积分掉是再正常不过的做法，应该早就有人做过</li><li>Mean marginalized Q baseline: 这个idea就值得玩味了，这里作者直接用$b(s_{t},a_{t}^{-i})=Q_{\pi}(s_{t},a_{t}^{-i},\mathbb{E}_{\pi}[a_{t}^{i}])$来加快计算速度。假设这个Q-function学的非常理想，且policy $\pi$的输出是一个高斯分布，那么取$a_{t}^{i}$的均值点，baseline的形势会变成$b_{i}=\max_{a_{j}\in{\mathcal{A}^{-i}}}Q_{\pi}(s_{t},a_{t}^{-i},a_{j})$，和Monte-Carlo完全不是一码事</li></ul><p>实际上按照作者在实验部分的说法，两者的效果很接近，后者要比前者稍高一点点，这让我有点费解</p><h2 id="Marginal-Policy-Gradients-A-Unified-Family-of-Estimators-for-Bounded-Action-Spaces-with-Applications"><a href="#Marginal-Policy-Gradients-A-Unified-Family-of-Estimators-for-Bounded-Action-Spaces-with-Applications" class="headerlink" title="Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications"></a>Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications</h2><p>又是来自Tencent AILab大佬Han Liu组的paper，文章的理论部分写的非常严谨，用到了大量的测度论知识，给人一种Princeton数学科班生对我等数学渣降维打击的感觉。由于文章本身就已经不是很易懂了，这里尽可能从intuition的角度出发解释下个人对文章的理解，想要更严谨还是去读原文吧</p><p>这篇文章研究的是policy gradient方法的一个细节问题：典型的policy gradient算法，包括TRPO和PPO在内，常常将policy network $\pi(a|s)$参数化成一个高斯分布的形式，然后从$\pi(a|s)$中采样得到实际与环境交互的action。问题在于，很多环境的action space是定义在一个特定的区间内的（譬如Pendulum-v0中action space是[-2,2]），从高斯分布中采样可能会导致action不在这个区间内，这时常见的做法是</p><ul><li>允许环境去裁剪action（打开OpenAI gym的源码他们就是这样做的） =&gt; 作者声称，这样的做法会带来不必要的高方差</li><li>Clip the action and update according to the clipped action =&gt; 这种做法是off-policy的</li></ul><p>作者举得另一个例子是在RTS游戏或机器人导航任务中，action space常常指的是英雄或机器人移动的方向，这个方向的值是连续的，不是像Atari那样的上下左右四个离散动作，这时如果我们还是用老办法让$\pi(a|s)$输出$\mathcal{N}(\mu,\Sigma | \mu\in{(0,2\pi)})$然后做clipping，由于角度的空间不是$\mathbb{R}^{2}$，i.e. $\epsilon=2\pi+\epsilon$，那么对于$\pi(a|s)$的概率分布$\mathcal{N}({\mu,\Sigma})$而言，$\mathbb{P}(\mu-\epsilon)\neq{}\mathbb{P}(\mu+\epsilon),\forall{\mu\neq{\pi}}$，针对这个问题有两种解决思路</p><ul><li>与上面的思路类似，允许环境对action<strong>重新参数化</strong>：采样$a\sim\mathcal{N}(\mu,\Sigma)$，其中$\mu\in{\mathbb{R}^{2}}$，与环境交互的时候执行$a/||a||$ =&gt; 这个方法可以推广到$\mathbb{R}^{d}$高维空间中，问题在于，我们训练的policy有$d$个自由度，而环境实际执行的动作只有$d-1$的自由度，可以想到这个方法的方差一定很高，收敛也会比较慢</li><li>作者在Section 3中提出的新方法，叫做<strong>Angular Policy Gradient</strong>，思路也比较直观，就是用坐标变换把$\mathbb{R}^{d}$空间中由$a/||a||$所确定的density直接映射到spherical measure $\mathcal{S}^{d-1}$上去，作者将$\mathcal{S}^{d-1}$空间中的这个density称为<strong>Angular Gaussian distribution</strong></li></ul><h3 id="Angular-Policy-Gradient"><a href="#Angular-Policy-Gradient" class="headerlink" title="Angular Policy Gradient"></a>Angular Policy Gradient</h3><p><img src="./angular.png" width="95%"></p><p>这个让人看着头大的式子里，最让人头大的莫过于这个$\mathcal{M}_{d-1}(\alpha)$了，这货是一个没有closed form的积分，所幸它的数学性质还不错，用分部积分可以得到</p><script type="math/tex; mode=display">d\mathcal{M}_{d-1}(x)=\mathcal{M}_{d+1}(x)-x\mathcal{M}_{d}(x)</script><p>看到这个公式就发现可以用动态规划求解，边界条件为</p><script type="math/tex; mode=display">\mathcal{M}_{1}(x)=xp(x|\mu=0,\sigma=1)+\mathbb{P}(x|\mu=0,\sigma=1),\ \mathcal{M}_{0}=p(x|\mu=0,\sigma=1)</script><p>其中$\mathbb{P}$和$p$分别是$\mathcal{N}(0,1)$的cdf和pdf，这个DP的复杂度为$O(d)$</p><p>实际做的时候对log probability作者采用了与TRPO相同的处理方法</p><p><img src="trpo.png" width="95%"></p><h3 id="Marginal-Policy-Gradient-Estimators"><a href="#Marginal-Policy-Gradient-Estimators" class="headerlink" title="Marginal Policy Gradient Estimators"></a>Marginal Policy Gradient Estimators</h3><p>把上面讨论的两种解决policy输出与action space不同的思路数学抽象一下，设$Q(s_{t},a_{t})=\mathbb{E}_{\pi}[\sum_{i=t}^{\infty}\gamma^{i-t}R(s_{i},a_{i})]$为state-action function，把$\pi$当成一个probability measure，$T_{*}\pi$为在映射$T$下$\pi$的push-forward measure，忽略文章中各种测度相关的假设（这些假设在现实环境下显然是无条件成立的）</p><ul><li>$g_{1}=Q(T(a),s)\nabla\log{f_{\pi}(a|s)}$: 先由环境输出action，与环境交互时经映射T将action映射到环境所要求的action space中去</li><li>$g_{2}=Q(a,s)\nabla\log{f_{T_{*}\pi}(a|s)}$: 直接将policy network的参数化形式经$T$映射到真正的action space中去，然后直接采样与环境交互</li></ul><p>整个Section 4中作者讨论的核心问题是：</p><ul><li>$g_{1}$与$g_{2}$哪个梯度估计的variance更大 =&gt; 结论当然是$Var(g_{1})\geq{Var(g_{2})}$，不然Section 3里面推导了半天的angular policy gradient不就没用了？</li><li>定量分析，两个梯度估计的方差之间差距有多大 =&gt; Theorem 4.6中作者给出的结果是</li></ul><p><img src="./theorem-46.png" width="95%"></p><p>其中$\mathcal{I}(q,\theta)$为Total Scaled Fisher Information：</p><p><img src="./def44.png" width="95%"></p><p>可以想到当$q(a)=1$时这个定义等价于原始形式的Fisher information的trace，众所周知Fisher information是某个概率分布期望下梯度的方差，其实Theorem 4.6中数学期望下Fisher information的形式就是stochastic gradient的方差的trace（加上了在策略分布下随机游走的平稳分布$\rho(s)$积分，数学期望里面的outer product变成了inner product），这个trace之前常常被用来分析SGD的方差，与普通形式的方差拥有相同的性质</p><blockquote><p>The implication of Theorem 4.6 is that if there is some information loss via a function T before the action interacts with the dynamics of the environment, then one obtains a lower variance estimator of the gradient by replacing the density of $π$ with the density of $T_{∗}π$ in the<br>expression for the policy gradient.</p></blockquote><h3 id="Theory-and-Methodology"><a href="#Theory-and-Methodology" class="headerlink" title="Theory and Methodology"></a>Theory and Methodology</h3><h4 id="1-The-law-of-total-variance"><a href="#1-The-law-of-total-variance" class="headerlink" title="1. The law of total variance"></a>1. The law of total variance</h4><p><em>Refer to <a href="https://en.wikipedia.org/wiki/Law_of_total_variance" target="_blank" rel="noopener">Wikipedia</a> for detailed description</em></p><blockquote><p>In probability theory, the law of total variance, or variance decomposition formula, states that if $X$ and $Y$ are random variables on the <strong>same</strong> probability space, and the variance of Y is <strong>finite</strong>, then</p><script type="math/tex; mode=display">Var(Y)=\mathbb{E}[Var(Y|X)]+Var(\mathbb{E}[Y|X])</script><p>In actuarial science, specifically credibility theory, the first component is called the expected value of the process variance (EVPV) and the second is called the variance of the hypothetical means (VHM).</p></blockquote><p>不难证，可以自己证一下练练手</p><h4 id="2-Fisher-information-decomposition"><a href="#2-Fisher-information-decomposition" class="headerlink" title="2. Fisher information decomposition"></a>2. Fisher information decomposition</h4><p>注意以下说的Fisher information指的全部都是total scaled fisher information</p><p>Fisher information既然是log likelihood的一阶导的方差估计，那么Fisher information也显然可以分解</p><p><img src="./fisher-decomposition.png" width="95%"></p><p>有了这个结论，theorem 4.6就比较显然了</p><h4 id="3-Other-details"><a href="#3-Other-details" class="headerlink" title="3. Other details"></a>3. Other details</h4><ul><li>作者一上来就在related work里面说<a href="https://arxiv.org/abs/1802.07564" target="_blank" rel="noopener">Fujita &amp; Maeda在ICML-2018发表的CAPG文章中</a>理论分析部分是错的，实在是大佬气场，三个审稿人居然没有一个对这一点提出质疑（或许审稿人都是直接跳过related work的</li><li>果然实验又是在王者荣耀上做的，非腾讯内部员工根本不可能复现，希望以后去了腾讯可以有机会接触这方面的东西吧，不知道AILab这些文章中用到的方法是否真的有应用在AI平台部设计的AI bot中</li><li>虽然作者声称这是第一个在RL方向上探索action space为方向的工作，我个人感觉这个问题在robotics里应该非常常见，把欧式空间中的高斯分布映射到spherical measure的思路也很直观，如果说做robotics的人都没有在这个方向上探索过，那足以说明robotics领域对于RL持有何等保守的态度</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Paper reading </tag>
            
            <tag> Research </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepMind Paper Reading —— How is DeepMind Different from OpenAI</title>
      <link href="/2019/03/18/rl-up-to-date/"/>
      <url>/2019/03/18/rl-up-to-date/</url>
      
        <content type="html"><![CDATA[<p><em>知乎之前有这样一个问题：<a href="https://www.zhihu.com/question/316626294/answer/627373838" target="_blank" rel="noopener">DeepMind和OpenAI身后的两大RL流派有什么具体的区别?</a></em></p><a id="more"></a><p><em>由于之前研究方向的原因，我个人对OpenAI以及UCB系列的工作更熟悉一些，这里主要是几篇DeepMind的文章梳理，也会包含一些其他的文章，持续更新</em></p><h2 id="Retrace-Safe-and-Efficient-Off-Policy-Reinforcement-Learning"><a href="#Retrace-Safe-and-Efficient-Off-Policy-Reinforcement-Learning" class="headerlink" title="Retrace: Safe and Efficient Off-Policy Reinforcement Learning"></a>Retrace: Safe and Efficient Off-Policy Reinforcement Learning</h2><p>文章研究的是一个基础而关键的问题：如何将TD方法推广至off-policy。针对此问题作者提出了retrace方法，这种方法具有以下三种性质</p><ul><li>Low variance: avoid variance explosion by cliiping</li><li>Efficient: the algorithm is able to use full returns</li><li>Safety guarantee: applicable for arbitrary target policy and behavior policy</li></ul><p>回顾一下MDP基础知识，off-policy TD方法对应的是数学期望形式的Bellman equation，对于每一步更新，其广义的Bellman operator可以写为</p><script type="math/tex; mode=display">\mathcal{R}Q(s_{t},a_{t})=Q(s_{t},a_{t})+\mathbb{E}_{\mu}[\sum_{t=1}^{T}\gamma^{t}(\prod_{i=0}^{t}c_{i})(r_{t}+\gamma\mathbb{E}_{\pi}Q(s_{t+1},.)-Q(s_{t},a_{t}))]</script><p>其中$\pi$是target policy，$\mu$是behavior policy，按照$c_{i}$的定义不同可以分为几种不同的TD方法，包括</p><ol><li><p><strong>Importance sampling</strong>: $c_{i}=\frac{\pi(a_{i}|s_{i})}{\mu(a_{i}|s_{i})}$</p></li><li><p><strong>Off-policy $Q^{\pi}(\lambda)$ and $Q^{*}(\lambda)$: $c_{i}=\lambda$</strong></p></li></ol><p>Let $\epsilon=\max_{s}||\pi(a|s)-\mu(a|s)||_{1}$ be the <strong>off-policyness</strong>, <a href="https://arxiv.org/abs/1602.04951" target="_blank" rel="noopener"><em>Harutyunyan et al</em></a> proved that the operator defined by $c_{i}=\lambda$ is a contraction mapping with the fixed point $Q^{\pi}$ if $\lambda&lt;\frac{1-\gamma}{\gamma\epsilon}$, and the contraction mapping with the fixed point $Q^{*}$ if $\lambda&lt;\frac{1-\gamma}{2\gamma}$</p><p>The author has pointed out that the convergence guarantee of this contraction mapping relies on the knowledge of $\epsilon$, which can not be evaluated in practice during the policy evaluation. Thus this is not <strong>safe</strong> for arbitrary $\pi$ and $\mu$.</p><ol><li><strong>Tree-backup $TB(\lambda)$: $c_{i}=\lambda\pi(a_{i}|s_{i})$</strong></li></ol><p>The operator defines a contraction mapping for any arbitrary $\pi$ and $\mu$, which makes it a safe algorithm. However, the algorithm is not effcient for the case when $\pi$ and $\mu$ is near, because it prevents the model to make full use of longer returns.</p><ol><li><strong>Retrace</strong>$(\lambda)$:d $c_{i}=\lambda\min(1,\frac{\pi(a_{i}|s_{i})}{\mu(a_{i}|s_{i}})$</li></ol><p>用文中作者的原话来总结</p><blockquote><p>$Retrace(\lambda)$ uses an importance sampling ratio truncated at 1. Compared to IS it does not suffer from variance explosion of the product of IS ratios.</p><p>It does not cut the traces as much as $TB(\lambda)$, making it possible to benifit from full returns.</p></blockquote><p>一份表格说明$Retrace(\lambda)$和这几种方法的关系与区别</p><p><img src="./table.png" width="80%"></p><ul><li>$Retrace(\lambda)$ is the first algorithm for off-policy evaluation and control whose convergence <strong>does not require GLIE assumption</strong>.</li><li>The author proves the convergence of Watkin’s $Q(\lambda)$, which is an open problem since 1989.</li><li>Experimental evaluation is performed on Atari 2600 games. I believe the major contribution of this work is that its theory extends the <em>eligibility trace algorithm</em>, and becomes the motivation of V-trace (IMPALA), which is much more applicable for large and complicated MDPs.</li></ul><h2 id="IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures"><a href="#IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures" class="headerlink" title="IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"></a>IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</h2><p>DeepMind在AlphaStar星际争霸AI上用到的技术之一，主要做了以下几点改进</p><ol><li><strong>Actor-Learner Architecture</strong></li></ol><p>基于A3C改进，作者采用Actor-Learner架构：</p><ul><li>多个Actor异步地与环境交互，将交互得到的trajectories放入全局队列，并周期性地从learner的policy网络copy参数给自己;</li><li>Learner采用trajectory-based optimization的结构，通过一个LSTM网络对trajectories循环进行处理（如下图所示。该架构可以极大地提高分布式训练的吞吐量，满足data efficiency和resource utilization的要求</li></ul><p><img src="./impala.png" width="80%"></p><ol><li><strong>V-trace</strong></li></ol><p>由于整个架构是异步更新，actor的behavior policy会落后于learner所学习的target policy，因此本文引入了V-trace方法来对off-policyness做correction。V-trace方法的表达式形式非常恶心，首先定义对于状态$s_{i}$的value approximation $v_{i}$为:</p><script type="math/tex; mode=display">v_{i}:=V(s_{i})+\sum_{t=i}^{i+n-1}\gamma^{t-i}(\prod_{k=i}^{t-1}c_{k})\delta_{t}V</script><p>其中</p><ul><li>$\delta_{t}{V}=\rho_{t}(r_{t}+\gamma{V}(s_{t+1})-V(s_{t}))$ 为TD error乘上一个不知道从哪里冒出来的$\rho_{t}$</li><li>$\rho_{t}=\min(\bar{\rho},\frac{\pi(a_{t}|s_{t})}{\mu(a_{t}|s_{t})})$, $c_{k}=\min(\bar{c}, \frac{\pi(a_{k}|s_{k})}{\mu(a_{k}|s_{k})})$ 这两项是truncated importance sampling ratios</li><li>$\bar{\rho}$和$\bar{c}$是两个常数，作者的设定是$\bar{\rho}&gt;\bar{c}$，且$\bar{c}\geq{1}$，如果这两个条件满足，那么在on-policy训练的时候V-trace operator等价于n-step Bellman target（这个性质Retrace没有</li></ul><p>除此以外DeepMind团队还做了一些工程优化，包括将数据准备与网络计算流水线化，用XLA编译了部分的tensorflow静态图，以及修改输入数据格式来更好地适应cudnn框架等等。</p><p>由此可见，IMPALA与其说是一套算法，不如说是一套工业级强度的asynchronous deep reinforcement learning框架，DeepMind这篇文章的野心，在于希望IMPALA成为ResNet、RCNN之于CV，seq2seq、BERT之于NLP同等的baseline而存在。</p><h2 id="Mean-Field-Multi-Agent-Reinforcement-Learning"><a href="#Mean-Field-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Mean Field Multi-Agent Reinforcement Learning"></a>Mean Field Multi-Agent Reinforcement Learning</h2><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>发表于ICML2018的一篇文章，其实用mean field来做multi-agent RL这个思路想来还是很直观的，因为multi-agent RL中，我们的最终目标是学习最优的joint-policy，而这个全局最优的joint-policy很可能对于每个单一的agent而言都是次优的，所以每个agent的更新都应当取决于其他agent，通过交替迭代每个agent，最终的结果近似等价于对joint-policy做coordinate ascent优化。</p><p>上面的思路只是基于学过PRML之后对mean field有限的理解延伸得到的，实际文章中的处理还是要更加精巧一些。具体来说，这篇文章研究的问题背景可以归纳为</p><ul><li>Each agent is directly interacted with a <strong>finite</strong> set of other agents</li><li>Stochastic games with a <strong>finite</strong> state space and action space</li><li>The global joint-policy is guaranteed to make the future state distribution <strong>stationary</strong></li></ul><p>这篇文章的贡献主要在于</p><ul><li>作者提出了一套基于mean field approximation的方法来训练multi-agent RL，在此框架下相邻agent的交互可以被简化成每一个agent与其相邻节点的action均值交互的过程。基于此formulation作者提出了<strong>mean field Q-Learning</strong>和<strong>mean field actor-critic</strong>两个算法</li><li>作者证明了在<strong>finite-state stochastic game</strong>情况下，加上若干<b color="red">technical assumption</b>，mean field MARL算法最终可以收敛至Nash equilibrium</li></ul><p><img src="./mean-field-q-learning.png" width="80%"><br><img src="./mean-field-ac.png" width="80%"></p><h3 id="以下开始吐槽"><a href="#以下开始吐槽" class="headerlink" title="以下开始吐槽"></a>以下开始吐槽</h3><p>Section 3的notation略混乱，导致读的时候比较费力，主要是由于作者上来就先假设action space是离散的，扭头就用二阶可微的条件对action space做了泰勒展开（见eq 7），再细琢磨了下发现作者其实是假设对于每一个agent，其所有相邻agent的action space可以近似看成连续的，因此eq 10处才对$\pi_{j}$用了累加形式的数学期望，却对$\pi_{-j}$用$\mathbb{E}_{\pi_{-j}}[.]$表示数学期望——然而Section 4中的证明又完全是在discrete setting下证的</p><p>此外，eq 7推到eq 8这一步的近似比较牵强，作者在Appendix B中为了证明泰勒展开的二阶项可以消掉，抬手就是一个二阶Lipchitz条件——要求$\nabla{Q}$满足Lipchitz continuous条件——由于现代神经网络普遍采用ReLU激活函数，这个假设显然不成立</p><p>假设太强最终会影响到算法在真实世界的任务上的适用性，虽然文章的实验比较充分，但三个实验都没有在benchmark任务上做。综上所述这几点，其实我还蛮希望可以看到这篇文章的rebuttal过程的</p><p>作者在introduction中claim说mean field MARL方法可以更好地应用于多agent问题，这一点其实是非常好的motivation，因为MARL领域中，如何处理agent数量较多的任务至今仍然是一个open problem，大部分方法的训练难度/训练时间都会随着agent数量的增加呈现出至少是线性（甚至是指数级别）的增长。因此，在benchmark任务上，尤其是continuous control任务上复现下这篇文章应该是个蛮有意思的探索方向</p><h3 id="Technical-Part"><a href="#Technical-Part" class="headerlink" title="Technical Part"></a>Technical Part</h3><p>接下来看看作者的证明用到了哪些assumption</p><blockquote><p>Each action-state pair is visited infinitely often, and the reward is bounded by some constant $K$</p></blockquote><ul><li>$Q(s,a)$本身就是为了处理$v(s)$估计方差过大的问题而提出的，action-state pair无数次访问是$Q(s,a)$收敛的常规假设</li></ul><blockquote><p>Agent policy满足GLIE条件</p></blockquote><ul><li>文章中称采用的是Boltzmman policy，这种情况下随着temperature参数下降到0，policy会收敛至greedy，这条假设也是合理的</li></ul><blockquote><p>The Nash equilibrium for $\pi$ is recognized either as 1) global optimum or 2) a saddle point expressed as</p><ol><li><script type="math/tex; mode=display">\mathbb{E}_{a\sim\pi^{*}}[Q(s,a)] \geq \mathbb{E}_{a\sim\pi}[Q(s,a)]</script></li><li><script type="math/tex; mode=display">\mathbb{E}_{a\sim\pi^{*}}[Q(s,a)] \geq \mathbb{E}_{\pi^{j}}\mathbb{E}_{\pi_{*}^{-j}}[Q(s,a)]</script><script type="math/tex; mode=display">\mathbb{E}_{a\sim\pi^{*}}[Q(s,a)] \leq \mathbb{E}_{\pi_{*}^{j}}\mathbb{E}_{\pi^{-j}}[Q(s,a)]</script></li></ol></blockquote><p>相比假设1假设2，这条假设是比较强的，它认为在训练中的任何时刻$t$，对每个agent单独的policy $\pi_{j}$而言，Nash equilibrium都存在。</p><p>问题在哪里?这里引用<a href="https://en.wikipedia.org/wiki/Nash_equilibrium#Nash&#39;s_Existence_Theorem" target="_blank" rel="noopener">Wikipedia的Nash’s existence theorem</a>的内容：</p><blockquote><p>Nash proved that if we allow mixed strategies, then every game with a <strong>finite</strong> number of players in which each player can choose from <strong>finitely</strong> many <strong>pure strategies</strong> has <strong>at least one</strong> Nash equilibrium.</p><p>Nash equilibria need not exist if the set of choices is <strong>infinite and noncompact</strong>. An example is a game where two players simultaneously name a natural number and the player naming the larger number wins. However, a Nash equilibrium exists if the set of choices is <strong>compact</strong> with <strong>continuous payoff</strong>. An example, in which the equilibrium is a mixture of continuously many pure strategies, is a game where two players simultaneously pick a real number between 0 and 1 (inclusive) and player one’s winnings (paid by the second player) equal the square root of the distance between the two numbers.</p></blockquote><p>作者在文章中也解释说这个假设太强，实验中发现即使条件不满足也可以收敛（<del>虽然实验部分并没有很好地justify这点</del></p><h2 id="Model-Based-Reinforcement-Learning-via-Meta-Policy-Optimization"><a href="#Model-Based-Reinforcement-Learning-via-Meta-Policy-Optimization" class="headerlink" title="Model-Based Reinforcement Learning via Meta-Policy Optimization"></a>Model-Based Reinforcement Learning via Meta-Policy Optimization</h2><p>简称MB-MPO，Pieter Abbeel组2018年发表在CoRL的一篇文章，OpenAI巨佬John Schulman也出现在了作者列表中，本文主要的内容是利用ensemble+meta-learning的思路来学习MDP的dynamics，从而训练一个可以快速在不同task上adapt的agent。</p><p>文章涉及到了两个重要的关键词：Model-based RL和meta-learning，我对这两个方向的了解都比较有限，但这篇文章的脉络很清晰，偏向于robotics方面的应用，没有什么理论分析，读起来也比较轻松愉快。实验部分非常充分，可以得到不少insights</p><h3 id="Ensemble-Learning-for-Model-Dynamics"><a href="#Ensemble-Learning-for-Model-Dynamics" class="headerlink" title="Ensemble Learning for Model Dynamics"></a>Ensemble Learning for Model Dynamics</h3><p>这里的model dynamics指的是一个MDP中，在当前状态下采取某个动作后到达下一状态的映射关系$f:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{S}$，在广义的MDP定义下$f$是一个随机映射，由$f$所确定的状态转换是一个Markov随机过程。传统的model-based RL方法就是要直接去学习这个映射关系$f$，这个学习过程会非常依赖于model的准确性：即使model对于每个transition三元组$(s_{t},a_t,s_{t+1})$的预测bias都很小，最终这个bias可能会在trajectory中积累，对最终结果带来很大的bias；换用更数学的语言来说，就是在MDP的设定下最终整个trajectory的bias是不能像普通的empirical risk minimization一样被bound的，这种现象叫做distributional shift</p><p>传统的解决方法包括</p><ul><li>用data aggregation在训练时不断加入由model与agent直接交互得到的新的trajectories</li><li>避免让agent去探索model bias比较大的区域，可以训练Bayesian model来刻画model的uncertainty，或者直接训练一个生成式的模型来学习model的概率分布（文中指出这种方法训练出的agent往往过于保守）</li><li>通过特殊的优化方式或优化目标，代表性工作有<a href="http://www.roboticsproceedings.org/rss11/p12.pdf" target="_blank" rel="noopener">DeepMPC: Learning Deep Latent Features for<br>Model Predictive Control</a>以及<a href="http://papers.nips.cc/paper/5183-reinforcement-learning-in-robust-markov-decision-processes" target="_blank" rel="noopener">Robust policy optimization</a></li><li>Differentiable trajectory optimization，优化时会遇到与RNN类似的问题，gradients either explode or vanish</li></ul><p>本文的一个重要motivation在于，作者希望即使在model不是很准确的情况下agent也可以进行有效的学习，为了解决这个问题作者引入了ensemble</p><script type="math/tex; mode=display">\max_{\theta}{\frac{1}{K}}\sum_{k=0}^{K}J_{k}(\theta'_{k}) \quad \text{s.t.}\ \theta'_{k}=\theta+\alpha\nabla_{\theta}J_{k}(\theta)</script><p>其中$J_{k}(\theta)$为在dynamic model $\hat{f}_{\phi_{k}}$下policy $\pi_{\theta}$的expected discounted reward</p><script type="math/tex; mode=display">J_{k}(\theta)=\mathbb{E}_{a_{t}\sim\pi_{\theta}(a_{t}|s_{t})}[\sum_{t=0}^{T}\gamma^{t}R(s_{t},a_{t})|s_{t+1}=\hat{f}_{\phi_{k}}(s_{t},a_{t})]</script><h3 id="Meta-RL-with-Learned-Dynamic-models"><a href="#Meta-RL-with-Learned-Dynamic-models" class="headerlink" title="Meta-RL with Learned Dynamic models"></a>Meta-RL with Learned Dynamic models</h3><p><img src="./mb-mpo.png"></p><p>在我看来，本文在meta-learning的方面并无创新之处，只是<a href="https://arxiv.org/abs/1703.03400" target="_blank" rel="noopener">MARL算法框架</a>的简单套用，因此这里不展开讨论这部分算法实现</p><p>即使如此，meta-learning背后的一些思想还是很吸引我的，按照我个人的理解，加上作者在related works一节中的梳理，meta-learning主要可以分为几类</p><ul><li>Learning to learn</li><li>Learn an initialization of the network that can quickly adapt to several different tasks sharing the same state and action space (e.g. MARL)</li><li><a href="https://arxiv.org/abs/1611.02779" target="_blank" rel="noopener">RL2: Fast Reinforcement Learning via Slow Reinforcement Learning</a>, and <a href="https://arxiv.org/abs/1605.06065v1" target="_blank" rel="noopener">One-shot Learning with Memory-Augmented Neural Networks</a></li></ul><p>个人疑问：</p><ul><li>Meta-learning的定义貌似不是很清晰，meta-learning和one-shot learning, few-shot learning是什么关系？和transfer learning又是什么关系？有什么区别？</li><li>MARL中，几个不同的任务可以在同一个parameter initialization的基础上通过一步gradient迭代得到，这种现象在何等程度上具有普遍性？两个语义完全不同的任务是否可以通过同一个<strong>元</strong>联系到一起？因为按照直觉来讲，利用随机梯度优化得到的网络参数应该是比较稳定的，参数空间在收敛点附近的梯度是很小的，在参数空间的微小扰动应该不足以完成一个网络的功能性迁移</li><li>为什么文章中优化adapted policy只需要用原始形式的policy gradient即可，而优化meta需要用TRPO？这是否说明从meta做adaption是很容易的，优化meta是很困难的？如果两个都用TRPO会如何？</li><li>作者展示了若干实验结果表明ensemble的估计方差与$D_{KL}(\pi_{\theta}||\pi_{\theta’_{k}})$呈显著正相关关系，为什么？这背后的原因值得深挖</li></ul><h2 id="Feedback-based-MCTS"><a href="#Feedback-based-MCTS" class="headerlink" title="Feedback-based MCTS"></a>Feedback-based MCTS</h2><p>腾讯AILab发在ICML-2018上的文章，比较难读，文章主要做了两个工作</p><ul><li>在continuous state space and finite action space MDP上提出了一种新的MCTS方法，按照作者的说法：“Leaf evaluators can be updated to produce a stronger tree search using previous tree search results”</li><li>对上面提出的MCTS方法提供了sample complexity analysis</li></ul><p>除feedback-based MCTS方法以外，作者采用四种baseline作为对比</p><ul><li>No rollouts: 与AlphaZero设定相同</li><li>Direct policy iteration</li><li>Approximate value iteration</li><li>Behavior clone (termed as <em>SL agent in the paper</em>)</li></ul><p>比较有趣的地方在于作者直接对比了AlphaZero的方法，很少有人有能力做这样的对比实验。此外实验部分还有一些细节</p><ul><li>直接用游戏引擎里的41维游戏信息作为输入</li><li>只做了1V1的evaluation</li><li>Intead of using the argmax of the UCB scores, the authors sample actions from the softmax of UCT scores</li><li>Hand-crafted reward function including <em>health, damage, etc.</em></li><li>Reward function mimics reward shaping proposed by <em>Ng et al.</em></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Paper reading </tag>
            
            <tag> Research </tag>
            
            <tag> Resource collection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>multiprocessing与threading模块相关踩坑记录</title>
      <link href="/2019/02/26/multiprocessing-and-threading/"/>
      <url>/2019/02/26/multiprocessing-and-threading/</url>
      
        <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><a id="more"></a><p>多线程/多进程/分布式编程在深度学习/强化学习的应用中是很常见的问题，本文的问题就是在实现DPPO的时候遇到的。</p><p>在开始复现这个算法之前我已经参考运行了<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/12_Proximal_Policy_Optimization/DPPO.py" target="_blank" rel="noopener">Morvan大神的demo实现</a>，这份代码的可读性非常棒，它的并行是按照读者-写者模式执行的，其中模型更新的master线程是读者，收集数据的worker线程是写者，双方的操作严格互斥：写者与环境交互得到训练数据放入队列，读者从队列中取出数据进行训练。后续测试中发现代码虽然确实实现了并行，然而运行效率并不是很高，运行时的CPU利用率始终保持在一个比较低的水平，经分析后原因主要有以下几点</p><ul><li>Python GIL的限制</li><li>对于读者而言，每次执行完一次更新后都会触发同步；对于写者而言，每次收集到一个batch的数据后也都会触发同步，因此很多时间会浪费在操作系统/线程级别的IO上</li><li>与A3C的实现不同，每次一个worker获取到数据时，不管其他worker处于什么状态，它们的buffer都会被清空，这就导致不管你开了多少个worker，最终只会有一个worker收集到的数据有效并传递给master线程。按照我个人的理解，这样实现的目的在于维持训练的稳定性——每当一个worker推送数据给master时，master都会进行模型参数的更新，而更新后的policy已经不是其他worker收集数据时的policy了，由于PPO方法只能用于on-policy，这部分数据理论上来讲应当舍弃</li></ul><p>那么既然在异步调度中会有这么多限制，首先一个问题，是否可以抛弃一部分理论上的严谨性，把程序实现变成纯粹异步的？</p><p>答案是否定的，原因在于模型更新这一步无论如何都必须要进行同步，否则如果模型正在更新参数的时候worker运行，那么worker得到的trajectories就会是脏数据，因为这些trajectories从概率分布上讲既不服从旧的policy distribution，也不服从更新后的policy distribution。</p><p>那么进一步，是否可以只对于模型更新操作进行同步，剩余操作全部异步呢？</p><p>理论上来说似乎是可行的，然而后续的实验中发现，由于模型更新速度比worker收集trajectory快，大部分时间里master都会抢占掉锁，全局队列中的元素长期很少，这反而使得程序在操作系统/线程级别的IO上花费了更多的时间效率。</p><p>因此我最后选择了一个折中的方案，并将这份代码改成了自己的风格，这里总结下修改过的地方</p><ul><li>设置一个队列大小的上限阈值<code>MAX_QSIZE</code>，同步操作仅发生在队列大小达到上限或队列为空时<ul><li>当队列大小达到上限，阻塞worker，进行模型更新直到队列为空</li><li>当队列为空，阻塞master，启动所有worker异步收集数据</li></ul></li><li>经验上来讲，PPO本就是TRPO的近似，而TRPO方法中每步更新的KL divergence upper bound是有理论保障的，因此每步更新policy distribution不会有太大变化，每次模型参数更新后可以不清空其他worker的buffer</li><li>考虑Python GIL的问题根深蒂固，用multiprocessing代替threading模块是更好的选择</li><li>子线程/进程的运行不阻塞主线程/进程，主线程实时进行evaluation和render</li><li>实例化一个Event类成员来管理训练的迭代停止，防止程序无法正常结束的情况</li></ul><h2 id="multiprocessing"><a href="#multiprocessing" class="headerlink" title="multiprocessing"></a>multiprocessing</h2><p>Python的multiprocessing库提供了与threading非常接近的API，且是由强变量类型的Python实现的，非常人性化，以下是几种使用multiprocessing创建进程的方式</p><h4 id="简单进程的创建"><a href="#简单进程的创建" class="headerlink" title="简单进程的创建"></a>简单进程的创建</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">worker</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="string">"""thread worker function"""</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Worker:'</span>, num</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    jobs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        p = multiprocessing.Process(target=worker, args=(i,))</span><br><span class="line">        jobs.append(p)</span><br><span class="line">        p.start()</span><br></pre></td></tr></table></figure><h4 id="继承派生"><a href="#继承派生" class="headerlink" title="继承派生"></a>继承派生</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Worker</span><span class="params">(multiprocessing.Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Worker, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'In %s'</span> % self.name</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    jobs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        p = Worker()</span><br><span class="line">        jobs.append(p)</span><br><span class="line">        p.start()</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> jobs:</span><br><span class="line">        j.join()</span><br></pre></td></tr></table></figure><p>即使如此，如果你认为可以用与多线程并行的相同方式实现多进程并行，那将是调bug噩梦的开始。</p><p><img src="http://img.99danji.com/uploadfile/2016/0419/20160419034745372.jpg" width="200px;"></p><p>所谓基础不牢地动山摇，如果你不明白其中原因，请重复仔细阅读下面这两句话：</p><blockquote><p><strong>线程是操作系统调度的最小单位，进程是操作系统中资源分配的最小单位</strong></p></blockquote><p>换个说法</p><blockquote><p><strong>线程之间资源可以共享，进程则不然</strong></p></blockquote><ul><li>具体来说，如果每个子进程执行需要消耗的时间非常短，则不必使用多进程，因为进程的启动关闭也会耗费资源</li><li>使用多进程往往是用来处理CPU密集型的需求，如果是IO密集型则使用多线程去处理更加合适</li></ul><h3 id="threading"><a href="#threading" class="headerlink" title="threading"></a>threading</h3><h2 id="Our-approach"><a href="#Our-approach" class="headerlink" title="Our approach"></a>Our approach</h2><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://docs.python.org/2/library/multiprocessing.html" target="_blank" rel="noopener">Python multiprocess library documentary</a></li><li><a href="https://www.cnblogs.com/haitaoli/p/9837697.html" target="_blank" rel="noopener">一个讲的比较详细的博客</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
            <tag> Coding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>On the Numerical Instablity of Conjugate Gradient</title>
      <link href="/2019/02/15/numerical-instability-conjugate-gradient/"/>
      <url>/2019/02/15/numerical-instability-conjugate-gradient/</url>
      
        <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><a id="more"></a><p>在tensorflow计算图中实现的Conjugate gradient算法，一开始迭代数值就变成nan。</p><p>这个问题是在实现natural gradient optimization的时候发现的，代码实现的思路严格参照了<a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method" target="_blank" rel="noopener">Wikipedia上有关conjugate gradient的讲解</a>，由于代码真正运行是在静态的计算图中，运行时也很难debug出到底出问题的是哪一步。</p><p>最后问题解决的办法也非常暴力——在屏幕上打印出所有的变量一一检查，逐层定位问题。经检查，最有可能的两个问题是：</p><ul><li>Conjugate gradient算法实现中可能会出现分母为零的情况</li><li>计算KL divergence时出现浮点数下溢，导致log输出NaN</li></ul><p>理论上来讲这种情况是不会发生的，因为information geometry optimization的核心度量Fisher information matrix一般来讲都是非奇异的。给定$N$个不同的样本，FIM的rank不小于$N\times{\dim(\mathcal{Z})}$，其中$\dim(\mathcal{Z})$为模型输出空间概率分布本身的维度。这也就是说我们只要把batch size给到足够大，FIM不会出现奇异的情况</p><p>但实际代码运行的时候，由于一些数值上的不稳定性，FIM奇异还是有可能会出现</p><h2 id="Code-implementation"><a href="#Code-implementation" class="headerlink" title="Code implementation"></a>Code implementation</h2><h3 id="Conjugate-gradient-in-TensorFlow-static-computational-graph"><a href="#Conjugate-gradient-in-TensorFlow-static-computational-graph" class="headerlink" title="Conjugate gradient in TensorFlow static computational graph"></a>Conjugate gradient in TensorFlow static computational graph</h3><p>Conjugate gradient的实现确实是比较复杂，想要将其写在TensorFlow的静态计算图内部还是比较麻烦，遇到的坑会比用numpy实现多不少，以下先放代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hessian_vector_product</span><span class="params">(x, grad, variable)</span>:</span></span><br><span class="line">    kl_grad_prod = tf.reduce_sum(grad * x)</span><br><span class="line">    <span class="keyword">return</span> tf.stop_gradient(tf.gradients(kl_grad_prod, variable)[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_conjugate_gradient</span><span class="params">(x, kl_grad, variable, n_iter=<span class="number">10</span>, func_Ax=hessian_vector_product)</span>:</span></span><br><span class="line">    <span class="string">"""build_conjugate_gradient</span></span><br><span class="line"><span class="string">    :param x: type tf.Tensor, the initial value of x</span></span><br><span class="line"><span class="string">    :param kl_grad: type tf.Tensor, the gradient of the objective</span></span><br><span class="line"><span class="string">    :param variable: type tf.Variable</span></span><br><span class="line"><span class="string">    :return: the converged conjugate gradient vector \tilde&#123;x&#125; = H^&#123;-1&#125;x</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Fixed number of iterations in the inner loop</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x = tf.stop_gradient(x)</span><br><span class="line">    r = x - func_Ax(x, kl_grad, variable)</span><br><span class="line">    p = tf.stop_gradient(r)</span><br><span class="line">    r_dot_r = tf.reduce_sum(tf.square(r))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(n_iter):</span><br><span class="line">        Ap = func_Ax(p, kl_grad, variable)</span><br><span class="line">        p_dot_Ap = tf.reduce_sum(p * Ap)</span><br><span class="line">        alpha = r_dot_r / (p_dot_Ap + EPSILON)</span><br><span class="line">        x = x + alpha * p</span><br><span class="line">        r = r - alpha * Ap</span><br><span class="line">        r_dot_r_new = tf.reduce_sum(tf.square(r))</span><br><span class="line">        beta = r_dot_r_new / (r_dot_r + EPSILON)</span><br><span class="line">        r_dot_r = r_dot_r_new</span><br><span class="line">        p = r + beta * p</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>Ideas worth noting:</p><ul><li>The conjugate gradient does not require direct access to the explicit form of the matrix, only matrix-vector-product is needed. Let $\hat{F}\in{\mathbb{R}^{N\times{N}}}$ be the FIM estimated with a batch of data, and let $v\in{\mathbb{R}^{N}}$ be an arbitrary vector, then $\hat{F}^{-1}v=\nabla_{\theta}(v^{T}\nabla_{\theta}D_{KL}(\pi_{\text{old}}||\pi))$.</li><li>When calculating the matrix-vector-product $\hat{F}^{-1}v$, remember to block the gradient from vector $v$ using <code>tf.stop_gradient</code> in every iteration. Your gradient should <strong>only come from the KL divergence term</strong>.</li><li>There are devision operations for variables <code>alpha</code> and <code>beta</code>. Remember to add a small number (<code>EPSILON</code>) for the denominator in case of NaN output.</li></ul><p>github上也可以找到其他的基于TensorFlow静态计算图的conjugate gradient实现，<a href="https://github.com/steveKapturowski/tensorflow-rl/blob/master/algorithms/trpo_actor_learner.py" target="_blank" rel="noopener">其中一个印象比较深刻的实现方法</a>是将while循环判断也写在了计算图的内部，用<code>tf.while_loop</code>实现，代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_conjugate_gradient_ops</span><span class="params">(self, pg_grads, kl_grads, max_iterations=<span class="number">20</span>, residual_tol=<span class="number">1e-10</span>)</span>:</span></span><br><span class="line">    i0 = tf.constant(<span class="number">0</span>, dtype=tf.int32)</span><br><span class="line">    loop_condition = <span class="keyword">lambda</span> i, r, p, x, rdotr: tf.logical_and(</span><br><span class="line">        tf.greater(rdotr, residual_tol), tf.less(i, max_iterations))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(i, r, p, x, rdotr)</span>:</span></span><br><span class="line">        fvp = utils.ops.flatten_vars(tf.gradients(</span><br><span class="line">            tf.reduce_sum(tf.stop_gradient(p)*kl_grads),</span><br><span class="line">            self.policy_network.params))</span><br><span class="line">        z = fvp + self.cg_damping * p</span><br><span class="line">        alpha = rdotr / (tf.reduce_sum(p*z) + <span class="number">1e-8</span>)</span><br><span class="line">        x += alpha * p</span><br><span class="line">        r -= alpha * z</span><br><span class="line">        new_rdotr = tf.reduce_sum(r*r)</span><br><span class="line">        beta = new_rdotr / (rdotr + <span class="number">1e-8</span>)</span><br><span class="line">        p = r + beta * p</span><br><span class="line">        new_rdotr = tf.Print(new_rdotr, [i, new_rdotr], <span class="string">'Iteration / Residual: '</span>)</span><br><span class="line">        <span class="keyword">return</span> i+<span class="number">1</span>, r, p, x, new_rdotr</span><br><span class="line"></span><br><span class="line">    _, r, p, stepdir, rdotr = tf.while_loop(</span><br><span class="line">        loop_condition,</span><br><span class="line">        body,</span><br><span class="line">        loop_vars=[i0,</span><br><span class="line">                    pg_grads,</span><br><span class="line">                    pg_grads,</span><br><span class="line">                    tf.zeros_like(pg_grads),</span><br><span class="line">                    tf.reduce_sum(pg_grads*pg_grads)])</span><br><span class="line">    fvp = utils.ops.flatten_vars(tf.gradients(</span><br><span class="line">        tf.reduce_sum(tf.stop_gradient(stepdir)*kl_grads),</span><br><span class="line">        self.policy_network.params))</span><br><span class="line">    shs = <span class="number">0.5</span> * tf.reduce_sum(stepdir*fvp)</span><br><span class="line">    lm = tf.sqrt((shs + <span class="number">1e-8</span>) / self.max_kl)</span><br><span class="line">    fullstep = stepdir / lm</span><br><span class="line">    neggdotstepdir = tf.reduce_sum(pg_grads*stepdir) / lm</span><br><span class="line">    <span class="keyword">return</span> fullstep, neggdotstepdir</span><br></pre></td></tr></table></figure><h3 id="Model-definition-and-environment-interaction"><a href="#Model-definition-and-environment-interaction" class="headerlink" title="Model definition and environment interaction"></a>Model definition and environment interaction</h3><p>调了一晚上的bug，根源就在这部分代码中。在<a href="https://github.com/ChenShawn/advanced_policy_gradient_methods/blob/master/debug.ipynb" target="_blank" rel="noopener">jupyter notebook</a>中打印出了所有变量的实际值后，发现一些奇怪的现象：</p><ul><li>问题出在从普通的gradient转成natural gradient的过程中，普通gradient数值没有问题，natural gradient时常数值爆炸，或者直接出现NaN</li><li>搭了一个小网络来测试conjugate gradient实现的正确与否，发现偶尔会出现FIM奇异的情况，推测是numerical instability所致</li><li>进一步的测试中发现，每一步的迭代中并不总是会出现FIM奇异。相对而言较大、且靠近底层的参数不容易出现FIM奇异，参数量较小且靠近输出层的参数容易出现NaN，当时最频繁出现NaN的参数，就是网络最后一层的bias</li></ul><p>猜测这种numerical instability可能是由于KL divergence不稳定导致的，所以干脆把旧的policy网络和新的policy网络写成同一个，设置<code>reuse=True</code>的来共享网络参数，这样KL divergence就会始终为0，毕竟我们需要的只是KL divergence的gradient和Hessian而已。</p><p>后续实验中发现这样写会使得网络迭代更新速度缓慢，且容易收敛到sub-optimal点，有关这个问题之后如有新的发现再来更新这里的内容吧</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect_multi_batch</span><span class="params">(env, agent, maxlen, batch_size=<span class="number">64</span>, qsize=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""collect_multi_batch</span></span><br><span class="line"><span class="string">    See collect_one_trajectory docstring</span></span><br><span class="line"><span class="string">    :return: three lists of batch data (s, a, r)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    que = []</span><br><span class="line">    s_init = env.reset()</span><br><span class="line">    que.append(s_init[<span class="literal">None</span>, :])</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(qsize - <span class="number">1</span>):</span><br><span class="line">        st, r, done, _ = env.step([<span class="number">-0.99</span>])</span><br><span class="line">        que.append(st[<span class="literal">None</span>, :])</span><br><span class="line">    <span class="comment"># Interact with environment</span></span><br><span class="line">    buffer_s, buffer_a, buffer_r = [], [], []</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(maxlen):</span><br><span class="line">        s = np.concatenate(que, axis=<span class="number">-1</span>)</span><br><span class="line">        a = agent.choose_action(s)</span><br><span class="line">        buffer_s.append(s)</span><br><span class="line">        s, r, done, _ = env.step(a)</span><br><span class="line">        que.pop(<span class="number">0</span>)</span><br><span class="line">        que.append(s[<span class="literal">None</span>, :])</span><br><span class="line">        buffer_a.append(a[<span class="literal">None</span>, :])</span><br><span class="line">        r = (r + <span class="number">0.3</span>) * <span class="number">2.0</span></span><br><span class="line">        buffer_r.append(r)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># Accumulate rewards</span></span><br><span class="line">    discounted = <span class="number">1.0</span></span><br><span class="line">    last_value = model.value_estimate(s)</span><br><span class="line">    discounted_r = []</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(len(buffer_r) - <span class="number">2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        last_value = r + args.gamma * last_value</span><br><span class="line">        discounted_r.append(last_value)</span><br><span class="line">    state_data, action_data, reward_data = [], [], []</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">0</span>, maxlen, batch_size):</span><br><span class="line">        <span class="keyword">if</span> it &gt;= len(buffer_s):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        states_array = np.concatenate(buffer_s[it: it + batch_size], axis=<span class="number">0</span>)</span><br><span class="line">        actions_array = np.concatenate(buffer_a[it: it + batch_size], axis=<span class="number">0</span>)</span><br><span class="line">        rewards_array = np.array(discounted_r[it: it + batch_size], dtype=np.float32)[:, <span class="literal">None</span>]</span><br><span class="line">        <span class="comment"># rewards_array = np.clip(rewards_array, -1.0, 5.0)</span></span><br><span class="line">        state_data.append(states_array)</span><br><span class="line">        action_data.append(actions_array)</span><br><span class="line">        reward_data.append(rewards_array)</span><br><span class="line">    <span class="keyword">return</span> state_data, action_data, reward_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TNPGModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, v_lr, pi_lr, model_dir, delta=<span class="number">1e-3</span>)</span>:</span></span><br><span class="line">        self.state = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>], name=<span class="string">'state'</span>)</span><br><span class="line">        self.action = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], name=<span class="string">'action'</span>)</span><br><span class="line">        self.reward = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], name=<span class="string">'reward'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Advantage function definition</span></span><br><span class="line">        print(<span class="string">' [*] Building advantage function...'</span>)</span><br><span class="line">        kwargs = &#123;<span class="string">'kernel_initializer'</span>: tf.orthogonal_initializer()&#125;</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'value'</span>):</span><br><span class="line">            h1 = tf.layers.dense(self.state, <span class="number">128</span>, activation=tf.nn.relu, name=<span class="string">'h1'</span>, **kwargs)</span><br><span class="line">            self.value = tf.layers.dense(h1, <span class="number">1</span>, activation=<span class="literal">None</span>, name=<span class="string">'value'</span>, **kwargs)</span><br><span class="line">            self.advantage = self.reward - self.value</span><br><span class="line"></span><br><span class="line">            self.v_loss = tf.reduce_mean(tf.square(self.advantage))</span><br><span class="line">        v_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="string">'value'</span>)</span><br><span class="line">        self.v_train = tf.train.AdamOptimizer(v_lr).minimize(self.v_loss, var_list=v_vars)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Policy function definition</span></span><br><span class="line">        print(<span class="string">' [*] Building policy function...'</span>)</span><br><span class="line">        self.policy, pi_vars = build_gaussian_network(self.state, <span class="number">1</span>, scope=<span class="string">'policy'</span>)</span><br><span class="line">        old_policy, old_vars = build_gaussian_network(self.state, <span class="number">1</span>, scope=<span class="string">'policy'</span>, trainable=<span class="literal">False</span>, reuse=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'policy_ops'</span>):</span><br><span class="line">            <span class="comment"># self.assign_op = [old.assign(new) for old, new in zip(old_vars, pi_vars)]</span></span><br><span class="line">            self.sample_op = self.policy.sample(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'surrogate_loss'</span>):</span><br><span class="line">            ratio = self.policy.prob(self.action) / old_policy.prob(self.action)</span><br><span class="line">            surrogate = ratio * self.advantage</span><br><span class="line">            self.pi_loss = -tf.reduce_mean(surrogate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert Adam gradient to natural gradient</span></span><br><span class="line">        print(<span class="string">' [*] Building natural gradient...'</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'policy_optim'</span>):</span><br><span class="line">            kl = tf.distributions.kl_divergence(old_policy, self.policy)</span><br><span class="line">            optim = tf.train.AdamOptimizer(pi_lr)</span><br><span class="line">            pi_grads_and_vars = optim.compute_gradients(surrogate, var_list=pi_vars)</span><br><span class="line">            pi_grads = [pair[<span class="number">0</span>] <span class="keyword">for</span> pair <span class="keyword">in</span> pi_grads_and_vars]</span><br><span class="line">            kl_grads = tf.gradients(kl, pi_vars)</span><br><span class="line"></span><br><span class="line">            conj_grads = []</span><br><span class="line">            <span class="keyword">for</span> grad, kl_grad, var <span class="keyword">in</span> zip(pi_grads, kl_grads, pi_vars):</span><br><span class="line">                conj = build_conjugate_gradient(grad, kl_grad, var)</span><br><span class="line">                nat_grad = tf.sqrt((<span class="number">2.0</span> * delta) / (tf.reduce_sum(grad * conj) + EPSILON)) * conj</span><br><span class="line">                conj_grads.append((nat_grad, var))</span><br><span class="line">            self.pi_train = optim.apply_gradients(conj_grads)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Summaries definition</span></span><br><span class="line">        print(<span class="string">' [*] Building summaries...'</span>)</span><br><span class="line">        model_variance = tf.reduce_mean(self.policy._scale)</span><br><span class="line">        self.sums = tf.summary.merge([</span><br><span class="line">            tf.summary.scalar(<span class="string">'max_rewards'</span>, tf.reduce_max(self.reward)),</span><br><span class="line">            tf.summary.scalar(<span class="string">'mean_advantage'</span>, tf.reduce_mean(self.advantage)),</span><br><span class="line">            tf.summary.scalar(<span class="string">'pi_loss'</span>, self.pi_loss),</span><br><span class="line">            tf.summary.scalar(<span class="string">'v_loss'</span>, self.v_loss),</span><br><span class="line">            tf.summary.scalar(<span class="string">'model_variance'</span>, model_variance)</span><br><span class="line">        ], name=<span class="string">'summaries'</span>)</span><br><span class="line"></span><br><span class="line">        config = tf.ConfigProto()</span><br><span class="line">        <span class="comment"># config.gpu_options.allow_growth = True</span></span><br><span class="line">        self.sess = tf.Session(config=config)</span><br><span class="line">        self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        print(<span class="string">' [*] Model built finished'</span>)</span><br><span class="line">        _, self.counter = load(self.sess, model_dir)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        a = self.sess.run(self.sample_op, feed_dict=&#123;self.state: s&#125;)</span><br><span class="line">        <span class="keyword">return</span> a[<span class="number">0</span>, :, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, s, a, r, v_iter, pi_iter, writer=None, counter=<span class="number">0</span>)</span>:</span></span><br><span class="line">        feed_dict = &#123;self.state: s, self.action: a, self.reward: r&#125;</span><br><span class="line">        <span class="comment"># self.sess.run(self.assign_op)</span></span><br><span class="line">        <span class="comment"># update policy</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(pi_iter):</span><br><span class="line">            self.sess.run(self.pi_train, feed_dict=feed_dict)</span><br><span class="line">        <span class="comment"># update value function</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(v_iter):</span><br><span class="line">            self.sess.run(self.v_train, feed_dict=feed_dict)</span><br><span class="line">        <span class="keyword">if</span> writer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            sumstr = self.sess.run(self.sums, feed_dict=feed_dict)</span><br><span class="line">            writer.add_summary(sumstr, counter)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RL Resources Collection</title>
      <link href="/2019/02/12/rl-resources-collection/"/>
      <url>/2019/02/12/rl-resources-collection/</url>
      
        <content type="html"><![CDATA[<h2 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h2><a id="more"></a><ul><li><a href="https://www.bilibili.com/video/av32149008?from=search&amp;seid=8453514189125212595" target="_blank" rel="noopener">David Silver video from bilibili</a></li><li><a href="https://www.bilibili.com/video/av24724071?from=search&amp;seid=8453514189125212595" target="_blank" rel="noopener">李宏毅 video from bilibili</a></li><li><a href="https://www.bilibili.com/video/av20957290/?p=7" target="_blank" rel="noopener">UCB CS294-112 video from bilibili</a></li><li><a href="http://rail.eecs.berkeley.edu/deeprlcourse/" target="_blank" rel="noopener">UCB CS294-112课程主页</a></li><li><a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="noopener">UCL David Silver课程主页</a></li></ul><h2 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs"></a>Blogs</h2><ul><li><a href="http://www.wildml.com/2016/10/learning-reinforcement-learning/" target="_blank" rel="noopener">DennyBrytz’s blog about his RL repository codes</a></li><li><a href="https://blog.openai.com/" target="_blank" rel="noopener">OpenAI’s blog</a></li><li><a href="https://deepmind.com/blog/?category=research" target="_blank" rel="noopener">DeepMind’s blog</a></li><li><a href="https://bair.berkeley.edu/blog/" target="_blank" rel="noopener">UCB RL-related blog</a></li><li><a href="https://www.zhihu.com/people/zhang-chu-heng/posts" target="_blank" rel="noopener">知乎大佬的主页</a></li></ul><h2 id="Textbooks"><a href="#Textbooks" class="headerlink" title="Textbooks"></a>Textbooks</h2><ul><li><a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Textbook “Reinforcement Learning: An Introduction” by Sutton and Barto</a></li></ul><p>Some of the materials above is adapted from <a href="https://zhuanlan.zhihu.com/p/34918639" target="_blank" rel="noopener">an article about reinforcement learning on zhihu</a>.</p><h2 id="Codes-for-study"><a href="#Codes-for-study" class="headerlink" title="Codes for study"></a>Codes for study</h2><ul><li><a href="https://github.com/openai/baselines" target="_blank" rel="noopener">OpenAI Baselines: high-quality implementations of RL algorithms</a></li><li><a href="https://github.com/dennybritz/reinforcement-learning" target="_blank" rel="noopener">DennyBrytz’s guthub repository about RL (9K+ stars)</a></li><li><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow" target="_blank" rel="noopener">MorvanZhou’s github repository about reinforcement learning</a></li><li><a href="https://github.com/carpedm20/deep-rl-tensorflow" target="_blank" rel="noopener">carpedm20/deep-rl-tensorflow</a></li><li><a href="https://github.com/matthiasplappert/keras-rl" target="_blank" rel="noopener">matthiasplappert/keras-rl</a></li></ul><h2 id="Tutorials"><a href="#Tutorials" class="headerlink" title="Tutorials:"></a>Tutorials:</h2><ul><li><a href="http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/" target="_blank" rel="noopener">Introduction to Reinforcement Learning (Joelle Pineau @ Deep Learning Summer School 2016)</a></li><li><a href="http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/" target="_blank" rel="noopener">Deep Reinforcement Learning (Pieter Abbeel @ Deep Learning Summer School 2016)</a></li><li><a href="http://techtalks.tv/talks/deep-reinforcement-learning/62360/" target="_blank" rel="noopener">Deep Reinforcement Learning ICML 2016 Tutorial (David Silver)</a></li><li><a href="https://www.youtube.com/watch?v=ggqnxyjaKe4" target="_blank" rel="noopener">Tutorial: Introduction to Reinforcement Learning with Function Approximation</a></li><li><a href="https://www.youtube.com/playlist?list=PLjKEIQlKCTZYN3CYBlj8r58SbNorobqcp" target="_blank" rel="noopener">John Schulman - Deep Reinforcement Learning (4 Lectures)</a></li><li><a href="http://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf" target="_blank" rel="noopener">Deep Reinforcement Learning Slides @ NIPS 2016</a></li><li><a href="https://spinningup.openai.com/en/latest/user/introduction.html" target="_blank" rel="noopener">OpenAI Spinning Up</a></li></ul><h2 id="Algorithm-implementation"><a href="#Algorithm-implementation" class="headerlink" title="Algorithm implementation"></a>Algorithm implementation</h2><ul><li><a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">Tensorflow implementation of DQN</a></li></ul><h2 id="Papers-up-to-date"><a href="#Papers-up-to-date" class="headerlink" title="Papers up to date"></a>Papers up to date</h2><ul><li>见<a href="https://chenshawn.github.io/tags/Resource-collection/" target="_blank" rel="noopener">论文整理Tag下的若干文章</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Research </tag>
            
            <tag> Resource collection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用matplotlib绘制带有方差区间的曲线</title>
      <link href="/2019/02/10/matplotlib-variance-line/"/>
      <url>/2019/02/10/matplotlib-variance-line/</url>
      
        <content type="html"><![CDATA[<p>在强化学习的论文中经常可以看到一条收敛线，周围还有浅浅的范围线，一直比较疑惑这个范围线的实际含义，似乎不同论文中这个范围线的实际含义是不同的</p><p>例如有些文章中，范围线随时间变化非常剧烈，表示的是不同random seed下运行结果的标准差，而大部分Berkeley和OpenAI的文章中，范围线都很比较平滑，代表的似乎是标准差的滑动平均</p><p>直到看TD3的时候才发现，Figure 5的caption处写明了画图的方式</p><blockquote><p>The shaded region represents <strong>half</strong> a standard deviation of the average evaluation over 10 trials. Curves are smoothed uniformly for visual clarity.</p></blockquote><a id="more"></a><h2 id="matplotlib-pyplot-fill-between"><a href="#matplotlib-pyplot-fill-between" class="headerlink" title="matplotlib.pyplot.fill_between"></a>matplotlib.pyplot.fill_between</h2><p>实现该绘图功能需要用到的最重要的一个就是<code>matplotlib.pyplot.fill_between</code>，参数说明详见<a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.fill_between.html#matplotlib.pyplot.fill_between" target="_blank" rel="noopener">官方文档</a>，函数原型如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">matplotlib.pyplot.fill_between(x, </span><br><span class="line">                               y1, </span><br><span class="line">                               y2=<span class="number">0</span>, </span><br><span class="line">                               where=<span class="literal">None</span>,</span><br><span class="line">                               interpolate=<span class="literal">False</span>, </span><br><span class="line">                               step=<span class="literal">None</span>, *, </span><br><span class="line">                               data=<span class="literal">None</span>, **kwargs)</span><br></pre></td></tr></table></figure><p>函数的功能是将两条曲线之间的面积用制定颜色填充，在绘图时我们只需要手动计算出方差区间，然后使用该函数填充区间即可</p><p>注意需要将alpha参数调小，从而降低填充区域的透明度，避免原来绘制的图像被填充区域覆盖掉</p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Suppose variable `reward_sum` is a list containing all the reward summary scalars</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_variance</span><span class="params">(reward_mean, reward_std, color=<span class="string">'yellow'</span>, savefig_dir=None)</span>:</span></span><br><span class="line">    <span class="string">"""plot_with_variance</span></span><br><span class="line"><span class="string">        reward_mean: typr list, containing all the means of reward summmary scalars collected during training</span></span><br><span class="line"><span class="string">        reward_std: type list, containing all variance</span></span><br><span class="line"><span class="string">        savefig_dir: if not None, this must be a str representing the directory to save the figure</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    half_reward_std = reward_std / <span class="number">2.0</span></span><br><span class="line">    lower = [x - y <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(reward_mean, half_reward_std)]</span><br><span class="line">    upper = [x + y <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(reward_mean, half_reward_std)]</span><br><span class="line">    plt.figure()</span><br><span class="line">    xaxis = list(range(len(lower)))</span><br><span class="line">    plt.plot(xaxis, reward_mean, color=color)</span><br><span class="line">    plt.fill_between(xaxis, lower, upper, color=color, alpha=<span class="number">0.2</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.xlabel(<span class="string">'Episode'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Average reward'</span>)</span><br><span class="line">    plt.title(<span class="string">'The convergence of rewards'</span>)</span><br><span class="line">    <span class="keyword">if</span> savefig_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> type(savefig_dir) <span class="keyword">is</span> str:</span><br><span class="line">        plt.savefig(savefig_dir, format=<span class="string">'svg'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
            <tag> Manual </tag>
            
            <tag> Data analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>On Implementing the Reparameterization Trick</title>
      <link href="/2019/02/06/reparameterization-trick/"/>
      <url>/2019/02/06/reparameterization-trick/</url>
      
        <content type="html"><![CDATA[<p>第一次接触到reparameterization trick是在variational auto-encder的文章中，由于其中损失函数含有hidden layer真实分布与高斯先验之间的KL divergence项，在实现时将hidden layer重新参数化成一个高斯分布。</p><a id="more"></a><p>后来发现reparameterization trick的应用场景远不止VAE这么简单，这种实现上的trick在Bayesian deep learning与reinforcement learning中都有非常广泛的应用。本文基于tensorflow对reparameterization trick的实现做简单总结。</p><h2 id="1-什么情况下可以用reparameterization-trick？"><a href="#1-什么情况下可以用reparameterization-trick？" class="headerlink" title="1. 什么情况下可以用reparameterization trick？"></a>1. 什么情况下可以用reparameterization trick？</h2><p>总体来说，有下面几种情况可以用到reparameterization trick</p><ul><li>神经网络的某个部分需要参数化成一个概率分布的形式，e.g. Gaussian distribution</li><li>需要将神经网络的输出进行随机化，i.e. 神经网络不再是一个固定的从输入空间到输出空间的映射关系，而是一个包含了随机性的模型</li></ul><h2 id="2-VAE-Implementation"><a href="#2-VAE-Implementation" class="headerlink" title="2. VAE Implementation"></a>2. VAE Implementation</h2><p>以下为简化版的VAE示意代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vae</span><span class="params">(batch_xs, scope=<span class="string">'VAE'</span>, reuse=False)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=reuse):</span><br><span class="line">        mu, sigma = build_encoder(batch_xs)</span><br><span class="line">        sampled = mu + sigma * tf.random_normal(mu.get_shape().as_list(), <span class="number">0.0</span>, <span class="number">1.0</span>, dtype=tf.float32)</span><br><span class="line">        output = build_decoder(sampled, batch_xs.get_shape().as_list()[<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">return</span> output, mu, sigma</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_encoder</span><span class="params">(input_op)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'encoder'</span>):</span><br><span class="line">        h1 = tf.layers.dense(input_op, <span class="number">256</span>, activation=tf.nn.relu, use_bias=<span class="literal">True</span>)</span><br><span class="line">        h2 = tf.layers.dense(h1, <span class="number">128</span>, activation=tf.nn.relu, use_bias=<span class="literal">True</span>)</span><br><span class="line">        mu = tf.layers.dense(h2, <span class="number">50</span>, activation=<span class="literal">None</span>, use_bias=<span class="literal">True</span>)</span><br><span class="line">        sigma = tf.layers.dense(h2, <span class="number">50</span>, activation=tf.softplus, use_bias=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> mu, sigma</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_decoder</span><span class="params">(input_op, output_dim)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'decoder'</span>):</span><br><span class="line">        h1 = tf.layers.dense(input_op, <span class="number">128</span>, activation=tf.nn.relu, use_bias=<span class="literal">True</span>)</span><br><span class="line">        h2 = tf.layers.dense(h1, <span class="number">256</span>, activation=tf.nn.relu, use_bias=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.layers.dense(h2, output_dim)</span><br><span class="line"></span><br><span class="line">batch_xs = tf.placeholder(tf.float32, [<span class="literal">None</span>, INPUT_DIM], name=<span class="string">'batch_xs'</span>)</span><br><span class="line">rec_xs, mu, sigma = build_vae(batch_xs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loss term definition</span></span><br><span class="line">rec_loss = tf.reduce_mean(tf.square(rec_xs - batch_xs))</span><br><span class="line">kl_loss = tf.reduce_sum(tf.square(mu) + tf.square(sigma) - tf.log(tf.square(sigma)) - <span class="number">1.0</span>, axis=<span class="number">-1</span>)</span><br><span class="line">kl_loss = tf.reduce_mean(kl_loss, name=<span class="string">'kl_loss'</span>)</span><br><span class="line"></span><br><span class="line">loss = rec_loss + kl_loss</span><br></pre></td></tr></table></figure><p>注意方差标准差一定是大于零的，一般常规做法会使用softplus激活函数来确保它们的非负性。简单来说，softplus函数是对ReLU激活函数的光滑近似，其形式为</p><script type="math/tex; mode=display">f(x)=\log(1+e^{x})</script><p>图片来自<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks" target="_blank" rel="noopener">wikipedia</a>#Softplus)</p><div align="center">    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Rectifier_and_softplus_functions.svg/1024px-Rectifier_and_softplus_functions.svg.png" width="350px"></div><p>除此以外，也可以用<code>tf.square</code>来保证标准差的非负性</p><h2 id="3-tf-distributions-Distribution"><a href="#3-tf-distributions-Distribution" class="headerlink" title="3. tf.distributions.Distribution"></a>3. tf.distributions.Distribution</h2><p>tensorflow中提供了一个概率分布类，可以比较方便的对计算图中的随机概率分布进行建模，<code>tf.distributions.Distribution</code>类是抽象基类，其详细API可以参考<a href="https://www.tensorflow.org/api_docs/python/tf/distributions/Distribution" target="_blank" rel="noopener">官方文档</a>。</p><p>最核心的几个成员函数总结如下</p><ul><li>sample(shape): 输入一个采样维度shape，返回一个可以从概率分布中采样的sampler op</li><li>prob(x): 输入一个shape为[batch_size, 1]的tf.Tensor或np.array，返回概率密度函数在$x$点的值</li><li>cdf(x): 输入一个shape为[batcg_size, 1]的tf.Tensor或np.array，返回累计分布函数在$x$点的值，i.e., Let $p(t)$ be the pdf, then the returned op $\int_{-\infty}^{x}p(t)dt$</li><li>log_prob(x): The same as prob(x) except that the returned value is, literally, the logarithm of the pdf on a given point.</li></ul><h2 id="4-Reparameterization-trick-in-Bayesian-deep-learning"><a href="#4-Reparameterization-trick-in-Bayesian-deep-learning" class="headerlink" title="4. Reparameterization trick in Bayesian deep learning"></a>4. Reparameterization trick in Bayesian deep learning</h2><p>上面的例子中解释了reparameterization trick在VAE中的应用，其中我们为了将hidden layer的概率分布建模成Gaussian distribution，在具体实现时，直接让神经网络输出Gaussian distribution的$\mu$和$\sigma$，再从这两个参数构成的Gaussian distribution中采样得到randomized hidden layer。</p><p>更常见的一种情况是，我们希望模型的输出不仅表达了模型对输入的预测，同时也可以建模模型本身的uncertainty（e.g. 用Gaussian distribution的方差来表达）。使用与上面类似的思路，可以让模型直接输出$\mu$和$\sigma$两个vector，真实的预测值来源于$\mathcal{N}(\mu,\sigma^{2})$的采样。</p><p>这种思路非常适合应用于Information geometry optimization (IGO)，原因在于，IGO中的一个基本任务是估计Fisher information matrix，一般的计算方式是这样的</p><script type="math/tex; mode=display">G^{\theta}=\nabla_{\theta_{k}}\nabla_{\theta_{k}}D_{KL}(p(x,y|\theta)||p(x,y|\theta_{k}))</script><ul><li>如果不使用这种特殊的参数化方式，那么上式中的KL divergence就只能通过empirical distribution来进行估计，这样做是没有办法对数据本身的uncertainty进行建模的。</li><li>在神经网络中计算Hessian的复杂度太高了，为什么还是执意使用Hessian of KL的形式？主要的目的是引入conjugate gradient来计算${G^{\theta}}^{-1}g$，where $g$ is the gradient under the Euclidean metric。</li></ul><p>接下来放示意代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(input_op, pred_dim, reuse=False, scope=<span class="string">'model'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=reuse):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Implement the model definition here</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">mu, sigma = build_model(batch_xs, PRED_DIM, reuse=<span class="literal">False</span>)</span><br><span class="line">mu_old, sigma_old = build_model(batch_xs, PRED_DIM, reuse=<span class="literal">True</span>)</span><br><span class="line">mu_old, sigma_old = tf.stop_gradient(mu_old), tf.stop_gradient(sigma_old)</span><br><span class="line">gaussian = tf.distributions.Normal(mu, sigma)</span><br><span class="line">gaussian_old = tf.distribution.Normal(mu_old, sigma_old)</span><br><span class="line"></span><br><span class="line">kl_divergence = tf.distributions.kl_divergence(gaussian_old, gaussian)</span><br></pre></td></tr></table></figure><h2 id="5-Gumbel-Softmax-Trick"><a href="#5-Gumbel-Softmax-Trick" class="headerlink" title="5. Gumbel-Softmax Trick"></a>5. Gumbel-Softmax Trick</h2><p><strong>References</strong></p><ul><li><a href="https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html" target="_blank" rel="noopener">The Gumbel-Softmax Trick for Inference of Discrete Variables</a></li><li><a href="https://www.cnblogs.com/initial-h/p/9468974.html" target="_blank" rel="noopener">Gumbel-Softmax Trick和Gumbel分布</a></li></ul><p><strong>Goal</strong></p><p>上面VAE中，使用reparameterization trick的最终目的是从连续概率分布中 (Gaussian) 采样，同时保持网络的可微性，相比之下Gumbel-softmax trick则是从softmax层输出的额归一化概率中采样，并保持网络的可微分性质</p><p><strong>Implementation</strong></p><ol><li>Sample $\bm{\epsilon}:=(\epsilon_{1},\epsilon_{2},…,\epsilon_{n})$ where each $\epsilon_{i}$ is i.i.d. sampled from $U(0,1)$</li><li>$\bm{g}=-\log(-\log(\bm{\epsilon}))$</li><li>$\bm{v}’=\bm{v}+\bm{g}$, where $\bm{v}$ is the output of the softmax layer normalized to $\sum_{i}v_{i}=1$</li><li>Softmax again by $\sigma_{i}(\bm{v}’)=\frac{\exp(v’_{i}/\tau)}{\sum_{j=1}^{n}\exp(v’_{j}/\tau)}$, i.e. <code>gumbel = tf.softmax(v / tau)</code>, where $\tau$ is the temperature parameter, which should perform an anealing during training</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Advanced Policy Gradient</title>
      <link href="/2019/02/06/policy-gradient/"/>
      <url>/2019/02/06/policy-gradient/</url>
      
        <content type="html"><![CDATA[<p>本文的主要内容为CS294-112课程中『Advanced Policy Gradient』一节的总结与代码实现。<br><a id="more"></a><br>代码参考了<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow" target="_blank" rel="noopener">莫凡大神的github repo</a></p><h2 id="Policy-performance-bound"><a href="#Policy-performance-bound" class="headerlink" title="Policy performance bound"></a>Policy performance bound</h2><h2 id="Truncated-natural-policy-gradient-TNPG"><a href="#Truncated-natural-policy-gradient-TNPG" class="headerlink" title="Truncated natural policy gradient (TNPG)"></a>Truncated natural policy gradient (TNPG)</h2><h2 id="Trust-region-policy-gradient-TRPO"><a href="#Trust-region-policy-gradient-TRPO" class="headerlink" title="Trust region policy gradient (TRPO)"></a>Trust region policy gradient (TRPO)</h2><h2 id="Proximal-policy-gradient-PPO"><a href="#Proximal-policy-gradient-PPO" class="headerlink" title="Proximal policy gradient (PPO)"></a>Proximal policy gradient (PPO)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PPO</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line">        self.tfs = tf.placeholder(tf.float32, [<span class="literal">None</span>, S_DIM], <span class="string">'state'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Critic (Using advantage function to reduce variance)</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'critic'</span>):</span><br><span class="line">            l1 = tf.layers.dense(self.tfs, <span class="number">100</span>, activation=tf.nn.relu)</span><br><span class="line">            self.v = tf.layers.dense(l1, <span class="number">1</span>)</span><br><span class="line">            self.tfdc_r = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], <span class="string">'discounted_r'</span>)</span><br><span class="line">            self.advantage = self.tfdc_r - self.v</span><br><span class="line">            self.closs = tf.reduce_mean(tf.square(self.advantage))</span><br><span class="line">            self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Actor (Function pi(a|s) following behavior policy)</span></span><br><span class="line">        pi, pi_params = self._build_anet(<span class="string">'pi'</span>, trainable=<span class="literal">True</span>)</span><br><span class="line">        oldpi, oldpi_params = self._build_anet(<span class="string">'oldpi'</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'sample_action'</span>):</span><br><span class="line">            self.sample_op = tf.squeeze(pi.sample(<span class="number">1</span>), axis=<span class="number">0</span>)       <span class="comment"># choosing action</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'update_oldpi'</span>):</span><br><span class="line">            self.update_oldpi_op = [oldp.assign(p) <span class="keyword">for</span> p, oldp <span class="keyword">in</span> zip(pi_params, oldpi_params)]</span><br><span class="line"></span><br><span class="line">        self.tfa = tf.placeholder(tf.float32, [<span class="literal">None</span>, A_DIM], <span class="string">'action'</span>)</span><br><span class="line">        self.tfadv = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], <span class="string">'advantage'</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'loss'</span>):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">'surrogate'</span>):</span><br><span class="line">                <span class="comment"># ratio = tf.exp(pi.log_prob(self.tfa) - oldpi.log_prob(self.tfa))</span></span><br><span class="line">                ratio = pi.prob(self.tfa) / oldpi.prob(self.tfa)</span><br><span class="line">                surr = ratio * self.tfadv</span><br><span class="line">            <span class="keyword">if</span> METHOD[<span class="string">'name'</span>] == <span class="string">'kl_pen'</span>:</span><br><span class="line">                self.tflam = tf.placeholder(tf.float32, <span class="literal">None</span>, <span class="string">'lambda'</span>)</span><br><span class="line">                kl = tf.distributions.kl_divergence(oldpi, pi)</span><br><span class="line">                self.kl_mean = tf.reduce_mean(kl)</span><br><span class="line">                self.aloss = -tf.reduce_mean(surr - self.tflam * kl)</span><br><span class="line">            <span class="keyword">elif</span> METHOD[<span class="string">'name'</span>] == <span class="string">'clip'</span>:</span><br><span class="line">                <span class="comment"># clipping method, find this yields better performance</span></span><br><span class="line">                self.aloss = -tf.reduce_mean(tf.minimum(</span><br><span class="line">                    surr,</span><br><span class="line">                    tf.clip_by_value(ratio, <span class="number">1.0</span> - METHOD[<span class="string">'epsilon'</span>], <span class="number">1.0</span> + METHOD[<span class="string">'epsilon'</span>]) * self.tfadv)</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'atrain'</span>):</span><br><span class="line">            self.atrain_op = tf.train.AdamOptimizer(A_LR).minimize(self.aloss)</span><br><span class="line"></span><br><span class="line">        tf.summary.FileWriter(<span class="string">'log/'</span>, self.sess.graph)</span><br><span class="line">        self.sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
            <tag> Coding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Applications of Fisher Information Matrix</title>
      <link href="/2019/01/19/Fisher/"/>
      <url>/2019/01/19/Fisher/</url>
      
        <content type="html"><![CDATA[<p><em>未完成，待更新。。。</em></p><p><em>Unfinished, to be updated…</em></p><h2 id="1-Statistical-definition"><a href="#1-Statistical-definition" class="headerlink" title="1. Statistical definition"></a>1. Statistical definition</h2><a id="more"></a><h3 id="1-1-Informal-definition"><a href="#1-1-Informal-definition" class="headerlink" title="1.1. Informal definition"></a>1.1. Informal definition</h3><p>The following definition is from <a href="https://en.wikipedia.org/wiki/Fisher_information" target="_blank" rel="noopener">the wikipedia of Fisher information</a>.</p><blockquote><p>In mathematical statistics, the <strong>Fisher information</strong> (sometimes simply called <strong>information</strong>) is a way of measuring the amount of information that an observable random variable $X$ carries about an unknown parameter $\theta$ of a distribution that models $X$.</p></blockquote><p>The following is from the “related works” section in <a href="https://arxiv.org/abs/1810.03806" target="_blank" rel="noopener">my paper</a>.</p><blockquote><p>The Fisher information is initially proposed to measure the variance of the likelihood estimation given by a statistical model.<br>Then the idea was extended by introducing differential geometry to statistics.<br>By considering the FIM of the exponential family distributions as the Riemannian metric tensor,<br>Chenstov further proves that the FIM as a Riemannian measure is the only invariant measure for distributions.</p></blockquote><h3 id="1-2-Formal-definition"><a href="#1-2-Formal-definition" class="headerlink" title="1.2. Formal definition"></a>1.2. Formal definition</h3><p>Formally, let $p(x|\theta)$ be a probability density function of random variable $X$ conditioned on parameter $\theta$.<br>The Fisher information matrix of $\theta$, denoted as $G^{\theta}$,<br>is defined as the variance of the expectation over the derivative of log-likelihood with respect to $\theta$:</p><script type="math/tex; mode=display">G^{\theta}_{ij}=\mathbb{E}_{x|\theta}[(\frac{\partial}{\partial{\theta_{i}}}\log{p(x|\theta)})                                        (\frac{\partial}{\partial{\theta_{j}}}\log{p(x|\theta)})^{T}]</script><p>Some other forms are</p><script type="math/tex; mode=display">G^{\theta}_{ij}=\frac{\partial^{2}}{\partial{\eta_{i}}\partial{\eta_{j}}}\mathbb{E}_{x|\theta}[\log{p(x|\theta)||p(x|\theta+\eta)}]\\=\mathbb{D}_{x|\theta}[\frac{\partial}{\partial{\theta}}\log{p(x|\theta)}]\\=-\mathbb{E}_{x|\theta}[\frac{\partial^{2}}{\partial{\theta_{i}}\partial{\theta_{j}}}\log{p(x|\theta)}]</script><h2 id="2-Fisher-information-in-practice"><a href="#2-Fisher-information-in-practice" class="headerlink" title="2. Fisher information in practice"></a>2. Fisher information in practice</h2><p>TODO: </p><ul><li>vanilla natural gradient descent</li><li>Natural CMA-ES</li><li>TRPO / PPO / ACKTR / etc.</li><li>Virtual adversarial training</li></ul><h2 id="3-Are-all-of-the-definitions-numerically-equivalent"><a href="#3-Are-all-of-the-definitions-numerically-equivalent" class="headerlink" title="3. Are all of the definitions numerically equivalent?"></a>3. Are all of the definitions numerically equivalent?</h2><p>In this section, our goal is to verify that all of the aforementioned forms of Fisher information matrix are not only equivalent to the other in theory, but also match in programming computation.</p><p>This might not be a problem at first glance, but it has confused me for a long time. The major question including</p><ul><li>If calculating the expectation over the outer product of gradient is sufficient to obtain the matrix, why do people bother calculating second order derivatives when performing natural gradient descent?</li><li>What the hell is the conditional distribution $p(y,x|\theta)$ in practice? What the hell is the conditional distribution $p(y|x,\theta)$ in practice?</li><li>Are all of the forms numerically stable such that all of them result in the exactly the same result when programming?</li></ul><p>Let us first verify that the Hessian of KL divergence is indeed equivalent to the expectation over outer product.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_net</span><span class="params">(xs)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'test'</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">        batch_xs = tf.expand_dims(xs, axis=<span class="number">0</span>)</span><br><span class="line">        hs = tf.layers.dense(batch_xs, <span class="number">30</span>, activation=tf.nn.relu)</span><br><span class="line">        zs = tf.layers.dense(hs, <span class="number">3</span>, activation=<span class="literal">None</span>)</span><br><span class="line">        probs = tf.nn.softmax(zs, axis=<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> probs, zs</span><br><span class="line"></span><br><span class="line">xs = tf.ones((<span class="number">3</span>))</span><br><span class="line">perturb = <span class="number">0.5</span> * tf.ones_like(xs)</span><br><span class="line">aprob, alogits = build_net(xs)</span><br><span class="line">bprob, blogits = build_net(xs + perturb)</span><br><span class="line">ys = tf.placeholder(tf.int32, [<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">loss = tf.nn.softmax_cross_entropy_with_logits(logits=alogits, labels=ys)</span><br><span class="line">aloss = tf.log(aprob + <span class="number">1e-25</span>)</span><br><span class="line">bloss = tf.log(bprob + <span class="number">1e-25</span>)</span><br><span class="line">kl = tf.reduce_sum(aprob * aloss) - tf.reduce_sum(bprob * bloss)</span><br><span class="line">grad = tf.gradients(tf.reduce_mean(loss), xs)[<span class="number">0</span>]</span><br><span class="line">hessian = tf.hessians(kl, perturb)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    fisher = np.zeros((<span class="number">3</span>, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">    p = sess.run(aprob)</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        y = np.zeros((<span class="number">1</span>, <span class="number">3</span>), dtype=np.int32)</span><br><span class="line">        y[<span class="number">0</span>, it] = <span class="number">1</span></span><br><span class="line">        g = sess.run(grad, feed_dict=&#123;ys: y&#125;)</span><br><span class="line">        fisher += p[<span class="number">0</span>, it] * np.matmul(g[:, <span class="literal">None</span>], g[<span class="literal">None</span>, :])</span><br><span class="line"></span><br><span class="line">    fisher2 = -sess.run(hessian)</span><br><span class="line"></span><br><span class="line">print(fisher)</span><br><span class="line">print(fisher2)</span><br><span class="line"></span><br><span class="line">print(np.linalg.eig(fisher)[<span class="number">0</span>].max())</span><br><span class="line">print(np.linalg.eig(fisher2)[<span class="number">0</span>].max())</span><br></pre></td></tr></table></figure><p>TODO: add results below</p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git使用记录</title>
      <link href="/2019/01/18/git-summary/"/>
      <url>/2019/01/18/git-summary/</url>
      
        <content type="html"><![CDATA[<p>git一些不是很常用的功能经常忘记指令，在这里简单记录<br><a id="more"></a></p><h2 id="1-冲突解决"><a href="#1-冲突解决" class="headerlink" title="1. 冲突解决"></a>1. 冲突解决</h2><p>通常发生在多人协作开发时，不同的人对同一分支做了不同的修改，在提交的时候会由于冲突而无法通过。一般如果远端分支比本地分支更新的话，会选择直接抛弃本地分支，将远端分支的内容全部覆盖到现有代码上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 若本地存在未commit的改动，需要这条指令</span></span><br><span class="line">$ git stash</span><br><span class="line">$ git pull --rebase origin master</span><br></pre></td></tr></table></figure><p>但如果本地分支的改动和远端分支的改动都想保留，或者希望自定义需要保留的内容的话，就需要解决冲突。首先我们知道 <code>git pull</code> 等同于 <code>git fetch</code> + <code>git merge</code>两条指令，解决冲突的步骤包括</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载远端分支到本地缓存区</span></span><br><span class="line">$ git fetch</span><br><span class="line"><span class="comment"># 若本地分支的改动尚未保存，需要先将改动commit掉</span></span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m <span class="string">"xxxx"</span></span><br><span class="line">$ git merge</span><br><span class="line"><span class="comment"># 执行git merge后会显示哪些文件可以自动merge，哪些文件存在冲突</span></span><br><span class="line"><span class="comment"># 自动merge的文件会显示auto merge，冲突的文件会显示大写的CONFICT</span></span><br><span class="line"><span class="comment"># 然后就需要定位到存在冲突的文件中解决冲突</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 解决完冲突后，下面两种方法都是可以的</span></span><br><span class="line"><span class="comment"># 方法1</span></span><br><span class="line">$ git rebase --<span class="built_in">continue</span></span><br><span class="line"><span class="comment"># 方法2</span></span><br><span class="line">$ git add .</span><br><span class="line">$ git commit -m <span class="string">"xxxx"</span></span><br></pre></td></tr></table></figure><h2 id="2-恢复本地缓冲区中暂存的分支"><a href="#2-恢复本地缓冲区中暂存的分支" class="headerlink" title="2. 恢复本地缓冲区中暂存的分支"></a>2. 恢复本地缓冲区中暂存的分支</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git stash</span><br><span class="line"><span class="comment"># xxxxxxxxxxxxxx</span></span><br><span class="line">$ git stash pop</span><br></pre></td></tr></table></figure><h2 id="3-新分支创建、合并与删除"><a href="#3-新分支创建、合并与删除" class="headerlink" title="3. 新分支创建、合并与删除"></a>3. 新分支创建、合并与删除</h2><p>创建分支、切分支<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看当前分支</span></span><br><span class="line">$ git branch</span><br><span class="line"><span class="comment"># 开一个叫做new-feature的新分支，该操作不会影响到当前正在进行的代码修改工作</span></span><br><span class="line">$ git checkout -b <span class="variable">$&#123;new-branch&#125;</span></span><br><span class="line"><span class="comment"># 如果 $&#123;new-branch&#125; 已存在</span></span><br><span class="line">$ git checkout <span class="variable">$&#123;new-branch&#125;</span></span><br><span class="line"><span class="comment"># 切回主分支</span></span><br><span class="line">$ git checkout master</span><br></pre></td></tr></table></figure></p><p>本地删除分支、远程删除分支<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本地删除分支</span></span><br><span class="line">$ git branch -d <span class="variable">$&#123;new-branch&#125;</span></span><br><span class="line"><span class="comment"># 远程删除分支</span></span><br><span class="line">$ git push origin --delete <span class="variable">$&#123;new-branch&#125;</span></span><br></pre></td></tr></table></figure></p><p>如果本地删除分支前该分支还未合并，会提示使用下面的命令强制销毁，一般除非<code>git branch -d</code>失败，且确定要放弃分支，否则不要直接用这条命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git branch -D <span class="variable">$&#123;new-branch&#125;</span></span><br></pre></td></tr></table></figure></p><p>如果需要在删除前先合并分支，比如需要把<code>new-branch</code>合并到<code>master</code>然后再删除<code>new-branch</code>，那么<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout master</span><br><span class="line">$ git merge <span class="variable">$&#123;new-branch&#125;</span></span><br><span class="line">$ git branch -d <span class="variable">$&#123;new-branch&#125;</span></span><br></pre></td></tr></table></figure></p><h2 id="4-其他"><a href="#4-其他" class="headerlink" title="4. 其他"></a>4. 其他</h2><p><code>git add .</code>是一个很容易错误加入不想要的文件的命令，撤销方法是<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git reset HEAD</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> Manual </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Oral Presentation Notes</title>
      <link href="/2019/01/18/oral-presentation-notes/"/>
      <url>/2019/01/18/oral-presentation-notes/</url>
      
        <content type="html"><![CDATA[<p><strong>The presentation note is used in AAAI-2019 oral presentation</strong></p><a id="more"></a><p>Good morning/afternoon, everyone. It’s honored to be here to present our work “the adversarial attack and detection under the Fisher information metric”. My name is Zhao Chenxiao. It’s joint work with Prof. Fletcher, Yu Mixue, Prof Peng, Prof Zhang, and Prof Shen. </p><p>So let’s start our topic with the notion of adversarial examples. A wide range of researches point out that deep learning models are vulnerable to the adversarial attack. Some specifically designed noise added on the input can be imperceptible for human, but make the models completely failed, and the phenomenon is quite general for various tasks.</p><p>To better understand the adversarial examples, a fundamental problem is how to measure the vulnerability of the deep learning models. There are many ways to generate adversarial examples, but is there any way to describe the models’ robustness to these attacks under the same framework? One of the ideas is to use the worst case perturbations. If we can quantify it or optimize a surrogate of it, the robustness can be progressively improved by adversarial training. Another stream of study assumes that the distributions of the adversarial examples and normal samples are different. Using some characteristics that can describe the difference between them, this leads to the approach of adversarial detection.</p><p><strong>Our approach combines both the adversarial attack and detection under the same framework.</strong> We assume that there exists a metric tensor in the output space, and the mapping of the neural network from the input space to the output space makes a pullback. That means we have a metric that can be used to characterize how the perturbation on the input will influence the output. Given an input sample, if we use \eta to denote the perturbation vector, the variation of the output in the tangent space will be given by this equation. As can be seen, they are related by both the Jacobian matrix and the metric tensor.</p><p>So let’s skip the mathematical definitions first, say, if we already have a well-defined metric tensor $G^{x}$ in the data space, how do we formulate the objective function for adversarial perturbations? It is easy to figure out that the vulnerability can be defined by the quadratic form, where we constrain the norm of the perturbation on an epsilon sphere. The solution is also very simple: the optimal adversarial perturbation $\eta$ is the greatest eigenvector. So let’s get back to the question: <strong>how to define the metric tensor G?</strong></p><p><strong>Intrinsically, why should the metric for the output predictions be non-linear?</strong> The basic idea is that, the model should be regarded as not only a function mapping, but also a probabilistic model. The set of all probability distributions conditioned on the model parameter is a manifold. So we don’t use Euclidean distance to measure to distance between distributions. </p><p>Among all the distance measures, the metric defined by Fisher information has been proved to be the unique measure that is invariant to reparameterization. It is obviously positive semi-definite by definition. There are many other theoretical benefits. Please refer to this reference.</p><p>For adversarial attacks, the input $x$ is the only changeable variable. We can obtain the following form of FIM using some exchange of variables. But more specifically, when compute the matrix numerically, what is $p(y|x)$ here? Basically we have two choices, the model distribution or empirical distribution. The model distribution can not be directly observed but really reflects some intrinsic features of the model, like uncertainty. The empirical distribution is just a multi-dimensional discrete distribution given by the last softmax layer. </p><p>Using different definitions, here are some ways to compute the matrix. The vanilla approach is to use the explicit form of the Jacobian, and this would be computationally expensive. Another way is to compute the second derivative of the KL divergence. The adversarial training based on this theory is known as the virtual adversarial training.</p><p>So our solution is to use the empirical distribution to compute the Fisher, which is the expectation over the outer product. This can give us many engineering benefits. First of all, it is easy to compute, providing one is already calculating the gradient. Another advantage is that this form make it a lot easier to calculate the matrix-vector-product without access to the explicit form of the matrix, and some eigen-decomposition methods, incuding power iteration and Lanczos method, only require matrix-vector-product to compute the eigenvalues and eigenvectors.</p><p><strong>When the algorithm is applied on large datasets. We still have two problems.</strong> The first one is that if input sample X is high dimensional, the matrix G will be too big that one can not even load it into memory. As mentioned before, this can be solved using power iteration or Lanczos method. Both of them only need to matrix-vector-product instead of the explicit matrices. One thing to note is that the Lanczos algorithm can calculate a group of eigenvalues and the eigenvectors instead of just the maximum one, and it’s particularly fast for sparse matrices.</p><p>Another problem is that for datasets like ImageNet, there are 1000 classes. In this case it will be hard to implement the expectation in parallel. Our solution is to use Monte-Carlo sampling from the empirical distribution $r$. </p><p>We empirically find the output probabilities given by ImageNet models is long tail, so using only about 1/5 times of sampling is sufficient to guarantee the performance.</p><p>Here are some comparisons of the attack using our algorithm, where we have compared our results with two gradient based methods. The horizontal axis is the fooling rate. The interesting part is that our method often gets higher fooling rate in one-step attack case. But if you use the algorithm iteratively the results get very similar to the results of the iterative variant of the fast gradient method.</p><p>This figure shows the relationship between the vulnerability of the models and the eigenvalues. The vertical axis is the $\ell_{2}$ norm of the least perturbations, which is obtained via binary search along the direction of our attack. The horizontal axis is the logarithm of eigenvalues. As we can see the empirical evidence shows a linear relationship between them: The larger the eigenvalues are, the more vulnerable the model is in the corresponding subspaces.</p><p>This suggests that we can use the eigenvalues as the vulnerability characteristic. This figure further shows the eigenvalues of adversarial examples and normal samples are distributed differently. In the left figure, we can see adding Gaussian noise on samples does not really change the eigenvalues of them, but the adversarial examples are more likely to have larger eigenvalues. In the right figure, we find that if we modify the samples along the direction of the perturbation, most of the eigenvalues of the samples are rapidly increasing with the increasing of the perturbation size.</p><p>Based on the observation, we use an auxiliary classifier to distinguish the adversarial examples. For practical reasons we use the logarithm of eigenvalues as features. We use the aforementioned Lanczos method to get a group of eigenvalues instead of just the maximum one. Motivated by the previous literature, we also add noisy samples into the positive training set. This works very well on enhancing the generalization ability of the classifier.</p><p>Some of our evaluations are shown here. We compare our method with the kernel density estimation and Bayesian uncertainty. We observe that our method achieves good performance on MNIST. </p><p>The performance drops a little bit on CIFAR-10, but we can see the method still works nice when recognizing the three attacks on the right side.</p><p>This form shows the generalization ability of our classifier trained using only one kind of attacks. As we can see here, our proposed method generalizes well on $\ell_{2}$ and $\ell_{\infty}$ norm attacks, but it fails to generalize to JSMA, which is an L0 norm case. As we know the $\ell_{0}$ norm space is discrete, making its topological structure a lot different with the other norms. From my personal point of view, it will be an interesting future work to extend the idea to a more general framework.</p><p>That’s all. Thank you.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Personal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDNN_STATUS_NOT_INITIALIZED</title>
      <link href="/2019/01/11/cudnn-handle-initialization-failed/"/>
      <url>/2019/01/11/cudnn-handle-initialization-failed/</url>
      
        <content type="html"><![CDATA[<h2 id="错误信息"><a href="#错误信息" class="headerlink" title="错误信息"></a>错误信息</h2><a id="more"></a><p>服务器之前的tensorflow环境莫名其妙地坏掉了，定义graph的时候没有问题，一旦代码运行到<code>sess.run()</code>，就会触发报错</p><blockquote><p>UnknownError (see above for traceback): Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.</p></blockquote><p>另一种报错信息是这样的</p><blockquote><p>2019-01-11 15:54:13.351947: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED<br>2019-01-11 15:54:13.352012: E tensorflow/stream_executor/cuda/cuda_dnn.cc:381] Possibly insufficient driver version: 384.130.0<br>2019-01-11 15:54:13.352024: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED<br>2019-01-11 15:54:13.352042: E tensorflow/stream_executor/cuda/cuda_dnn.cc:381] Possibly insufficient driver version: 384.130.0<br>2019-01-11 15:54:13.352049: F tensorflow/core/kernels/conv_grad_input_ops.cc:981] Check failed: stream-&gt;parent()-&gt;GetConvolveBackwardDataAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(stream-&gt;parent()), &amp;algorithms)<br>Aborted (core dumped)</p></blockquote><h2 id="环境"><a href="#环境" class="headerlink" title="环境:"></a>环境:</h2><ul><li>Linux Ubuntu 16.04.3</li><li>tensorflow 1.12.0</li><li>CUDA 9.1</li><li>cudnn 7.0</li></ul><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>StackOverFlow+Github+CSDN搜索得到的解决方案经测试全部无效，记录如下</p><ul><li>StackOverFlow上很多人说这是由于GPU上已经有进程导致的，因而要在代码中加入下面的代码来限制GPU占用，经测试无效<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(config=config) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run()</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></li><li>针对上面第二种报错，CSDN上有人说是cudnn版本太低而导致的，遂重装cudnn，换新版本，经测试无效</li><li>使用sudo权限运行代码，即使用<code>sudo python baseline.py</code>而不是<code>python baseline.py</code>。经测试，这种方法不会触发上述报错信息，但程序运行到<code>sess.run()</code>的时候会卡住并停止响应，必须手动kill才能真正的结束进程</li><li>tensorflow版本太高导致，强制pip重装低版本tensorflow，原博客给出的命令为<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip3 install --upgrade --force-reinstall tensorflow-gpu==1.9.0 --user</span><br></pre></td></tr></table></figure>经测试无效，且这样下载的tensorflow-gpu版本会是python 2.7的，这显然不合理，因此将其改为以下命令，后经测试无效<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow-gpu==1.9.0</span><br></pre></td></tr></table></figure></li></ul><h2 id="最终解决问题的方案"><a href="#最终解决问题的方案" class="headerlink" title="最终解决问题的方案"></a><b style="color: red;">最终解决问题的方案</b></h2><p>仔细研究了一下报错信息，似乎是说NVIDIA驱动的版本太低，和cudnn的版本不匹配，因而想到更新NVIDIA驱动。</p><p>安装或更新NVIDIA驱动的文章网上极多，这里列出几个查资料的时候读过的</p><ul><li><a href="https://blog.csdn.net/ghw15221836342/article/details/79571559" target="_blank" rel="noopener">[专业亲测]Ubuntu16.04安装Nvidia显卡驱动（cuda）—解决你的所有困惑</a></li><li><a href="https://blog.csdn.net/linhai1028/article/details/79445722" target="_blank" rel="noopener">Ubuntu下安装nvidia显卡驱动（安装方式简单）</a></li><li><a href="https://blog.csdn.net/u014797226/article/details/79626693" target="_blank" rel="noopener">Ubuntu 16.04安装NVIDIA驱动</a></li></ul><p>这些文章的内容大同小异，总结下来有以下几个步骤</p><ul><li>开始安装驱动之前要把Xorg或nouveau一类会占用显卡资源的进程全部关掉</li><li>用bash或者chmod一类的方法执行.run文件，后面加<code>–no-opengl-files</code>参数</li><li><code>sudo reboot</code></li></ul><p>安装结束后发现<code>nvidia-smi</code>无法正常显示，报错信息为</p><blockquote><p>Failed to initialize NVML: Driver/library version mismatch.</p></blockquote><p>抓耳挠腮，只得再卸载nvidia驱动相关的一些组建，参考博客</p><ul><li><a href="https://comzyh.com/blog/archives/967/" target="_blank" rel="noopener">解决Driver/library version mismatch</a></li><li><a href="https://my.oschina.net/wangsifangyuan/blog/1606093" target="_blank" rel="noopener">NVIDIA驱动问题解决方案：Failed to initialize NVML: driver/library version mismatch</a></li><li><a href="https://blog.csdn.net/jiandanjinxin/article/details/80688900" target="_blank" rel="noopener"><strong>最有用的一篇博客</strong></a></li></ul><p>在命令行中执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo rmmod nvidia_uvm</span><br><span class="line">$ sudo rmmod nvidia_modeset</span><br><span class="line">$ sudo rmmod nvidia</span><br></pre></td></tr></table></figure><p>如果执行过程中报错说有其他项目对这几个存在依赖，则先行rmmod掉存在依赖的项目。执行结束后再<code>nvidia-smi</code>，发现驱动的版本也成功从384.0升级到了390.25，且之前的cudnn问题也误打误撞地解决了</p><div align="center">    <img src="http://pic.downyi.com/upload/2018-8/2018815183334229320.jpg" width="500" /></div>]]></content>
      
      
      
        <tags>
            
            <tag> Manual </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构类问题总结</title>
      <link href="/2019/01/10/data-structure-like-problems/"/>
      <url>/2019/01/10/data-structure-like-problems/</url>
      
        <content type="html"><![CDATA[<h2 id="基础：堆排序"><a href="#基础：堆排序" class="headerlink" title="基础：堆排序"></a>基础：堆排序</h2><a id="more"></a><p>算法分为两步</p><ul><li><strong>建堆</strong>，对于所有的非叶子节点，从右下至左上依次调整节点顺序进行建堆</li><li><strong>出堆</strong>，对对中所有节点，从右下至左上，依次将每个节点与堆顶节点交换，从而使堆顶元素出堆，并调整堆中剩余元素使其仍为一个大顶堆/小顶堆</li></ul><p>需要注意的几个点</p><ul><li>大顶堆构造时比较操作符用大于号，即对于一个节点而言，若其左子节点或右子节点值大于父节点，则进行交换，小顶堆反之</li><li>构造大顶堆最终的排序结果是从小到大的，小顶堆反之</li><li>设堆中元素总数为$N$，若下标从0开始算，则非叶节点的节点范围是0到$\frac{N}{2}-1$；若下标从1开始，则非叶节点的节点范围是1到$\frac{N}{2}$</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">adjust</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> size, <span class="keyword">int</span> root)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> left = ((root + <span class="number">1</span>) &lt;&lt; <span class="number">1</span>) - <span class="number">1</span>, right = (root + <span class="number">1</span>) &lt;&lt; <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> maxidx = root;</span><br><span class="line">    <span class="keyword">if</span> (left &lt; size &amp;&amp; nums[left] &gt; nums[maxidx])</span><br><span class="line">        maxidx = left;</span><br><span class="line">    <span class="keyword">if</span> (right &lt; size &amp;&amp; nums[right] &gt; nums[maxidx])</span><br><span class="line">        maxidx = right;</span><br><span class="line">    <span class="keyword">if</span> (maxidx != root) &#123;</span><br><span class="line">        swap(nums[root], nums[maxidx]);</span><br><span class="line">        adjust(nums, size, maxidx);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">heap_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> size = nums.size();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=size/<span class="number">2</span><span class="number">-1</span>; i&gt;=<span class="number">0</span>; i--) &#123;</span><br><span class="line">        adjust(nums, size, i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=size<span class="number">-1</span>; i&gt;=<span class="number">0</span>; i--) &#123;</span><br><span class="line">        swap(nums[i], nums[<span class="number">0</span>]);</span><br><span class="line">        adjust(nums, i, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="21-Merge-Two-Sorted-Lists"><a href="#21-Merge-Two-Sorted-Lists" class="headerlink" title="21. Merge Two Sorted Lists"></a>21. Merge Two Sorted Lists</h2><p>很简单的题，用了二级指针，4 lines 12ms。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode* <span class="title">mergeTwoLists</span><span class="params">(ListNode* a, ListNode* b)</span> </span>&#123;</span><br><span class="line">    ListNode **p = &amp;a, **q = &amp;b;</span><br><span class="line">    <span class="keyword">while</span> ((*q) != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">while</span> (*p != <span class="literal">NULL</span> &amp;&amp; (*p)-&gt;val &lt;= (*q)-&gt;val)</span><br><span class="line">            p = &amp;((*p)-&gt;next);</span><br><span class="line">        swap(*p, *q);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="23-Merge-k-Sorted-Lists"><a href="#23-Merge-k-Sorted-Lists" class="headerlink" title="23. Merge k Sorted Lists"></a>23. Merge k Sorted Lists</h2><blockquote><p>Merge k sorted linked lists and return it as one sorted list. Analyze and describe its complexity.</p></blockquote><p>21题升级版，hard难度，题意从merge两个链表变成merge k个排序过的链表。</p><h3 id="Approach-1-Compare-one-by-one"><a href="#Approach-1-Compare-one-by-one" class="headerlink" title="Approach 1: Compare one by one"></a>Approach 1: Compare one by one</h3><p>最容易想到的方案就是21题的基础上简单变形，21题中我们沿着指针p遍历链表，直到找到第一个不小于指针q所指的值的节点，然后交换指针，继续遍历；这道题中，只需要把每一步中的比较操作变成遍历k个链表去比较即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mergeKLists</span><span class="params">(self, lists)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type lists: List[ListNode]</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.nodes = []</span><br><span class="line">        head = point = ListNode(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> lists:</span><br><span class="line">            <span class="keyword">while</span> l:</span><br><span class="line">                self.nodes.append(l.val)</span><br><span class="line">                l = l.next</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> sorted(self.nodes):</span><br><span class="line">            point.next = ListNode(x)</span><br><span class="line">            point = point.next</span><br><span class="line">        <span class="keyword">return</span> head.next</span><br></pre></td></tr></table></figure><ul><li><strong>Time complexity</strong>: $O(kN) where $N$ is the total number of nodes in the final returned linked-list.</li><li><strong>Space complexity</strong>: $O(N)$ creating a new linked-list with $N$ nodes.</li></ul><h3 id="Approach-2-Optimize-Approach-1-by-Priority-Queue"><a href="#Approach-2-Optimize-Approach-1-by-Priority-Queue" class="headerlink" title="Approach 2: Optimize Approach 1 by Priority Queue"></a>Approach 2: Optimize Approach 1 by Priority Queue</h3><p>Almost the same as the one above but optimize the comparison process by priority queue. This approach can not only reduce the time complexity, but also significantly simplify the codes.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node_t</span> &#123;</span></span><br><span class="line">    Node_t(ListNode* _node, <span class="keyword">int</span> _val): node(_node), val(_val) &#123;&#125;</span><br><span class="line">    <span class="keyword">bool</span> <span class="keyword">operator</span> &gt; (<span class="keyword">const</span> Node_t&amp; cmp) <span class="keyword">const</span> &#123; <span class="keyword">return</span> val &gt; cmp.val; &#125;</span><br><span class="line">    ListNode *node;</span><br><span class="line">    <span class="keyword">int</span> val;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">ListNode* <span class="title">mergeKLists</span><span class="params">(<span class="built_in">vector</span>&lt;ListNode*&gt;&amp; lists)</span> </span>&#123;</span><br><span class="line">        priority_queue&lt;Node_t, <span class="built_in">vector</span>&lt;Node_t&gt;, greater&lt;Node_t&gt;&gt; que;</span><br><span class="line">        ListNode *head = <span class="keyword">new</span> ListNode(<span class="number">0</span>);</span><br><span class="line">        ListNode *point = head;</span><br><span class="line">        <span class="keyword">for</span> (ListNode *ptr : lists) &#123;</span><br><span class="line">            <span class="keyword">if</span> (ptr != <span class="literal">NULL</span>)</span><br><span class="line">                que.push(Node_t(ptr, ptr-&gt;val));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (!que.empty()) &#123;</span><br><span class="line">            Node_t tmp = que.top();</span><br><span class="line">            que.pop();</span><br><span class="line">            point-&gt;next = <span class="keyword">new</span> ListNode(tmp.val);</span><br><span class="line">            point = point-&gt;next;</span><br><span class="line">            <span class="keyword">if</span> (tmp.node-&gt;next != <span class="literal">NULL</span>) &#123;</span><br><span class="line">                que.push(Node_t(tmp.node-&gt;next, tmp.node-&gt;next-&gt;val));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        point = head-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> head;</span><br><span class="line">        <span class="keyword">return</span> point;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ul><li><strong>Time complexity</strong>: C++中的priority_queue底层是由大顶堆/小顶堆实现的，因此每次插入队列的复杂度为$O(\log(k))$，yielding a total time complexity of $O(N\log(k))$</li><li><strong>Space complexity</strong>: $O(N+k)$, where $O(N)$ for the new returned linked-list, and $O(k)$ for the priority_queue. In most cases, when $k$ is far less than $N$, the space complexity is $O(N)$.</li></ul><h3 id="Other-approaches"><a href="#Other-approaches" class="headerlink" title="Other approaches"></a>Other approaches</h3><p>Refer to <a href="https://leetcode.com/problems/merge-k-sorted-lists/solution/" target="_blank" rel="noopener">the solution panel of this problem</a> for more approaches.</p><h2 id="530-Minimum-Absolute-Difference-in-BST"><a href="#530-Minimum-Absolute-Difference-in-BST" class="headerlink" title="530. Minimum Absolute Difference in BST"></a>530. Minimum Absolute Difference in BST</h2><p>easy难度的题，已经是第二次做这道题了，但是还是花了很长时间才AC。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getMinimumDifference</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">return</span> midOrder(root);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">midOrder</span><span class="params">(TreeNode *root)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">stack</span>&lt;TreeNode*&gt; st;</span><br><span class="line">        TreeNode *p = root;</span><br><span class="line">        <span class="keyword">int</span> cur = <span class="number">1000000</span>, last = <span class="number">-1000000</span>, ans = <span class="number">10000000</span>;</span><br><span class="line">        <span class="keyword">while</span> (p != <span class="literal">NULL</span> || !st.empty()) &#123;</span><br><span class="line">            <span class="keyword">while</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">                st.push(p);</span><br><span class="line">                p = p-&gt;left;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (!st.empty()) &#123;</span><br><span class="line">                p = st.top();</span><br><span class="line">                st.pop();</span><br><span class="line">                </span><br><span class="line">                cur = p-&gt;val;</span><br><span class="line">                <span class="keyword">if</span> (cur - last &lt; ans)</span><br><span class="line">                    ans = cur - last;</span><br><span class="line">                last = p-&gt;val;</span><br><span class="line">                </span><br><span class="line">                p = p-&gt;right;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="222-Count-Complete-Tree-Nodes"><a href="#222-Count-Complete-Tree-Nodes" class="headerlink" title="222. Count Complete Tree Nodes"></a>222. Count Complete Tree Nodes</h2><blockquote><p>Given a complete binary tree, count the number of nodes.</p></blockquote><p>这道题的重点在于如何把完全二叉树的性质利用起来，第一次做没有做出来，<a href="https://leetcode.com/problems/count-complete-tree-nodes/discuss/61953/Easy-short-c%2B%2B-recursive-solution" target="_blank" rel="noopener">讨论区中C++的答案</a>技巧性非常强，先上代码</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">countNodes</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> left = <span class="number">0</span>, right = <span class="number">0</span>;</span><br><span class="line">    TreeNode *p = root, *q = root;</span><br><span class="line">    <span class="keyword">while</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        left += <span class="number">1</span>;</span><br><span class="line">        p = p-&gt;left;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (q != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        right += <span class="number">1</span>;</span><br><span class="line">        q = q-&gt;right;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (left == right)</span><br><span class="line">        <span class="keyword">return</span> (<span class="number">1</span> &lt;&lt; left) - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> countNodes(root-&gt;left) + countNodes(root-&gt;right) + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一眼看上去这份答案的复杂度仍然还是$O(N)$，还比普通的递归数节点多了判断左子树与右子树的操作，但是仔细考虑则不然，给定一颗高度为$h$的完全二叉树，对于每一个节点，可以归纳出以下3种情况</p><ul><li>a) 左子树与右子树的绝对高度都是$h$</li><li>b) 左子树高度为$h$，右子树高度为$h-1$</li><li>c) 左子树与右子树的高度都是$h-1$</li></ul><p>可以想到对于情况a与c，这个结点下的子树都是满二叉树，因而可以直接用公式$2^{d}-1$算出节点数量，只有情况b需要靠递归来计数节点数量</p><p>那么情况b有多少节点呢？可以想到整颗完全二叉树中，这样的节点数量在$O(\log(N))$量级，i.e., 从根结点到叶节点，每层只会有一个节点满足条件b。因此，该算法的复杂度为$O(\log(n)\times{\log(N)})$</p><h2 id="86-Partition-List"><a href="#86-Partition-List" class="headerlink" title="86. Partition List"></a>86. Partition List</h2><blockquote><p>Given a linked list and a value x, partition it such that all nodes less than x come before nodes greater than or equal to x.</p><p>You should preserve the original relative order of the nodes in each of the two partitions.</p></blockquote><h3 id="错误解法：链表快排"><a href="#错误解法：链表快排" class="headerlink" title="错误解法：链表快排"></a>错误解法：链表快排</h3><p>第一眼看到partition就想到链表快排，马上写出代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode* <span class="title">partition</span><span class="params">(ListNode* head, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    ListNode *p = head, *q = head;</span><br><span class="line">    <span class="keyword">while</span> (p != <span class="literal">NULL</span> &amp;&amp; q != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="keyword">while</span> (q != <span class="literal">NULL</span> &amp;&amp; q-&gt;val &lt; x)</span><br><span class="line">            q = q-&gt;next;</span><br><span class="line">        <span class="keyword">if</span> (q != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (p=q; p!=<span class="literal">NULL</span> &amp;&amp; p-&gt;val &gt;= x; )</span><br><span class="line">                p = p-&gt;next;</span><br><span class="line">            <span class="keyword">if</span> (p != <span class="literal">NULL</span>)</span><br><span class="line">                swap(p-&gt;val, q-&gt;val);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但是这样的解法无法满足题意中第二个条件，即经partition后的两个子串节点顺序需要和之前一样，快速排序是不稳定的，显然无法达到这个要求。</p><h3 id="Two-pointers-solution"><a href="#Two-pointers-solution" class="headerlink" title="Two-pointers solution"></a>Two-pointers solution</h3><p>另一种很简单的思路是构造两个链表<code>left</code>和<code>right</code></p><ul><li><code>left</code>链表包含所有值小于x的节点，<code>right</code>链表包含所有值大于等于x的节点</li><li>最后只需要将<code>left</code>链表的尾指针指向<code>right</code>链表的头指针，将<code>right</code>链表的尾指针置为NULL即可</li><li>具体实现时可以用一个指针<code>p</code>对原链表进行遍历，由于<code>left</code>与<code>right</code>链表的尾指针不会超过<code>p</code>，对尾指针的修改不会影响到指针<code>p</code>的下一步遍历，因此该方法的space complexity为$O(1)$</li></ul><p>以下代码</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode* <span class="title">partition</span><span class="params">(ListNode* head, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    ListNode left(0), right(0);</span><br><span class="line">    ListNode *l = &amp;left, *r = &amp;right;</span><br><span class="line">    <span class="keyword">while</span> (head != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        ListNode **ref = head-&gt;val &lt; x? (&amp;l) : (&amp;r);</span><br><span class="line">        (*ref)-&gt;next = head;</span><br><span class="line">        (*ref) = (*ref)-&gt;next;</span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    l-&gt;next = right.next;</span><br><span class="line">    r-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">return</span> left.next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><b style="color:red;">Note</b>: 由于寻址<code>-&gt;</code>运算符优先级大于解引用<code>*</code>，上面代码中while循环里的括号不能省略，这道题虽然很简单，但是第一次提交时在这里debug花了非常多的时间</p><h2 id="108-Convert-Sorted-Array-to-Binary-Search-Tree"><a href="#108-Convert-Sorted-Array-to-Binary-Search-Tree" class="headerlink" title="108. Convert Sorted Array to Binary Search Tree"></a>108. Convert Sorted Array to Binary Search Tree</h2><blockquote><p>Given an array where elements are sorted in ascending order, convert it to a height balanced BST.</p><p>For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of every node never differ by more than 1.</p></blockquote><p>Easy难度的题，用二分查找相同的方法搜索即可，然而基本功退化的厉害，在下标这种corner case卡了很久才做出来</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">BalancedBST</span> &#123;</span></span><br><span class="line">    BalancedBST(TreeNode *ptr): root(ptr) &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">int</span> val)</span> </span>&#123;</span><br><span class="line">        TreeNode **p = &amp;root;</span><br><span class="line">        <span class="keyword">while</span> ((*p) != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> ((*p)-&gt;val &lt; val)</span><br><span class="line">                p = &amp;((*p)-&gt;right);</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((*p)-&gt;val &gt; val)</span><br><span class="line">                p = &amp;((*p)-&gt;left);</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        *p = <span class="keyword">new</span> TreeNode(val);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    TreeNode *root;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">sortedArrayToBST</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        <span class="keyword">int</span> mid = (<span class="number">1</span> + nums.size()) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="function">BalancedBST <span class="title">tree</span><span class="params">(<span class="keyword">new</span> TreeNode(nums[mid - <span class="number">1</span>]))</span></span>;</span><br><span class="line">        makeTree(tree, nums, <span class="number">1</span>, mid - <span class="number">1</span>);</span><br><span class="line">        makeTree(tree, nums, mid + <span class="number">1</span>, nums.size());</span><br><span class="line">        <span class="keyword">return</span> tree.root;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">makeTree</span><span class="params">(BalancedBST&amp; tree, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (right &lt; left)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">int</span> mid = (left + right) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        tree.insert(nums[mid - <span class="number">1</span>]);</span><br><span class="line">        makeTree(tree, nums, left, mid - <span class="number">1</span>);</span><br><span class="line">        makeTree(tree, nums, mid + <span class="number">1</span>, right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="109-Convert-Sorted-List-to-Binary-Search-Tree"><a href="#109-Convert-Sorted-List-to-Binary-Search-Tree" class="headerlink" title="109. Convert Sorted List to Binary Search Tree"></a>109. Convert Sorted List to Binary Search Tree</h2><blockquote><p>Given a singly linked list where elements are sorted in ascending order, convert it to a height balanced BST.</p><p>For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of every node never differ by more than 1.</p></blockquote><p><a href="#108-Convert-Sorted-Array-to-Binary-Search-Tree">上面那道题</a>的变形，输入从数组变成了单链表，主要的问题在于不能像数组一样方便地用下标来reference元素了。</p><p>$O(N\log(N))$的解决思路很容易想到，用类似链表快排的思路即可</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">BalancedBST</span> &#123;</span></span><br><span class="line">    BalancedBST(TreeNode *ptr): root(ptr) &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">int</span> val)</span> </span>&#123;</span><br><span class="line">        TreeNode **p = &amp;root;</span><br><span class="line">        <span class="keyword">while</span> ((*p) != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> ((*p)-&gt;val &lt; val)</span><br><span class="line">                p = &amp;((*p)-&gt;right);</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((*p)-&gt;val &gt; val)</span><br><span class="line">                p = &amp;((*p)-&gt;left);</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        *p = <span class="keyword">new</span> TreeNode(val);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    TreeNode *root;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">TreeNode* <span class="title">sortedListToBST</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (head == <span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        <span class="function">BalancedBST <span class="title">bst</span><span class="params">(<span class="literal">NULL</span>)</span></span>;</span><br><span class="line">        makeTree(bst, head, <span class="literal">NULL</span>);</span><br><span class="line">        <span class="keyword">return</span> bst.root;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">makeTree</span><span class="params">(BalancedBST&amp; tree, ListNode* left, ListNode* right)</span> </span>&#123;</span><br><span class="line">        ListNode *slow = left, *fast = left;</span><br><span class="line">        <span class="keyword">if</span> (left == right || left == <span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (left-&gt;next == right) &#123;</span><br><span class="line">            tree.insert(left-&gt;val);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (slow != right &amp;&amp; fast != right &amp;&amp; fast-&gt;next != right) &#123;</span><br><span class="line">            slow = slow-&gt;next;</span><br><span class="line">            fast = fast-&gt;next;</span><br><span class="line">            <span class="keyword">if</span> (fast != <span class="literal">NULL</span> &amp;&amp; fast != right)</span><br><span class="line">                fast = fast-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        tree.insert(slow-&gt;val);</span><br><span class="line">        makeTree(tree, left, slow);</span><br><span class="line">        makeTree(tree, slow, right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>问题在于$O(N)$的解，讨论区里一个高赞的解法代码写的非常漂亮</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">count</span><span class="params">(ListNode *node)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> size = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (node) &#123;</span><br><span class="line">            ++size;</span><br><span class="line">            node = node-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> size;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">TreeNode *<span class="title">generate</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (n == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        TreeNode *node = <span class="keyword">new</span> TreeNode(<span class="number">0</span>);</span><br><span class="line">        node-&gt;left = generate(n / <span class="number">2</span>);</span><br><span class="line">        node-&gt;val = <span class="built_in">list</span>-&gt;val;</span><br><span class="line">        <span class="built_in">list</span> = <span class="built_in">list</span>-&gt;next;</span><br><span class="line">        node-&gt;right = generate(n - n / <span class="number">2</span> - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> node;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">TreeNode *<span class="title">sortedListToBST</span><span class="params">(ListNode *head)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;<span class="built_in">list</span> = head;</span><br><span class="line">        <span class="keyword">return</span> generate(count(head));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    ListNode *<span class="built_in">list</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Coding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>奇技淫巧类题目总结</title>
      <link href="/2019/01/06/tricks/"/>
      <url>/2019/01/06/tricks/</url>
      
        <content type="html"><![CDATA[<p><em>未完成，待更新。。。</em><br><a id="more"></a></p><h2 id="基础：牛顿法开根号"><a href="#基础：牛顿法开根号" class="headerlink" title="基础：牛顿法开根号"></a>基础：牛顿法开根号</h2><p>头条面试被问过的问题，当时紧张加脑抽没答好，其实是很简单的一个问题</p><p>假设输入数字是$y$，我们要求的目标是$x$，那么用二范数误差目标函数可以写作</p><script type="math/tex; mode=display">\min_{x}\frac{1}{2}(y-x^{2})^{2}</script><p>牛顿法的公式为</p><script type="math/tex; mode=display">x_{t+1}\leftarrow{}x_{t}-\frac{f(x_{t})}{f'(x_{t})}</script><p>那么可以化简得到更新公式</p><script type="math/tex; mode=display">x_{t+1}\leftarrow{} x_{t}-\frac{x^{2}-y}{4x}</script><p>有关这个公式唯一需要注意的边界条件就是x不能为0，否则在后续的迭代中就会出现devided-by-zero的问题。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">float</span> <span class="title">ABS</span><span class="params">(<span class="keyword">const</span> <span class="keyword">float</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> n &gt; <span class="number">0</span> ? n : -n;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">newton</span><span class="params">(<span class="keyword">float</span> y, <span class="keyword">float</span> epsilon=<span class="number">1e-5</span>, <span class="keyword">int</span> maxiter=<span class="number">20</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (ABS(y) &lt; epsilon)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">float</span> x = y, res;</span><br><span class="line">    <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        res = ABS(y - x * x);</span><br><span class="line">        x -= (x * x - y) / (<span class="number">4.0</span> * x);</span><br><span class="line">        cnt += <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">while</span> (cnt &lt; maxiter &amp;&amp; res &gt; epsilon);</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="基础二：乱序数组找第k大的数"><a href="#基础二：乱序数组找第k大的数" class="headerlink" title="基础二：乱序数组找第k大的数"></a>基础二：乱序数组找第k大的数</h2><p>阿里二面中被问到的问题，原问题为乱序数组找中位数，现延伸到更宏观的层面：如何在乱序数组中找到第k大的数</p><p>思路就是partition，问题在于为什么这个算法的复杂度是$O(N)$而不是$O(N\log(N))$</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findKthLargest</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">    partition(nums, <span class="number">0</span>, nums.size() - <span class="number">1</span>, k);</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">0</span>, dep = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> nums[nums.size() - k];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> left, <span class="keyword">int</span> right, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (left &gt;= right)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">int</span> lo = left, hi = right, basis = nums[lo];</span><br><span class="line">    <span class="keyword">while</span> (lo &lt; hi) &#123;</span><br><span class="line">        <span class="keyword">while</span> (hi &gt; lo &amp;&amp; nums[hi] &gt;= basis)</span><br><span class="line">            -- hi;</span><br><span class="line">        <span class="keyword">if</span> (hi &lt;= lo)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        nums[lo] = nums[hi];</span><br><span class="line">        <span class="keyword">while</span> (lo &lt; hi &amp;&amp; nums[lo] &lt;= basis)</span><br><span class="line">            ++ lo;</span><br><span class="line">        <span class="keyword">if</span> (lo &gt;= hi)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        nums[hi] = nums[lo];</span><br><span class="line">    &#125;</span><br><span class="line">    nums[lo] = basis;</span><br><span class="line">    <span class="keyword">if</span> (lo &gt; nums.size() - k) &#123;</span><br><span class="line">        partition(nums, left, lo - <span class="number">1</span>, k);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (lo &lt; nums.size() - k) &#123;</span><br><span class="line">        partition(nums, lo + <span class="number">1</span>, right, k);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Binary-Search"><a href="#Binary-Search" class="headerlink" title="Binary Search"></a>Binary Search</h1><h2 id="240-Search-a-2D-Matrix-II"><a href="#240-Search-a-2D-Matrix-II" class="headerlink" title="240. Search a 2D Matrix II"></a>240. Search a 2D Matrix II</h2><blockquote><p>Write an efficient algorithm that searches for a value in an m x n matrix. This matrix has the following properties:</p><ul><li>Integers in each row are sorted in ascending from left to right.</li><li>Integers in each column are sorted in ascending from top to bottom.</li></ul></blockquote><p>乍看像binary search，一旦思路被绕进binary search就出不来了。。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">searchMatrix</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; matrix, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> m = matrix.size();</span><br><span class="line">    <span class="keyword">if</span> (m == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">int</span> n = matrix[<span class="number">0</span>].size();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> row = <span class="number">0</span>, col = n - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (row &lt; m &amp;&amp; col &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (matrix[row][col] == target)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (matrix[row][col] &gt; target) &#123;</span><br><span class="line">            col--;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            row++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4-Median-of-Two-Sorted-Arrays"><a href="#4-Median-of-Two-Sorted-Arrays" class="headerlink" title="4. Median of Two Sorted Arrays"></a>4. Median of Two Sorted Arrays</h2><blockquote><p>There are two <strong>sorted</strong> arrays nums1 and nums2 of size m and n respectively.</p><p>Find the median of the two sorted arrays. The overall run time complexity should be O(log (m+n)).</p><p>You may assume A and B cannot be both empty.</p></blockquote><p>头条最终面被问过的问题，显然是binary search，但是当时就是没想出来怎么写</p><p>Consider split indices i and j in array A and B correspondingly, visualizing as follows.</p><div class="table-container"><table><thead><tr><th>left_part</th><th>right_part</th></tr></thead><tbody><tr><td>$A_{0},…,A_{i-1}$</td><td>$A_{i},…,A_{m-1}$</td></tr><tr><td>$B_{0},…,B_{j-1}$</td><td>$B_{j},…,b_{n-1}$</td></tr></tbody></table></div><p>Provided that <code>len(left_part) == len(right_part)</code> and <code>min(A[i], B[j]) &gt;= max(A[i - 1], B[j - 1])</code>, the split indices i and j must satisfy</p><script type="math/tex; mode=display">    i+j=\lceil{\frac{1}{2}(m+n)}\rceil=\frac{1}{2}(m+n+1) \\    \Rightarrow j=\frac{1}{2}(m+n+1)-i</script><p>复杂度$O(\log(\min(A,B)))$，这里的min是通过A与B指针交换实现的。看懂了上面的公式，这道题已经完成了80%，但这并不意味着剩下的20%也同样一目了然。剩余的20%与edge case相关，共有这么几类</p><ul><li><code>i == 0 or j == 0</code>: Since A and B can not be both empty<ul><li>if <code>i == 0</code>, find <code>B[j - 1]</code></li><li>if <code>j == 0</code>, find <code>A[i - 1]</code></li></ul></li><li><code>i == m or j == n</code>: <ul><li>if <code>i == m</code>, find <code>B[j]</code></li><li>if <code>j == n</code>, find <code>A[i]</code></li></ul></li><li><code>(m + n) % 2</code>: The definition of median is different for array of even or odd lengths. Specifically, for this problem<ul><li>if <code>(m + n) % 2 == 1</code>: median is <code>max(A[i-1], B[j-1])</code></li><li>if <code>(m + n) % 2 == 0</code>: median is <code>(max_of_left + min_of_right)</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findMedianSortedArrays</span><span class="params">(self, A, B)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">:type nums1: List[int]</span></span><br><span class="line"><span class="string">:type nums2: List[int]</span></span><br><span class="line"><span class="string">:rtype: float</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">m, n = len(A), len(B)</span><br><span class="line"><span class="keyword">if</span> m &gt; n:</span><br><span class="line">A, B, m, n = B, A, n, m</span><br><span class="line"></span><br><span class="line">imin, imax, half_len = <span class="number">0</span>, m, (m + n + <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> imin &lt;= imax:</span><br><span class="line">i = (imin + imax) / <span class="number">2</span></span><br><span class="line">j = half_len - i</span><br><span class="line"><span class="keyword">if</span> i &lt; m <span class="keyword">and</span> B[j<span class="number">-1</span>] &gt; A[i]:</span><br><span class="line"><span class="comment"># i is too small, must increase it</span></span><br><span class="line">imin = i + <span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> A[i<span class="number">-1</span>] &gt; B[j]:</span><br><span class="line"><span class="comment"># i is too big, must decrease it</span></span><br><span class="line">imax = i - <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># i is perfect</span></span><br><span class="line"><span class="keyword">if</span> i == <span class="number">0</span>: max_of_left = B[j<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">elif</span> j == <span class="number">0</span>: max_of_left = A[i<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">else</span>: max_of_left = max(A[i<span class="number">-1</span>], B[j<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (m + n) % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line"><span class="keyword">return</span> max_of_left</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> i == m: min_of_right = B[j]</span><br><span class="line"><span class="keyword">elif</span> j == n: min_of_right = A[i]</span><br><span class="line"><span class="keyword">else</span>: min_of_right = min(A[i], B[j])</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> (max_of_left + min_of_right) / <span class="number">2.0</span></span><br></pre></td></tr></table></figure><h1 id="Back-tracking"><a href="#Back-tracking" class="headerlink" title="Back-tracking"></a>Back-tracking</h1><h2 id="39-Combination-Sum"><a href="#39-Combination-Sum" class="headerlink" title="39. Combination Sum"></a>39. Combination Sum</h2><p>题意是给一个数组，找到这个数组中所有可以加和得到target的组合，每个数字可以使用无限次</p><p>很简单很基础的back-tracking题，复杂度为指数级别，然而我做了将近一个小时，动用了IDE来调bug才做出来，当前代码功力下降可见一斑</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">combinationSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; ans;</span><br><span class="line"><span class="keyword">if</span> (nums.empty())</span><br><span class="line"><span class="keyword">return</span> ans;</span><br><span class="line">sort(nums.begin(), nums.end());</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tmp;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.size(); i++) &#123;</span><br><span class="line">dfs(nums, target, i, tmp, ans);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target, <span class="keyword">int</span> begin, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; tmp, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (target &lt; nums[begin])</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">tmp.push_back(nums[begin]);</span><br><span class="line"><span class="keyword">if</span> (target == nums[begin]) &#123;</span><br><span class="line">ans.push_back(tmp);</span><br><span class="line">tmp.pop_back();</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = begin; i&lt;nums.size(); i++) &#123;</span><br><span class="line">dfs(nums, target - nums[begin], i, tmp, ans);</span><br><span class="line">&#125;</span><br><span class="line">tmp.pop_back();</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="22-Generate-Parentheses"><a href="#22-Generate-Parentheses" class="headerlink" title="22. Generate Parentheses"></a>22. Generate Parentheses</h2><blockquote><p>Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses.</p></blockquote><p>热身难度的back-tracking题，但是想把代码写到比较简洁，需要把思路理顺。这道题的解法并不唯一，这次我的答案是将解空间看成一颗二叉树来求解的</p><p>何解？将每种string的状态看成是二叉树的一个节点，对于每个节点而言，若其不是叶子节点，那么这个节点就会有两种分支情况</p><ul><li>[左子树] 若加入一个左括号合法，则加入一个左括号</li><li>[右子树] 若加入一个右括号合法，则加入一个右括号</li><li>[叶子节点] 若左括号与右括号都无法加入，开始回溯</li></ul><p>那么这道题就转换成了一个遍历二叉树叶子节点的问题，代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; <span class="title">generateParenthesis</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; ans;</span><br><span class="line">        <span class="built_in">string</span> tmp;</span><br><span class="line">        generateParenthesis(n, <span class="number">0</span>, ans, tmp);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">generateParenthesis</span><span class="params">(<span class="keyword">int</span> nleft, <span class="keyword">int</span> nright, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; ans, <span class="built_in">string</span>&amp; s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nleft == <span class="number">0</span> &amp;&amp; nright == <span class="number">0</span>) &#123;</span><br><span class="line">            ans.push_back(s);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (nleft != <span class="number">0</span>) &#123;</span><br><span class="line">            s.push_back(<span class="string">'('</span>);</span><br><span class="line">            generateParenthesis(nleft - <span class="number">1</span>, nright + <span class="number">1</span>, ans, s);</span><br><span class="line">            s.pop_back();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (nright != <span class="number">0</span>) &#123;</span><br><span class="line">            s.push_back(<span class="string">')'</span>);</span><br><span class="line">            generateParenthesis(nleft, nright - <span class="number">1</span>, ans, s);</span><br><span class="line">            s.pop_back();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="51-N-Queens"><a href="#51-N-Queens" class="headerlink" title="51. N-Queens"></a>51. N-Queens</h2><p>经典back-tracking题。处理起来的麻烦之处在于每次back-tracking结束时要把queen占掉的位置还原回去，这一步琐碎且耗时。做这道题的时候干脆每一层递归都将棋盘标志位board变量复制一遍，以一些不易优化的效率换来了代码的干净整洁</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt; <span class="title">solveNQueens</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt; ans;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; <span class="title">tmp</span><span class="params">(n, <span class="built_in">string</span>(n, <span class="string">'.'</span>))</span></span>;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&gt; <span class="title">board</span><span class="params">(n, <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;(n, <span class="literal">true</span>))</span></span>;</span><br><span class="line">        solveNQueens(n, <span class="number">0</span>, board, tmp, ans);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">solveNQueens</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span> row, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&gt;&amp; board, </span></span></span><br><span class="line"><span class="function"><span class="params">                      <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; tmp, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (row == n) &#123;</span><br><span class="line">            ans.push_back(tmp);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (board[row][i]) &#123;</span><br><span class="line">                <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&gt; <span class="title">board_copy</span><span class="params">(board.begin(), board.end())</span></span>;</span><br><span class="line">                setBoard(board_copy, n, row, i);</span><br><span class="line">                tmp[row][i] = <span class="string">'Q'</span>;</span><br><span class="line">                solveNQueens(n, row + <span class="number">1</span>, board_copy, tmp, ans);</span><br><span class="line">                tmp[row][i] = <span class="string">'.'</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setBoard</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&gt;&amp; board, <span class="keyword">int</span> n, <span class="keyword">int</span> row, <span class="keyword">int</span> col)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)</span><br><span class="line">            board[row][i] == <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)</span><br><span class="line">            board[i][col] = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">int</span> r = row, c = col;</span><br><span class="line">        <span class="keyword">while</span> (r &lt; n &amp;&amp; c &lt; n)</span><br><span class="line">            board[r++][c++] = <span class="literal">false</span>;</span><br><span class="line">        r = row, c = col;</span><br><span class="line">        <span class="keyword">while</span> (r &gt;= <span class="number">0</span> &amp;&amp; c &gt;= <span class="number">0</span>)</span><br><span class="line">            board[r--][c--] = <span class="literal">false</span>;</span><br><span class="line">        r = row, c = col;</span><br><span class="line">        <span class="keyword">while</span> (r &lt; n &amp;&amp; c &gt;= <span class="number">0</span>)</span><br><span class="line">            board[r++][c--] = <span class="literal">false</span>;</span><br><span class="line">        r = row, c = col;</span><br><span class="line">        <span class="keyword">while</span> (r &gt;=<span class="number">0</span> &amp;&amp; c &lt; n)</span><br><span class="line">            board[r--][c++] = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="52-N-Queens-II"><a href="#52-N-Queens-II" class="headerlink" title="52. N-Queens II"></a>52. N-Queens II</h2><p>水题，上面的代码稍作改动即可，代码略</p><h2 id="5-Longest-Palindromic-Substring"><a href="#5-Longest-Palindromic-Substring" class="headerlink" title="5. Longest Palindromic Substring"></a>5. Longest Palindromic Substring</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">longestPalindrome</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s.size() &lt;= <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> s;</span><br><span class="line">        <span class="keyword">int</span> odd = <span class="number">0</span>, even = <span class="number">0</span>, ans = <span class="number">0</span>, axis;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;s.size()<span class="number">-1</span>; i++) &#123;</span><br><span class="line">            <span class="comment">// Palindrome length is odd</span></span><br><span class="line">            odd = expandPalindrome(s, i, i);</span><br><span class="line">            <span class="comment">// Palindrome length is even</span></span><br><span class="line">            even = expandPalindrome(s, i, i + <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span> (odd &gt; ans) &#123;</span><br><span class="line">                ans = odd;</span><br><span class="line">                axis = i;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (even &gt; ans) &#123;</span><br><span class="line">                ans = even;</span><br><span class="line">                axis = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ans &amp; <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> s.substr(axis - (ans &gt;&gt; <span class="number">1</span>), ans);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> s.substr(axis - (ans &gt;&gt; <span class="number">1</span>) + <span class="number">1</span>, ans);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">expandPalindrome</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; s, <span class="keyword">int</span> axis, <span class="keyword">int</span> raxis)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = axis, right = raxis;</span><br><span class="line">        <span class="keyword">while</span> (left &gt;=<span class="number">0</span> &amp;&amp; right &lt; s.size() &amp;&amp; s[left] == s[right]) &#123;</span><br><span class="line">            left --; right ++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> right - left - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="219-Contains-Duplicate-II"><a href="#219-Contains-Duplicate-II" class="headerlink" title="219. Contains Duplicate II"></a>219. Contains Duplicate II</h2><p>Easy难度的题，题意如下</p><blockquote><p>Given an array of integers and an integer k, find out whether there are two distinct indices i and j in the array such that nums[i] = nums[j] and the absolute difference between i and j is at most k.</p></blockquote><p>最简单直接的做法是每次迭代开一个大小为k的滑动窗口来查找，复杂度$O(kN)$</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">containsNearbyDuplicate</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (nums.empty())</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nums.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=i+<span class="number">1</span>; j&lt;nums.size() &amp;&amp; j-i&lt;=k; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[j] == nums[i])</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但上面的代码运行时间2000+ms，这种思路必然是非常低效的，可以想到每次迭代都会有很多重复的查找。</p><p>稍微好一点的思路是对每个大小为k的滑动窗口维持一个hash表来查找，假设哈希表增删改查复杂度都是常数级别，则这种做法的复杂度为$O(N)$</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">containsNearbyDuplicate</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (nums.empty())</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (k &gt;= nums.size())</span><br><span class="line">        k = nums.size() - <span class="number">1</span>;</span><br><span class="line">    <span class="function"><span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; <span class="title">table</span><span class="params">(nums.begin(), nums.begin() + k + <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="keyword">if</span> (table.size() != k + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=k+<span class="number">1</span>; i&lt;nums.size(); i++) &#123;</span><br><span class="line">        table.erase(nums[i - k - <span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">if</span> (table.find(nums[i]) != table.end())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        table.insert(nums[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="220-Contains-Duplicate-III"><a href="#220-Contains-Duplicate-III" class="headerlink" title="220. Contains Duplicate III"></a>220. Contains Duplicate III</h2><blockquote><p>Given an array of integers, find out whether there are two distinct indices i and j in the array such that the absolute difference between nums[i] and nums[j] is at most t and the absolute difference between i and j is at most k.</p></blockquote><p>题意要比之前的更加tricky，隐藏了若干edge case。</p><ul><li>t的值可以为负</li><li>nums数组中两个元素相减有可能会超出int的范围</li></ul><p>做这个题的的时候因为要用到set容器红黑树里上界与下界的查找，专门去查了<a href="http://www.cplusplus.com/reference/set/set/lower_bound/" target="_blank" rel="noopener">C++ reference的网站</a>，发现set容器没有查找正好比元素小的节点的API，后发现<code>set&lt;T&gt;::iterator</code>是有<code>--</code>operator的，可以直接用<code>set&lt;T&gt;::lower_bound</code>函数返回的iterator—来拿到所需结果。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">containsNearbyAlmostDuplicate</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">long</span> <span class="keyword">long</span> k, <span class="keyword">long</span> <span class="keyword">long</span> t)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.empty() || t &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (k &gt;= nums.size())</span><br><span class="line">            k = nums.size() - <span class="number">1</span>;</span><br><span class="line">        <span class="function"><span class="built_in">set</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt; <span class="title">table</span><span class="params">(nums.begin(), nums.begin() + k + <span class="number">1</span>)</span></span>;</span><br><span class="line">        <span class="keyword">if</span> (table.size() != k + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> it=table.begin(); it!=table.end(); it++) &#123;</span><br><span class="line">            <span class="keyword">auto</span> cmp = it;</span><br><span class="line">            cmp ++;</span><br><span class="line">            <span class="keyword">if</span> (cmp != table.end() &amp;&amp; *cmp - *it &lt;= t)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=k+<span class="number">1</span>; i&lt;nums.size(); i++) &#123;</span><br><span class="line">            table.erase(nums[i - k - <span class="number">1</span>]);</span><br><span class="line">            <span class="keyword">auto</span> it = table.lower_bound(nums[i]), jt = it;</span><br><span class="line">            <span class="keyword">if</span> (it != table.end() &amp;&amp; *it - nums[i] &lt;= t)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span> (jt != table.begin()) &#123;</span><br><span class="line">                jt --;</span><br><span class="line">                <span class="keyword">if</span> (nums[i] - *jt &lt;= t)</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            table.insert(nums[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="11-Container-With-Most-Water"><a href="#11-Container-With-Most-Water" class="headerlink" title="11. Container With Most Water"></a>11. Container With Most Water</h2><p>很有趣的一道题，思路很巧妙，代码意外地非常简单。</p><blockquote><p>Given n non-negative integers a1, a2, …, an , where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forms a container, such that the container contains the most water.</p><p>Note: You may not slant the container and n is at least 2.</p></blockquote><p><img src="https://s3-lc-upload.s3.amazonaws.com/uploads/2018/07/17/question_11.jpg" width="700"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxArea</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; height)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> maxarea = <span class="number">0</span>, left = <span class="number">0</span>, right = height.size() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">        maxarea = max(maxarea, min(height[left], height[right]) * (right - left));</span><br><span class="line">        <span class="keyword">if</span> (height[left] &lt; height[right])</span><br><span class="line">            left ++;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            right --;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> maxarea;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>How this approach works?</p><p>Initially we consider the area constituting the exterior most lines. Now, to maximize the area, we need to consider the area between the lines of larger lengths. <b style="color:red">If we try to move the pointer at the longer line inwards, we won’t gain any increase in area, since it is limited by the shorter line.</b> But moving the shorter line’s pointer could turn out to be beneficial, as per the same argument, despite the reduction in the width. This is done since a relatively longer line obtained by moving the shorter line’s pointer might overcome the reduction in area caused by the width reduction.</p><p>For further clarification <a href="https://leetcode.com/problems/container-with-most-water/discuss/6099/yet-another-way-to-see-what-happens-in-the-on-algorithm" target="_blank" rel="noopener">click here</a> and for the proof <a href="https://leetcode.com/problems/container-with-most-water/discuss/6089/anyone-who-has-a-on-algorithm" target="_blank" rel="noopener">click here</a>.</p></blockquote><h2 id="382-Linked-List-Random-Node"><a href="#382-Linked-List-Random-Node" class="headerlink" title="382. Linked List Random Node"></a>382. Linked List Random Node</h2><blockquote><p>Given a singly linked list, return a random node’s value from the linked list. Each node must have the same probability of being chosen.</p><p><strong>Follow up:</strong></p><p>What if the linked list is extremely large and its length is unknown to you? Could you solve this efficiently without using extra space?</p></blockquote><p>Resevoir sampling类型的题，要求在一个不知道长度的链表中采样，每个节点被采样到的概率相等，本题只需要采样一个节点</p><p>Resevoir sampling的基本思路：</p><ul><li>构造一个buffer来存储被采样到的节点（针对此题buffer大小为1）</li><li>从链表头指针开始向后遍历，第$k$个节点有$1/k$的概率被采样到</li><li>遍历结束时，返回buffer中的节点元素</li></ul><p>数学性质分析：</p><p>设事件$A_{k}=\{\text{Node k is put into the buffer}\}$，$B_{k}^{i}=\{\text{Node k is replaced by node i}\}$，$R_{k}=\{\text{Node k is returned as a sampled node}\}$，设链表总结点数为$N$，那么链表中第$k$个节点最终被返回的概率为</p><script type="math/tex; mode=display">P(R_{k})=P(A_{k})\prod_{i=k+1}^{N}[1-P(B_{k}^{i}|A_{k})]</script><p>第k个节点被采样到的概率为$\frac{1}{k}$，代入得</p><script type="math/tex; mode=display">P(R_{k})=\frac{1}{k}\prod_{i=k+1}^{N}[1-\frac{1}{i}]=\frac{1}{k}\frac{k}{k+1}\frac{k+1}{k+2}...\frac{N-1}{N}=\frac{1}{N}</script><p>最终的代码</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/** @param head The linked list's head.</span></span><br><span class="line"><span class="comment">        Note that the head is guaranteed to be not null, so it contains at least one node. */</span></span><br><span class="line">    Solution(ListNode* _head): head(_head) &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/** Returns a random node's value. */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getRandom</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ListNode *p = head, *ans = head;</span><br><span class="line">        <span class="keyword">float</span> prob = <span class="number">1.0</span>;</span><br><span class="line">        <span class="keyword">while</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="keyword">float</span> epsilon = <span class="keyword">float</span>(rand()) / <span class="keyword">float</span>(RAND_MAX);</span><br><span class="line">            <span class="keyword">if</span> (epsilon &lt; (<span class="number">1.0</span> / prob))</span><br><span class="line">                ans = p;</span><br><span class="line">            prob += <span class="number">1.0</span>;</span><br><span class="line">            p = p-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans-&gt;val;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    ListNode *head;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Coding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DP类问题总结</title>
      <link href="/2019/01/05/dp-like-problems/"/>
      <url>/2019/01/05/dp-like-problems/</url>
      
        <content type="html"><![CDATA[<p><em>未完成，待更新。。。</em><br><a id="more"></a></p><h2 id="123-Best-Time-to-Buy-and-Sell-Stock-III"><a href="#123-Best-Time-to-Buy-and-Sell-Stock-III" class="headerlink" title="123. Best Time to Buy and Sell Stock III"></a>123. Best Time to Buy and Sell Stock III</h2><p>hard难度，最优解应该是$O(N)$，但我的$O(N^{2})$加了魔法优化trick之后居然歪打正着地跑到了6ms，击败了55.08%的submission。。。</p><p>平方级别复杂度的思路其实是很简单的，题目要求只能进行两次交易，且不能同时进行两笔交易，所以就循环找一个分割点，使得分割点左边的subarray单次交易+分割点右边的subarray单次交易最大即可。单次交易最大化收益的方法见<a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock/" target="_blank" rel="noopener">第121题</a>。</p><p>这种解法大数据量测试会超时，因此想出一个魔法trick来进行效率优化：考虑股价函数$f(t)$为一个光滑函数，若存在极大值或极小值的分割点$s$满足$a\leq{b}\leq{s}\leq{c}\leq{d}$，使得$f(b)-f(a)+f(d)-f(c)$最大。进一步可以想到$f’’(s)&gt;0$或$f’’(s)&lt;0$只需要满足二者一即可。因而可以在迭代分割点的时候，只对极大值点进行计算，代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxProfit</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; prices)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (prices.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> ans = <span class="number">0</span>, tmp = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;prices.size()<span class="number">-1</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (prices[i] &gt; prices[i - <span class="number">1</span>] &amp;&amp; prices[i] &gt;= prices[i + <span class="number">1</span>]) &#123;</span><br><span class="line">                tmp = maxProfit(prices, <span class="number">0</span>, i + <span class="number">1</span>) + maxProfit(prices, i + <span class="number">1</span>, prices.size());</span><br><span class="line">                <span class="keyword">if</span> (tmp &gt; ans)</span><br><span class="line">                    ans = tmp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        tmp = maxProfit(prices, <span class="number">0</span>, prices.size());</span><br><span class="line">        <span class="keyword">return</span> ans &gt; tmp ? ans : tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxProfit</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; prices, <span class="keyword">int</span> begin, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> ans = <span class="number">0</span>, tmp = <span class="number">0</span>, slow = begin, fast = begin;</span><br><span class="line">        <span class="keyword">while</span> (fast + <span class="number">1</span> &lt; end) &#123;</span><br><span class="line">            <span class="keyword">if</span> (prices[fast + <span class="number">1</span>] &gt;= prices[fast]) &#123;</span><br><span class="line">                tmp = prices[fast + <span class="number">1</span>] - prices[slow];</span><br><span class="line">                <span class="keyword">if</span> (tmp &gt; ans)</span><br><span class="line">                    ans = tmp;</span><br><span class="line">                fast ++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                fast ++;</span><br><span class="line">                <span class="keyword">if</span> (prices[fast] &lt; prices[slow])</span><br><span class="line">                    slow = fast;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="64-Minimum-Path-Sum"><a href="#64-Minimum-Path-Sum" class="headerlink" title="64. Minimum Path Sum"></a>64. Minimum Path Sum</h2><blockquote><p>Given a m x n grid filled with non-negative numbers, find a path from top left to bottom right which minimizes the sum of all numbers along its path.</p></blockquote><p>很有意思的一道题，第一眼看题时候的反应是：欸这不是类似于value iteration嘛快赶紧写个DP出来展现一下某扎实的RL功底balabala。。。</p><p>然后就开始写value iteration了，说实话也是学了David Silver课程之后第一次写value iteration，代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> MAX_VAL = <span class="number">0x7fffffff</span>;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">minPathSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; grid)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (grid.empty() || grid[<span class="number">0</span>].empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> m = grid.size(), n = grid[<span class="number">0</span>].size(), comp;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">dp</span><span class="params">(m, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(n, MAX_VAL))</span></span>;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">dp_copy</span><span class="params">(m, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(n, MAX_VAL))</span></span>;</span><br><span class="line">        dp[m - <span class="number">1</span>][n - <span class="number">1</span>] = grid[m - <span class="number">1</span>][n - <span class="number">1</span>];</span><br><span class="line">        dp_copy[m - <span class="number">1</span>][n - <span class="number">1</span>] = grid[m - <span class="number">1</span>][n - <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            comp = dp[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i=m<span class="number">-1</span>; i&gt;=<span class="number">0</span>; i--) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j=n<span class="number">-1</span>; j&gt;=<span class="number">0</span>; j--) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (i == m - <span class="number">1</span> &amp;&amp; j == n - <span class="number">1</span>)</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    dp_copy[i][j] = minAround(dp, i, j);</span><br><span class="line">                    <span class="keyword">if</span> (dp_copy[i][j] != MAX_VAL)</span><br><span class="line">                        dp_copy[i][j] += grid[i][j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            dp.assign(dp_copy.begin(), dp_copy.end());</span><br><span class="line">        &#125; <span class="keyword">while</span> (comp == MAX_VAL || comp &gt; dp[<span class="number">0</span>][<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">minAround</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; dp, <span class="keyword">int</span> row, <span class="keyword">int</span> col)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> up = row + <span class="number">1</span> &lt; dp.size() ? dp[row + <span class="number">1</span>][col] : MAX_VAL;</span><br><span class="line">        <span class="keyword">int</span> down = row - <span class="number">1</span> &gt;= <span class="number">0</span> ? dp[row - <span class="number">1</span>][col] : MAX_VAL;</span><br><span class="line">        <span class="keyword">int</span> left = col - <span class="number">1</span> &gt;= <span class="number">0</span> ? dp[row][col - <span class="number">1</span>] : MAX_VAL;</span><br><span class="line">        <span class="keyword">int</span> right = col + <span class="number">1</span> &lt; dp[<span class="number">0</span>].size() ? dp[row][col + <span class="number">1</span>] : MAX_VAL;</span><br><span class="line">        <span class="keyword">int</span> lval = up &lt; down ? up : down;</span><br><span class="line">        <span class="keyword">int</span> rval = left &lt; right ? left : right;</span><br><span class="line">        <span class="keyword">return</span> lval &lt; rval ? lval : rval;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>写value iteration有两个需要注意的点</p><ul><li>dp数组必须要有两个，dp_copy用来做更新，dp用来做旧的参照，每次迭代结束后要将dp_copy完整地复制给dp</li><li>迭代终止条件：按照Contractiom Mapping Theorem，value iteration每一步迭代必定会使得解空间可行域变小。对于这道题而言，<code>dp[0][0]</code>的值应该是（非严格）单调递减的。因而迭代终止条件可以表达为<ul><li>若<code>dp[0][0] == MAX_VAL</code>，则迭代一定还未结束。其原因在于这道题中，一定存在一条路径可以从左上到右下</li><li>若<code>dp[0][0]</code>在某一次迭代中值变小，则迭代一定未结束</li></ul></li><li>是否有更好的终止条件，现在暂时没有想出来</li></ul><p>高高兴兴地写完提交，200+ms，足以说明这份代码的效率之低下。回头重新看题，发现漏掉了一个条件</p><blockquote><p><strong>Note</strong>: You can only move either down or right at any point in time.</p></blockquote><div align="center">    <img src="http://www.xfdown.com/uploads/allimg/1804/2-1P409151353232.png" width="200"></div><p>有了这个条件，这道题就会容易得多。事实上用value iteration来解这道题纯属杀鸡用牛刀。这里将value iteration的适用范围与此题做简单对比</p><ul><li>MDP的state transition是non-deterministic的</li><li>value iteration主要解决的问题是faster convergence和loopy MDP</li></ul><p>既然只能往下或往右走，那这道题就简化了很多，有DP和BFS求最短路两种解法，效率上应该是DP更优。一个显而易见的结论是每个<code>dp[i][j]</code>作为子问题也是最优的，那么递推公式应该为</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i][j] = min(dp[i<span class="number">-1</span>][j], dp[i][j<span class="number">-1</span>]) + grid[i][j]</span><br></pre></td></tr></table></figure><p>最终的8ms代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">minPathSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; grid)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (grid.empty() || grid[<span class="number">0</span>].empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> m = grid.size(), n = grid[<span class="number">0</span>].size(), comp;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">dp</span><span class="params">(m, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(n, <span class="number">0</span>))</span></span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// boundary condition</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = grid[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;m; i++) </span><br><span class="line">            dp[i][<span class="number">0</span>] = dp[i - <span class="number">1</span>][<span class="number">0</span>] + grid[i][<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;n; i++)</span><br><span class="line">            dp[<span class="number">0</span>][i] = dp[<span class="number">0</span>][i <span class="number">-1</span>] + grid[<span class="number">0</span>][i];</span><br><span class="line">        <span class="comment">// transition function</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;m; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;n; j++) &#123;</span><br><span class="line">                dp[i][j] = min(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>]) + grid[i][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[m - <span class="number">1</span>][n - <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="72-Edit-Distance"><a href="#72-Edit-Distance" class="headerlink" title="72. Edit Distance"></a>72. Edit Distance</h2><p>经典DP问题，相比于之前的DP题，这个问题的最优子问题性质没有那么一目了然，直接放讲解与答案</p><blockquote><p>The idea would be to reduce the problem to simple ones. For example, there are two words, horse and ros and we want to compute an edit distance D for them. One could notice that it seems to be more simple for short words and so it would be logical to relate an edit distance D[n][m] with the lengths n and m of input words.</p><p>Let’s go further and introduce an edit distance D[i][j] which is an edit distance between the first i characters of word1 and the first j characters of word2.</p><p>It turns out that one could compute D[i][j], knowing D[i - 1][j], D[i][j - 1] and D[i - 1][j - 1]</p><p>If the last character is the same, i.e. word1[i] = word2[j] then</p><script type="math/tex; mode=display">dp[i][j]=1+\min(dp[i-1][j], dp[i][j-1],dp[i-1][j-1]-1)</script><p>and if not, i.e. word1[i] != word2[j] we have to take into account the replacement of the last character during the conversion.</p><script type="math/tex; mode=display">dp[i][j]=1+\min(dp[i-1][j], dp[i][j-1],dp[i-1][j-1])</script></blockquote><p>Runtime 12ms，time complexity $O(mn)$ where m is the length of s1 and n is the length of s2</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">minDistance</span><span class="params">(<span class="built_in">string</span>&amp; s1, <span class="built_in">string</span>&amp; s2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s1.empty() &amp;&amp; s2.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (s1.empty() || s2.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">abs</span>(s1.size() - s2.size());</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">dp</span><span class="params">(s1.size() + <span class="number">1</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(s2.size() + <span class="number">1</span>))</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=s1.size(); i++)</span><br><span class="line">            dp[i][<span class="number">0</span>] = i;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=s2.size(); i++)</span><br><span class="line">            dp[<span class="number">0</span>][i] = i;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=s1.size(); i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=s2.size(); j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s1[i - <span class="number">1</span>] == s2[j - <span class="number">1</span>]) &#123;</span><br><span class="line">                    dp[i][j] = min(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>]);</span><br><span class="line">                    dp[i][j] = min(dp[i][j], dp[i - <span class="number">1</span>][j - <span class="number">1</span>] - <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[i][j] = min(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>]);</span><br><span class="line">                    dp[i][j] = min(dp[i][j], dp[i - <span class="number">1</span>][j - <span class="number">1</span>]) + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[s1.size()][s2.size()];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">constexpr</span> <span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">abs</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> n &gt; <span class="number">0</span> ? n : -n;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="96-Unique-Binary-Search-Trees"><a href="#96-Unique-Binary-Search-Trees" class="headerlink" title="96. Unique Binary Search Trees"></a>96. Unique Binary Search Trees</h2><blockquote><p>Given n, how many structurally unique BST’s (binary search trees) that store values 1 … n?</p></blockquote><p>这个题有一定的难度，虽说是DP，但不像其他的DP题有明显的套路痕迹。</p><p>这道题目的最优子问题是什么？可以想到，一颗有n个节点的树可能有的形状数量显然是和节点数量相关的，具体来说，如果$K_{i}$代表有i个节点的树有多少种形状，那么</p><script type="math/tex; mode=display">K_{i}=\sum_{j=0}^{i-1}K_{j}K_{i-1-j}</script><p>再加上边界条件$K_{0}=1, K_{1}=1$，代码其实非常简短</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">numTrees</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(n + <span class="number">1</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=n; i++) &#123;</span><br><span class="line">        dp[i] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;i; j++) &#123;</span><br><span class="line">            dp[i] += dp[i - <span class="number">1</span> - j] * dp[j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[n];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="95-Unique-Binary-Search-Trees-II"><a href="#95-Unique-Binary-Search-Trees-II" class="headerlink" title="95. Unique Binary Search Trees II"></a>95. Unique Binary Search Trees II</h2><p>在96题基础上的延伸，虽说是延伸，这道题目的DP套路同样不容易看出来。</p><p>刚看到这个题目的时候想到的一种解法是这样的</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">vector</span>&lt;TreeNode*&gt; <span class="title">generateTrees</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">vector</span>&lt;TreeNode*&gt;(<span class="number">0</span>);</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">perm</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="built_in">vector</span>&lt;TreeNode*&gt; ans;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++) &#123;</span><br><span class="line">        perm[i - <span class="number">1</span>] = i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        TreeNode* root = generateTreeFromVector(perm);</span><br><span class="line">        ans.push_back(root);</span><br><span class="line">    &#125; <span class="keyword">while</span> (next_permutation(perm.begin(), perm.end()));</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">TreeNode* <span class="title">generateTreeFromVector</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; perm)</span> </span>&#123;</span><br><span class="line">    TreeNode *root = <span class="keyword">new</span> TreeNode(perm[<span class="number">0</span>]);</span><br><span class="line">    TreeNode **p = &amp;root;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;perm.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">while</span> ((*p) != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (perm[i] &lt; (*p)-&gt;val)</span><br><span class="line">                p = &amp;((*p)-&gt;left);</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                p = &amp;((*p)-&gt;right);</span><br><span class="line">        &#125;</span><br><span class="line">        *p = <span class="keyword">new</span> TreeNode(perm[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>但是这种解法是彻彻底底的错误</strong>。如果用上一道题的递推公式验算，显然unique BST的数量和permutation的数量是不等的，例如给定数组[1, 3, 2]和[1, 2, 3]，二者构成的是同一颗BST。</p><p>正确的思路仍然沿用了上面那道题的思想，对于有n个节点的BST，若其根节点为i，那么其左子树就会有i-1个节点，右子树有n-i个节点。我们需要遍历所有1-n的数字为根的情况，并对其左子树和右子树进行递归。</p><p>出递归的边界条件为：当左子树或右子树的节点数量为0时，直接返回NULL</p><p>12ms代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;TreeNode*&gt; <span class="title">generateTrees</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">vector</span>&lt;TreeNode*&gt;(<span class="number">0</span>);</span><br><span class="line">        <span class="built_in">vector</span>&lt;TreeNode*&gt; ans;</span><br><span class="line">        recursion(<span class="number">1</span>, n, ans);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">recursion</span><span class="params">(<span class="keyword">int</span> start, <span class="keyword">int</span> end, <span class="built_in">vector</span>&lt;TreeNode*&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (start &gt; end) &#123;</span><br><span class="line">            ans.push_back(<span class="literal">NULL</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=start; i&lt;=end; i++) &#123;</span><br><span class="line">            <span class="built_in">vector</span>&lt;TreeNode*&gt; left_ptrs, right_ptrs;</span><br><span class="line">            recursion(start, i - <span class="number">1</span>, left_ptrs);</span><br><span class="line">            recursion(i + <span class="number">1</span>, end, right_ptrs);</span><br><span class="line">            <span class="keyword">for</span> (TreeNode* left: left_ptrs) &#123;</span><br><span class="line">                <span class="keyword">for</span> (TreeNode* right: right_ptrs) &#123;</span><br><span class="line">                    TreeNode *root = <span class="keyword">new</span> TreeNode(i);</span><br><span class="line">                    root-&gt;left = left;</span><br><span class="line">                    root-&gt;right = right;</span><br><span class="line">                    ans.push_back(root);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="221-Maximal-Square"><a href="#221-Maximal-Square" class="headerlink" title="221. Maximal Square"></a>221. Maximal Square</h2><blockquote><p>Given a 2D binary matrix filled with 0’s and 1’s, find the largest square containing only 1’s and return its area.</p></blockquote><p>这道题的难点在于</p><ul><li>递推公式不是很显然，无法归纳到几类典型的DP问题中</li><li>一维DP的优化思路非常不直观</li></ul><h3 id="基础DP解法"><a href="#基础DP解法" class="headerlink" title="基础DP解法"></a>基础DP解法</h3><p>定义$dp(i,j)$的值为从左上顶点到坐标$(i,\ j)$为止的最大正方形边长，则Solution中给出的递推公式是这样的</p><script type="math/tex; mode=display">dp(i, j) = \min(dp(i-1, j), dp(i, j-1), dp(i-1, j-1)) + 1 \quad{} \text{if}\ matrix(i-1, j-1)==1</script><p>虽然LeetCode上给出了下面这张图片来便于理解，但是这个递推公式仍然非常不直观：一般如果DP所求解的最终目标是一个离散空间中的最大值的话，那么递推公式中也应当包含$\max$项，然而这个递推公式中是用$\min$来确保遍历中所到达的每个点都是正方形</p><p>除此以外，我们还需要在每一次更新时记录最大边长的值</p><div align="center">    <img src="https://leetcode.com/media/original_images/221_Maximal_Square.PNG?raw=true" width=550px></div><p>代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maximalSquare</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt;&gt;&amp; matrix)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> m = matrix.size();</span><br><span class="line">    <span class="keyword">if</span> (m == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> n = matrix[<span class="number">0</span>].size();</span><br><span class="line">    <span class="keyword">int</span> maxlen = <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">dp</span><span class="params">(m + <span class="number">1</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(n + <span class="number">1</span>, <span class="number">0</span>))</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=m; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=n; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (matrix[i - <span class="number">1</span>][j - <span class="number">1</span>] == <span class="string">'1'</span>) &#123;</span><br><span class="line">                dp[i][j] = min(min(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>]), dp[i - <span class="number">1</span>][j - <span class="number">1</span>]) + <span class="number">1</span>;</span><br><span class="line">                maxlen = max(maxlen, dp[i][j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> maxlen * maxlen;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h3><p>这里为什么可以用内存优化策略？如果我们仔细观察基础DP解法的代码，会发现每次更新中，只用到了$dp(i-1,j),dp(i,j-1)$与$dp(i-1,j-1)$三个值，其中$dp(i,j-1)$是遍历中上一次所到达的坐标，因此只需要多开一个prev变量来记录之前的状态即可，而$dp(i-1,j-1)$可以用上一次dp更新前的数值来表示，因而也可以省去，唯一真正需要的是之前column的最大正方形边长。</p><p>因此，经优化后，可以用$dp(i)$表示到达第i列为止最大的正方形边长，代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maximalSquare</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt;&gt;&amp; matrix)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> m = matrix.size(), maxlen = <span class="number">0</span>, prev = <span class="number">0</span>, tmp;</span><br><span class="line">    <span class="keyword">if</span> (m == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> n = matrix[<span class="number">0</span>].size();</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(n + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=m; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=n; j++) &#123;</span><br><span class="line">            tmp = dp[j];</span><br><span class="line">            <span class="keyword">if</span> (matrix[i - <span class="number">1</span>][j - <span class="number">1</span>] == <span class="string">'1'</span>) &#123;</span><br><span class="line">                dp[j] = min(min(dp[j - <span class="number">1</span>], prev), dp[j]) + <span class="number">1</span>;</span><br><span class="line">                maxlen = max(maxlen, dp[j]);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                dp[j] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            prev = tmp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> maxlen * maxlen;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="673-Number-of-Longest-Increasing-Subsequence"><a href="#673-Number-of-Longest-Increasing-Subsequence" class="headerlink" title="673. Number of Longest Increasing Subsequence"></a>673. Number of Longest Increasing Subsequence</h2><blockquote><p>Given an unsorted array of integers, find the number of longest increasing subsequence.</p></blockquote><ul><li>数组<code>len[i]</code>表示到达下标i时，最长的LIS长度为<code>len[i]</code></li><li>数组<code>cnt[i]</code>表示到达下标i时，有<code>cnt[i]</code>种长度为<code>len[i]</code>的LIS</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findNumberOfLIS</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = nums.size(), maxlen = <span class="number">1</span>, ans = <span class="number">0</span>;</span><br><span class="line">    vector&lt;int&gt; cnt(n, 1), len(n, 1);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i] &gt; nums[j]) &#123;</span><br><span class="line">                <span class="keyword">if</span> (len[j] + <span class="number">1</span> &gt; len[i]) &#123;</span><br><span class="line">                    len[i] = len[j] + <span class="number">1</span>;</span><br><span class="line">                    cnt[i] = cnt[j];</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (len[j] + <span class="number">1</span> == len[i]) &#123;</span><br><span class="line">                    cnt[i] += cnt[j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        maxlen = max(maxlen, len[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) </span><br><span class="line">        <span class="keyword">if</span> (len[i] == maxlen)</span><br><span class="line">            ans += cnt[i];</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="87-Cheapest-Flights-Within-K-Stops"><a href="#87-Cheapest-Flights-Within-K-Stops" class="headerlink" title="87. Cheapest Flights Within K Stops"></a>87. Cheapest Flights Within K Stops</h2><blockquote><p>There are n cities connected by m flights. Each fight starts from city u and arrives at v with a price w.</p><p>Now given all the cities and flights, together with starting city src and the destination dst, your task is to find the cheapest price from src to dst with up to k stops. If there is no such route, output -1.</p></blockquote><p>起初认为是BFS水题，结果BFS写了半天也没AC，实现代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> MAX_DIST = <span class="number">0x7fffffff</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findCheapestPrice</span><span class="params">(<span class="keyword">int</span> n, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; edges, <span class="keyword">int</span> src, <span class="keyword">int</span> dst, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">queue</span>&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt; que;</span><br><span class="line">        que.push(pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;(src, <span class="number">0</span>));</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dist</span><span class="params">(n, MAX_DIST)</span></span>;</span><br><span class="line">        dist[src] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (!que.empty()) &#123;</span><br><span class="line">            pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; from = que.front();</span><br><span class="line">            <span class="keyword">if</span> (from.second &gt; k)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            que.pop();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; e: edges) &#123;</span><br><span class="line">                <span class="keyword">if</span> (e[<span class="number">0</span>] == from.first) &#123;</span><br><span class="line">                    que.push(pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;(e[<span class="number">1</span>], from.second + <span class="number">1</span>));</span><br><span class="line">                    <span class="keyword">if</span> (dist[from.first] != MAX_DIST &amp;&amp; dist[from.first] + e[<span class="number">2</span>] &lt; dist[e[<span class="number">1</span>]])</span><br><span class="line">                        dist[e[<span class="number">1</span>]] = dist[from.first] + e[<span class="number">2</span>];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (dist[dst] == MAX_DIST)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> dist[dst];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>问题出在哪里？问题在于距离更新的这一行</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (dist[from.first] != MAX_DIST &amp;&amp; dist[from.first] + e[<span class="number">2</span>] &lt; dist[e[<span class="number">1</span>]])</span><br><span class="line">    dist[e[<span class="number">1</span>]] = dist[from.first] + e[<span class="number">2</span>];</span><br></pre></td></tr></table></figure><p>这里的更新每次都需要去直接更改dist，可以想到，输入变量edges的顺序会影响到更新后dist数组内的值。</p><p>举例说明，假设给定一个graph，src为A，dst点为C，且edges的顺序为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A--&gt;B</span><br><span class="line">B--&gt;C</span><br></pre></td></tr></table></figure><br>那么第一轮更新时会先更新原点A到B的距离，然后更新C的距离的时候B的距离又会影响到C的距离值。若edges的顺序倒置，那么第一轮更新就只会更新节点B的距离，不会更新C。由于更新中edges的顺序会影响到迭代次数，既然题意中要求了中转不能超过K次，那么代码中就无法对中转次数的变量进行跟踪。</p><p>事实上这个题目的标准解法居然还有一个响亮的名字，叫做Bellman Ford算法，详见<a href="https://baike.baidu.com/item/bellman-ford%E7%AE%97%E6%B3%95/1089090" target="_blank" rel="noopener">百度百科</a>或<a href="https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm" target="_blank" rel="noopener">Wikipedia</a></p><p>思路和上面的基本相同，解决方法就是为dist数组储存一个完整的备份，每次迭代后将新迭代后的值赋值给备份即可（value iteration的既视感又出来了），代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> MAXVAL = <span class="number">1e+8</span>;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// Bellman Ford</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findCheapestPrice</span><span class="params">(<span class="keyword">int</span> n, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; edges, <span class="keyword">int</span> src, <span class="keyword">int</span> dst, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dist</span><span class="params">(n, MAXVAL)</span></span>;</span><br><span class="line">        dist[src] = <span class="number">0</span>;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dist_copy</span><span class="params">(dist.begin(), dist.end())</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;=k; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; e: edges) &#123;</span><br><span class="line">                dist_copy[e[<span class="number">1</span>]] = min(dist_copy[e[<span class="number">1</span>]], dist[e[<span class="number">0</span>]] + e[<span class="number">2</span>]);</span><br><span class="line">            &#125;</span><br><span class="line">            dist.assign(dist_copy.begin(), dist_copy.end());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (dist[dst] == MAXVAL)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> dist[dst];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Coding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>背包问题汇总</title>
      <link href="/2019/01/03/knappack/"/>
      <url>/2019/01/03/knappack/</url>
      
        <content type="html"><![CDATA[<h1 id="Problem-definition"><a href="#Problem-definition" class="headerlink" title="Problem definition"></a>Problem definition</h1><a id="more"></a><h2 id="0-1无价值"><a href="#0-1无价值" class="headerlink" title="0-1无价值"></a>0-1无价值</h2><blockquote><p>Given n items with size A_i, an integer m denotes the size of a backpack. How full can you fill this backpack?</p></blockquote><p><em>注意：0-1背包问题中每件物品只能使用一次</em></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxFilled</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; sizes, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(m + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> s: sizes) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=m; i&gt;=s; i--) &#123;</span><br><span class="line">            dp[i] = max(dp[i - s] + s, dp[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[m];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0-1有价值"><a href="#0-1有价值" class="headerlink" title="0-1有价值"></a>0-1有价值</h2><blockquote><p>Given n items with size A_i and value V_i, and a backpack with size m. What’s the maximum value you can put into the backpack?</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxValue</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; sizes, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; values, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(m + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;sizes.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=m; j&gt;=sizes[i]; j--) &#123;</span><br><span class="line">            dp[j] = max(dp[j - sizes[i]] + values[i], dp[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[m];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="完全背包"><a href="#完全背包" class="headerlink" title="完全背包"></a>完全背包</h2><p><em>和0-1背包唯一的区别在于物品件数变成无限个了</em></p><p><em>代码上的唯一区别是内循环的循环顺序，0-1背包是倒序遍历，完全背包是正序遍历</em></p><blockquote><p>Given n kind of items with size A_i and value V_i (each item has an infinite number available) and a backpack with size m. What’s the maximum value you can put into the backpack?</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxValue</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; sizes, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; values, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(m + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;sizes.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=sizes[i]; j&lt;=m; j++) &#123;</span><br><span class="line">            dp[j] = max(dp[j - sizes[i]] + values[i], dp[j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[m];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="多重背包"><a href="#多重背包" class="headerlink" title="多重背包"></a>多重背包</h2><blockquote><p>Given n items with size A_i, and a backpack of size m. Each item has an <strong>finite</strong> number N_i available and a corresponding value V_i. Find the maximum value you can put into the backpack.</p></blockquote><p><em>区别在于每样物品的数量是有限的</em></p><ul><li><p><strong>方案一：</strong> 如果物品的种类比较少，可以用多维数组来直接优化，<a href="https://leetcode.com/problems/ones-and-zeroes/" target="_blank" rel="noopener">LeetCode474题Ones and Zeros</a>是一个典型案例。</p></li><li><p><strong>方案二：</strong> 另一种方案是把问题转化成有$\sum_{i}N_{i}$件单独物品的0-1背包问题，三重循环</p></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxValue</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; sizes, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; values, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(m + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;sizes.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> k=<span class="number">0</span>; k&lt;nums[i]; k++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=m; j&gt;=sizes[i]; j--) &#123;</span><br><span class="line">                dp[j] = max(dp[j - sizes[i]] + values[i], dp[j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[m];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>方案三：</strong> <ul><li>Intuition可以这样理解：如果某一类物品的个数为5，那么上面方案二就是相当于把这一类物品拆分成1+1+1+1+1=5，而方案三则相当于分解为4+1=5</li><li>稍微严谨一点，给定整数$k$与一组$\alpha_{i}$满足$k=\sum_{i}\alpha_{i}2^{i}$，对于任何整数$0\leq{n}\leq{k}$，都存在一组整数$\beta_{i}\leq{\alpha_{i}}$使得$n=\sum_{i}\beta_{i}2^{i}$</li><li>若某样物品的种类为$N_{i}$，这个定理可以使代码优化到$\sum_{i}O(\log(N_{i}))$</li></ul></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxValue</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; sizes, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; values, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(m + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;sizes.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> k=<span class="number">1</span>; k&lt;=nums[i]; k*=<span class="number">2</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=m; j&gt;=k * sizes[i]; j--) &#123;</span><br><span class="line">                dp[j] = max(dp[j - k * sizes[i]] + k * values[i], dp[j]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[m];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="0-1背包方案数量"><a href="#0-1背包方案数量" class="headerlink" title="0-1背包方案数量"></a>0-1背包方案数量</h2><blockquote><p>Given n items with size A_i, and a backpack of size m. Find the number of possible ways to fill the backpack, return -1 if the filling is impossible.</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">numApproaches</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; sizes, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(m + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> s: sizes) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=m; i&gt;=s; i--) &#123;</span><br><span class="line">            dp[i] += dp[i - s];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[m];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="完全背包方案数量"><a href="#完全背包方案数量" class="headerlink" title="完全背包方案数量"></a>完全背包方案数量</h2><blockquote><p>Given n items with size A_i, and a backpack of size m. Each item has an infinite number available. Find the number of possible ways to fill the backpack, return -1 if the filling is impossible.</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">numApproaches</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; sizes, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(m + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> s: sizes) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=s; i&lt;=m; i++) &#123;</span><br><span class="line">            dp[i] += dp[i - s];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[m];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="LeetCode问题汇总"><a href="#LeetCode问题汇总" class="headerlink" title="LeetCode问题汇总"></a>LeetCode问题汇总</h1><h2 id="494-Target-Sum"><a href="#494-Target-Sum" class="headerlink" title="494. Target Sum"></a>494. Target Sum</h2><blockquote><p>You are given a list of non-negative integers, a1, a2, …, an, and a target, S. Now you have 2 symbols + and -. For each integer, you should choose one from + and - as its new symbol.</p><p>Find out how many ways to assign symbols to make sum of integers equal to target S.</p></blockquote><p>暴力做法是$O(2^{N})$，不可取，转而研究这个题是否可以化为dp求解。算法导论里学过，如果一个问题可以用dp求解，那么它一定满足两个条件</p><ul><li>存在最优子问题 optimal subproblem</li><li>子问题之间存在重叠 overlapping between subproblem</li></ul><p>首先考虑一个trick来将这个问题化成一个类似于0-1背包的问题，设所有前面加正号的集合为$P$，负号的集合为$N$，那么有下面两个等式成立</p><script type="math/tex; mode=display">P+N=Sum \\ P-N=target</script><p>可得$P=\frac{1}{2}(sum + target)$，至此就将题目转换成了一个类0-1背包问题——在数组中找一个子集$P$，使得P的和为$\frac{1}{2}(sum + target)$。容易想到，对于数组中的某个元素<code>nums[i]</code>，若数组存在若干个解可以加和得到S，那么也一定可以加和得到<code>S-nums[i]</code>。按照这个思路，开一个二维数组<code>dp[i][j]</code>表示前i个元素有<code>dp[i][j]</code>种方案可以加和得到<code>j</code>。接下来就非常简单了，复杂度为$O(NS)$，递推公式为</p><script type="math/tex; mode=display">dp[i][j]=dp[i-1][j]+dp[i-1][j-nums[i]]</script><p>我的答案是直接照搬递推公式实现，但实际上空间复杂度还是可以继续优化的。假设数组<code>dp[i]</code>代表截至到下标i之前有多少种方案，由此只需要 $dp[i]+=dp[i-nums[j]], \forall{j\in{nums}}, \forall{i\in{\{nums[j], nums[j]+1, ..., S\}}}$ （注意内循环要反过来，和0-1背包一样）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findTargetSumWays</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> val: nums) &#123;</span><br><span class="line">            sum += val;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (sum &lt; target || ((sum + target) &amp; <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> findSubSet(nums, (sum + target) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findSubSet</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">dp</span><span class="params">(nums.size(), <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(target + <span class="number">1</span>, <span class="number">0</span>))</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[<span class="number">0</span>] &lt;= target)</span><br><span class="line">            dp[<span class="number">0</span>][nums[<span class="number">0</span>]] += <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;nums.size(); i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;=target; j++) &#123;</span><br><span class="line">                dp[i][j] = dp[i - <span class="number">1</span>][j];</span><br><span class="line">                <span class="keyword">if</span> (j - nums[i] &gt;= <span class="number">0</span>)</span><br><span class="line">                    dp[i][j] += dp[i - <span class="number">1</span>][j - nums[i]];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp.back()[target];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>答案区里更简洁的代码</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findTargetSumWays</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = accumulate(nums.begin(), nums.end(), <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> sum &lt; s || (s + sum) &amp; <span class="number">1</span> ? <span class="number">0</span> : subsetSum(nums, (s + sum) &gt;&gt; <span class="number">1</span>); </span><br><span class="line">    &#125;   </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">subsetSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> dp[s + <span class="number">1</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> n : nums)</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = s; i &gt;= n; i--)</span><br><span class="line">                dp[i] += dp[i - n];</span><br><span class="line">        <span class="keyword">return</span> dp[s];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="322-Coin-Change"><a href="#322-Coin-Change" class="headerlink" title="322. Coin Change"></a>322. Coin Change</h2><blockquote><p>You are given coins of different denominations and a total amount of money amount. Write a function to compute the fewest number of coins that you need to make up that amount. If that amount of money cannot be made up by any combination of the coins, return -1.</p></blockquote><p>比较标准的完全背包题，开一个长度为amount + 1的数组dp，其中dp[i]代表凑够金额i最少需要使用dp[i]数量的硬币，由此可得递推公式为</p><script type="math/tex; mode=display">dp[i]=\min{(dp[i], dp[i-v] + 1)} \quad{} \forall{v\in{coins}}</script><p>第一次写的时候写出的代码是三层循环，效率很低，如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MAXVAL = <span class="number">100000001</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">coinChange</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; coins, <span class="keyword">int</span> amount)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tmp;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(amount + <span class="number">1</span>, MAXVAL)</span></span>;</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=amount; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> val: coins) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> k=<span class="number">1</span>; i - k * val &gt;= <span class="number">0</span>; k++) &#123;</span><br><span class="line">                dp[i] = min(dp[i - k * val] + k, dp[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[amount] == MAXVAL? <span class="number">-1</span> : dp[amount];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>事实上最里层的这个循环是不需要的，为什么，试考虑这个问题的最优子问题性质，可以想到，若<code>dp[i - val]</code>中存储了凑够金额<code>i - val</code>的最少硬币数量，那么<code>dp[i - val]</code>对于凑够金额<code>i - val</code>这个子问题是最优的，我们就无需再去搜索比<code>i - val</code>更小的子问题空间了。</p><p>最终24ms代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">coinChange</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; coins, <span class="keyword">int</span> amount)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(amount + <span class="number">1</span>, MAXVAL)</span></span>;</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=amount; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> val: coins) &#123;</span><br><span class="line">            <span class="keyword">if</span> (val &lt;= i) &#123;</span><br><span class="line">                dp[i] = min(dp[i], dp[i - val] + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[amount] == MAXVAL? <span class="number">-1</span> : dp[amount];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="518-Coin-Change-2"><a href="#518-Coin-Change-2" class="headerlink" title="518. Coin Change 2"></a>518. Coin Change 2</h2><p><a href="#322-Coin-Change">Coin change</a>题的变形，依然是不限数目的硬币，求用现有的硬币种类凑够amount的方法有多少种</p><p>很容易想到递推式为$dp[i]=\sum_{v\in{\text{coins}}}dp[i-v]$，关键在于循环顺序，虽然这道题还是0-1背包，但如果还像之前一样循环的话就会出现一个问题：</p><p>比如说coins为[1,2]的一个集合，那么凑够amount=3的方法应该有两种，一种是1+1+1=3，另一种是会2+1=3，但是上面的循环方法会将1+2=3和2+1=3算成两种方法，导致输出不对</p><p>解决方案也很简单，把内外层循环调转位置即可</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">change</span><span class="params">(<span class="keyword">int</span> amount, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; coins)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(amount + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> val: coins) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=val; i&lt;=amount; i++) &#123;</span><br><span class="line">            dp[i] += dp[i - val];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[amount];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="416-Partition-Equal-Subset-Sum"><a href="#416-Partition-Equal-Subset-Sum" class="headerlink" title="416. Partition Equal Subset Sum"></a>416. Partition Equal Subset Sum</h2><blockquote><p>Given a non-empty array containing only positive integers, find if the array can be partitioned into two subsets such that the sum of elements in both subsets is equal.</p></blockquote><p>标准完全背包题，套模板硬做即可</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">canPartition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> n: nums)</span><br><span class="line">        sum += n;</span><br><span class="line">    <span class="keyword">if</span> (sum &amp; <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    sum /= <span class="number">2</span>;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; <span class="title">dp</span><span class="params">(sum + <span class="number">1</span>, <span class="literal">false</span>)</span></span>;</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> val: nums) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=sum; i&gt;=val; i--) &#123;</span><br><span class="line">            <span class="keyword">if</span> (dp[i - val])</span><br><span class="line">                dp[i] = <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[sum];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="474-Ones-and-Zeroes"><a href="#474-Ones-and-Zeroes" class="headerlink" title="474. Ones and Zeroes"></a>474. Ones and Zeroes</h2><blockquote><p>In the computer world, use restricted resource you have to generate maximum benefit is what we always want to pursue.</p><p>For now, suppose you are a dominator of m 0s and n 1s respectively. On the other hand, there is an array with strings consisting of only 0s and 1s.</p><p>Now your task is to find the maximum number of strings that you can form with given m 0s and n 1s. Each 0 and 1 can be used at most once.</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">pair&lt;int, int&gt; count(const string&amp; s) &#123;</span><br><span class="line">    pair&lt;int, int&gt; p(0, 0);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">char</span> c: s) &#123;</span><br><span class="line">        <span class="keyword">if</span> (c == <span class="string">'0'</span>)</span><br><span class="line">            p.first ++;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            p.second ++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findMaxForm</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; strs, <span class="keyword">int</span> m, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">dp</span><span class="params">(m + <span class="number">1</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(n + <span class="number">1</span>, <span class="number">0</span>))</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">const</span> <span class="built_in">string</span>&amp; s: strs) &#123;</span><br><span class="line">        <span class="keyword">auto</span> p = count(s);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=m; i&gt;=p.first; i--) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=n; j&gt;=p.second; j--) &#123;</span><br><span class="line">                dp[i][j] = max(dp[i][j], dp[i - p.first][j - p.second] + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[m][n];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Coding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Array类问题总结</title>
      <link href="/2019/01/03/Sum-like-Problems-Summary/"/>
      <url>/2019/01/03/Sum-like-Problems-Summary/</url>
      
        <content type="html"><![CDATA[<p><em>未完成，待继续更新。。。</em></p><h2 id="1-Two-Sum"><a href="#1-Two-Sum" class="headerlink" title="1. Two Sum"></a>1. Two Sum</h2><p>最基本的hash table题，复杂度$O(N)$<br><a id="more"></a><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">twoSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; table;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nums.size(); i++) &#123;</span><br><span class="line">            table[nums[i]] = i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nums.size(); i++) &#123;</span><br><span class="line">            <span class="keyword">auto</span> iter = table.find(target - nums[i]);</span><br><span class="line">            <span class="keyword">if</span> (iter != table.end() &amp;&amp; iter-&gt;second != i) &#123;</span><br><span class="line">                ans.push_back(i);</span><br><span class="line">                ans.push_back(iter-&gt;second);</span><br><span class="line">                <span class="keyword">return</span> ans;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h2 id="167-Two-Sum-II-Input-array-is-sorted"><a href="#167-Two-Sum-II-Input-array-is-sorted" class="headerlink" title="167. Two Sum II - Input array is sorted"></a>167. Two Sum II - Input array is sorted</h2><p>一看到sorted就想到binary search，花了很大的时间代价思考，然而这题最优解是$O(N)$的，要用binary search反而得$O(N\log{N})$，得不偿失。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">twoSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">        <span class="keyword">int</span> left = <span class="number">1</span>, right = nums.size();</span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[left - <span class="number">1</span>] + nums[right - <span class="number">1</span>] &lt; target) &#123;</span><br><span class="line">                left ++;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[left - <span class="number">1</span>] + nums[right - <span class="number">1</span>] &gt; target) &#123;</span><br><span class="line">                right --;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                ans.push_back(left);</span><br><span class="line">                ans.push_back(right);</span><br><span class="line">                <span class="keyword">return</span> ans;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>有关binary search解法，唯一需要注意的点就是在求middle的时候，以往的不严谨写法一般是<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">middle = (left + right) / <span class="number">2</span>;</span><br></pre></td></tr></table></figure><br>其中<code>left + right</code>在数组较大时容易溢出，更加严谨的写法是<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">middle = left + ((right - left) &gt;&gt; <span class="number">1</span>);</span><br></pre></td></tr></table></figure></p><h2 id="15-3Sum"><a href="#15-3Sum" class="headerlink" title="15. 3Sum"></a>15. 3Sum</h2><p>我人生中面试最丢人的时刻，就是在百度面试时被问及这道题的时候，那时我给出的解法是$O(N^{2}\log{N})$的，即先排序然后最内层循环用binary search搜。</p><p>然而这道题的解法意外的简单，假设三个数分别为a, b, c，思路就是固定a，然后用two sum的办法去找b和c，由于two sum有$O(N)$解，则此题复杂度为$O(N^{2})$</p><p>我的写法比较繁琐，耗时900+ms，说明这个写法思路虽然对了，但效率上有比较严重的问题。在这个case种，用iterator比直接用下标慢，且容易写错，在对iterator有更深入的理解之前应当避免使用这种写法。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">threeSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; ans;</span><br><span class="line">        <span class="keyword">if</span> (nums.empty())</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        <span class="keyword">int</span> anchor;</span><br><span class="line">        sort(nums.begin(), nums.end());</span><br><span class="line">        <span class="keyword">auto</span> it = nums.begin();</span><br><span class="line">        <span class="keyword">while</span> (it != nums.end()) &#123;</span><br><span class="line">            anchor = *it;</span><br><span class="line">            <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">tmp</span><span class="params">(<span class="number">3</span>, *it)</span></span>;</span><br><span class="line">            twoSum(it, nums.end(), <span class="number">0</span> - (*it), tmp, ans);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Skip duplicate elements</span></span><br><span class="line">            <span class="keyword">while</span> (it != nums.end() &amp;&amp; (*it) == anchor)</span><br><span class="line">                ++it;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">twoSum</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::iterator&amp; begin, </span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::iterator&amp; end,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> target, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; tmp,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::iterator it = begin + <span class="number">1</span>;</span><br><span class="line">        <span class="built_in">map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; table;</span><br><span class="line">        <span class="keyword">int</span> anchor;</span><br><span class="line">        <span class="keyword">while</span> (it != end) &#123;</span><br><span class="line">            table[*it] = it - begin;</span><br><span class="line">            ++it;</span><br><span class="line">        &#125;</span><br><span class="line">        it = begin + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (it != end) &#123;</span><br><span class="line">            anchor = *it;</span><br><span class="line">            <span class="keyword">auto</span> res = table.find(target - (*it));</span><br><span class="line">            <span class="keyword">if</span> (res != table.end() &amp;&amp; res-&gt;second &gt; (it - begin)) &#123;</span><br><span class="line">                tmp[<span class="number">1</span>] = res-&gt;first;</span><br><span class="line">                tmp[<span class="number">2</span>] = *it;</span><br><span class="line">                ans.push_back(tmp);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// Skip duplicate elements</span></span><br><span class="line">            <span class="keyword">while</span> (it != end &amp;&amp; (*it) == anchor)</span><br><span class="line">                ++it;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>讨论区里最高赞答案如下，效率bottleneck主要在于map的开销，由于算法一开始已经将数组排好序，存map和查找的开销是可以省掉的。具体做法为</p><ul><li>在搜2Sum的时候，用头尾两个指针<code>lo</code>和<code>hi</code></li><li>若<code>nums[lo] + nums[hi] &lt; target</code>，lo右移</li><li>若<code>nums[lo] + nums[hi] &lt; target</code>，hi左移</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; threeSum(<span class="keyword">int</span>[] num) &#123;</span><br><span class="line">    Arrays.sort(num);</span><br><span class="line">    List&lt;List&lt;Integer&gt;&gt; res = <span class="keyword">new</span> LinkedList&lt;&gt;(); </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num.length-<span class="number">2</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">0</span> || (i &gt; <span class="number">0</span> &amp;&amp; num[i] != num[i-<span class="number">1</span>])) &#123;</span><br><span class="line">            <span class="keyword">int</span> lo = i+<span class="number">1</span>, hi = num.length-<span class="number">1</span>, sum = <span class="number">0</span> - num[i];</span><br><span class="line">            <span class="keyword">while</span> (lo &lt; hi) &#123;</span><br><span class="line">                <span class="keyword">if</span> (num[lo] + num[hi] == sum) &#123;</span><br><span class="line">                    res.add(Arrays.asList(num[i], num[lo], num[hi]));</span><br><span class="line">                    <span class="comment">// Skip duplicate elements</span></span><br><span class="line">                    <span class="keyword">while</span> (lo &lt; hi &amp;&amp; num[lo] == num[lo+<span class="number">1</span>]) lo++;</span><br><span class="line">                    <span class="keyword">while</span> (lo &lt; hi &amp;&amp; num[hi] == num[hi-<span class="number">1</span>]) hi--;</span><br><span class="line">                    lo++; hi--;</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (num[lo] + num[hi] &lt; sum) &#123;</span><br><span class="line">                    lo++;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    hi--;</span><br><span class="line">                &#125;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="18-4Sum"><a href="#18-4Sum" class="headerlink" title="18. 4Sum"></a>18. 4Sum</h2><p>既然3Sum只是在2Sum的基础上简单扩展，直接将这道题当做3Sum的扩展即可，复杂度$O(N^{3})$。代码写的比3Sum稍简洁了一些，去重的关键有两点</p><ul><li>在for循环的末尾跳过之后的重复元素</li><li>一开始sort整个数组，并保证对每组答案<code>[a, b, c, d]</code>，必须满足<code>a &lt;= b &lt;= c &lt;= d</code></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 28ms, faster than 61.58% of the codes </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">fourSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; ans;</span><br><span class="line">        <span class="keyword">if</span> (nums.empty())</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        sort(nums.begin(), nums.end());</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nums.size(); i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=i+<span class="number">1</span>; j&lt;nums.size(); j++) &#123;</span><br><span class="line">                twoSum(nums, target - nums[i] - nums[j], i, j, ans);</span><br><span class="line">                <span class="keyword">while</span> (j + <span class="number">1</span> &lt; nums.size() &amp;&amp; nums[j] == nums[j + <span class="number">1</span>])</span><br><span class="line">                    j++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">while</span> (i + <span class="number">1</span> &lt; nums.size() &amp;&amp; nums[i] == nums[i + <span class="number">1</span>])</span><br><span class="line">                i++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">twoSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target, <span class="keyword">int</span> iidx, <span class="keyword">int</span> jidx, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> left = jidx + <span class="number">1</span>, right = nums.size() - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[left] + nums[right] == target) &#123;</span><br><span class="line">                <span class="keyword">int</span> tmp[<span class="number">4</span>] = &#123;nums[iidx], nums[jidx], nums[left], nums[right]&#125;;</span><br><span class="line">                ans.push_back(&#123;tmp, tmp + <span class="number">4</span>&#125;);</span><br><span class="line">                <span class="comment">// Skip duplicate elements</span></span><br><span class="line">                <span class="keyword">while</span> (nums[left] == nums[left + <span class="number">1</span>])</span><br><span class="line">                    left ++;</span><br><span class="line">                <span class="keyword">while</span> (nums[right] == nums[right - <span class="number">1</span>])</span><br><span class="line">                    right --;</span><br><span class="line">                left ++; right --;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[left] + nums[right] &lt; target) &#123;</span><br><span class="line">                left ++;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                right --;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="169-Majority-Element"><a href="#169-Majority-Element" class="headerlink" title="169. Majority Element"></a>169. Majority Element</h2><blockquote><p>Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times.</p><p>You may assume that the array is non-empty and the majority element always exist in the array.</p></blockquote><h3 id="解法一：Brute-force"><a href="#解法一：Brute-force" class="headerlink" title="解法一：Brute force"></a>解法一：Brute force</h3><p><em>复杂度$O(N)$，运行时间4576ms</em></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = nums.size();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> k: nums) &#123;</span><br><span class="line">        <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> v: nums) &#123;</span><br><span class="line">            <span class="keyword">if</span> (k == v)</span><br><span class="line">                cnt ++;</span><br><span class="line">            <span class="keyword">if</span> (cnt &gt; (n &gt;&gt; <span class="number">1</span>))</span><br><span class="line">                <span class="keyword">return</span> k;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="解法二：快排变形"><a href="#解法二：快排变形" class="headerlink" title="解法二：快排变形"></a>解法二：快排变形</h3><p>一种同样暴力的方法是排序</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = nums.size();</span><br><span class="line">    sort(nums.begin(), nums.end());</span><br><span class="line">    <span class="keyword">return</span> nums[(nums.size() &gt;&gt; <span class="number">1</span>)];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在各种排序算法中，快排的思路是先做partition得到重点$m$使得$nums[i]<nums[m],\forall{i}<m$且$nums[j]>nums[m],\forall{j}&gt;m$，可以想到由于众数的性质要求其出现次数必须大于⌊ n/2 ⌋，则每次partition的时候只需要递归较长的一条子串即可，代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        partition(nums, <span class="number">0</span>, nums.size() - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> nums[(nums.size()) &gt;&gt; <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (left &gt;= right)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">int</span> basis = nums[left], l = left, r = right;</span><br><span class="line">        <span class="keyword">while</span> (l &lt; r) &#123;</span><br><span class="line">            <span class="keyword">while</span> (r &gt; l &amp;&amp; nums[r] &gt;= basis)</span><br><span class="line">                -- r;</span><br><span class="line">            <span class="keyword">if</span> (r &lt;= l)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            nums[l] = nums[r];</span><br><span class="line">            <span class="keyword">while</span> (l &lt; r &amp;&amp; nums[l] &lt; basis)</span><br><span class="line">                ++ l;</span><br><span class="line">            <span class="keyword">if</span> (l &gt;= r)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            nums[r] = nums[l];</span><br><span class="line">        &#125;</span><br><span class="line">        nums[l] = basis;</span><br><span class="line">        <span class="keyword">if</span> (l &lt; (nums.size() &gt;&gt; <span class="number">1</span>))</span><br><span class="line">            partition(nums, l + <span class="number">1</span>, right);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (l &gt; (nums.size() &gt;&gt; <span class="number">1</span>))</span><br><span class="line">            partition(nums, left, l - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这种思路并不会降低复杂度，只是一种优化trick而已，运行时间2400ms+</p><h3 id="解法三：Randomization"><a href="#解法三：Randomization" class="headerlink" title="解法三：Randomization"></a>解法三：Randomization</h3><p>最神奇的一种解法，实现思路极其简单，运行时间20ms。几何分布，由于众数出现的次数一定大于$⌊ N/2 ⌋$，那么时间复杂度期望最坏情况下为$2N$</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = nums.size();</span><br><span class="line">    srand(<span class="keyword">unsigned</span>(time(<span class="literal">NULL</span>)));</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="keyword">int</span> idx = rand() % n;</span><br><span class="line">        <span class="keyword">int</span> candidate = nums[idx];</span><br><span class="line">        <span class="keyword">int</span> counts = <span class="number">0</span>; </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">            <span class="keyword">if</span> (nums[i] == candidate)</span><br><span class="line">                counts++; </span><br><span class="line">        <span class="keyword">if</span> (counts &gt; n / <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">return</span> candidate;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="解法四：Moore-Voting-Algorithm"><a href="#解法四：Moore-Voting-Algorithm" class="headerlink" title="解法四：Moore Voting Algorithm"></a>解法四：Moore Voting Algorithm</h3><p>第一次做基本上很难想到这种解法，20ms，这个解法让我想起最大连续子列和的题目</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">majorityElement</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> major, counts = <span class="number">0</span>, n = nums.size();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!counts) &#123;</span><br><span class="line">            major = nums[i];</span><br><span class="line">            counts = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            counts += (nums[i] == major) ? <span class="number">1</span> : <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> major;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="229-Majority-Element-II"><a href="#229-Majority-Element-II" class="headerlink" title="229. Majority Element II"></a>229. Majority Element II</h2><blockquote><p>Given an integer array of size n, find all elements that appear more than ⌊ n/3 ⌋ times.</p><p><strong>Note:</strong> The algorithm should run in linear time and in O(1) space.</p></blockquote><p>Moore Voting Algorithm变种</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">majorityElement</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">    <span class="keyword">if</span> (nums.empty())</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (nums.size() == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> &#123;nums.begin(), nums.end()&#125;;</span><br><span class="line">    <span class="keyword">int</span> maj1 = <span class="number">0</span>, maj2 = <span class="number">0</span>, cnt1 = <span class="number">0</span>, cnt2 = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> val: nums) &#123;</span><br><span class="line">        <span class="keyword">if</span> (val == maj1) &#123;</span><br><span class="line">            cnt1 ++;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (val == maj2) &#123;</span><br><span class="line">            cnt2 ++;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cnt1 == <span class="number">0</span>) &#123;</span><br><span class="line">            maj1 = val;</span><br><span class="line">            cnt1 = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cnt2 == <span class="number">0</span>) &#123;</span><br><span class="line">            maj2 = val;</span><br><span class="line">            cnt2 = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            cnt1 --;</span><br><span class="line">            cnt2 --;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cnt1 = <span class="number">0</span>; cnt2 = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> val: nums) &#123;</span><br><span class="line">        <span class="keyword">if</span> (val == maj1)</span><br><span class="line">            cnt1 ++;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (val == maj2)</span><br><span class="line">            cnt2 ++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (cnt1 &gt; (nums.size() / <span class="number">3</span>))</span><br><span class="line">        ans.push_back(maj1);</span><br><span class="line">    <span class="keyword">if</span> (cnt2 &gt; (nums.size() / <span class="number">3</span>))</span><br><span class="line">        ans.push_back(maj2);</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="112-Path-Sum"><a href="#112-Path-Sum" class="headerlink" title="112. Path Sum"></a>112. Path Sum</h2><p>二叉树，水题，只需要注意判断叶节点</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">hasPathSum</span><span class="params">(TreeNode* root, <span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">return</span> hasPathSum(root, sum, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">hasPathSum</span><span class="params">(TreeNode* root, <span class="keyword">int</span> sum, <span class="keyword">int</span> depth)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root-&gt;left == <span class="literal">NULL</span> &amp;&amp; root-&gt;right == <span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> sum == root-&gt;val;</span><br><span class="line">        <span class="keyword">if</span> (root-&gt;left != <span class="literal">NULL</span> &amp;&amp; hasPathSum(root-&gt;left, sum - root-&gt;val, depth + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">if</span> (root-&gt;right != <span class="literal">NULL</span> &amp;&amp; hasPathSum(root-&gt;right, sum - root-&gt;val, depth + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="113-Path-Sum-II"><a href="#113-Path-Sum-II" class="headerlink" title="113. Path Sum II"></a>113. Path Sum II</h2><p>虽然标了middle难度，但依然是水题，最基本的back-tracking操作</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">pathSum</span><span class="params">(TreeNode* root, <span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; ans;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ans;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tmp;</span><br><span class="line">        pathSum(root, sum, tmp, ans);</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">pathSum</span><span class="params">(TreeNode* root, <span class="keyword">int</span> sum, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; tmp, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root-&gt;left == <span class="literal">NULL</span> &amp;&amp; root-&gt;right == <span class="literal">NULL</span> &amp;&amp; sum == root-&gt;val) &#123;</span><br><span class="line">            tmp.push_back(root-&gt;val);</span><br><span class="line">            ans.push_back(tmp);</span><br><span class="line">            tmp.pop_back();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        tmp.push_back(root-&gt;val);</span><br><span class="line">        <span class="keyword">if</span> (root-&gt;left != <span class="literal">NULL</span>)</span><br><span class="line">            pathSum(root-&gt;left, sum - root-&gt;val, tmp, ans);</span><br><span class="line">        <span class="keyword">if</span> (root-&gt;right != <span class="literal">NULL</span>)</span><br><span class="line">            pathSum(root-&gt;right, sum - root-&gt;val, tmp, ans);</span><br><span class="line">        tmp.pop_back();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="494-Target-Sum"><a href="#494-Target-Sum" class="headerlink" title="494. Target Sum"></a>494. Target Sum</h2><p>题目是给一个数组，在每个数字前加正负号使得加和等于target的值，返回有多少种加和方案</p><p>暴力做法是$O(2^{N})$，不可取，转而研究这个题是否可以化为dp求解。算法导论里学过，如果一个问题可以用dp求解，那么它一定满足两个条件</p><ul><li>存在最优子问题 optimal subproblem</li><li>子问题之间存在重叠 overlapping between subproblem</li></ul><p>首先考虑一个trick来将这个问题化成一个类似于0-1背包的问题，设所有前面加正号的集合为$P$，负号的集合为$N$，那么有下面两个等式成立</p><script type="math/tex; mode=display">P+N=Sum \\ P-N=target</script><p>可得$P=\frac{1}{2}(sum + target)$，至此就将题目转换成了一个类0-1背包问题——在数组中找一个子集$P$，使得P的和为$\frac{1}{2}(sum + target)$。容易想到，对于数组中的某个元素<code>nums[i]</code>，若数组存在若干个解可以加和得到S，那么也一定可以加和得到<code>S-nums[i]</code>。按照这个思路，开一个二维数组<code>dp[i][j]</code>表示前i个元素有<code>dp[i][j]</code>种方案可以加和得到<code>j</code>。接下来就非常简单了，复杂度为$O(NS)$，递推公式为</p><script type="math/tex; mode=display">dp[i][j]=dp[i-1][j]+dp[i-1][j-nums[i]]</script><p>我的答案是直接照搬递推公式实现，但实际上空间复杂度还是可以继续优化的。假设数组<code>dp[i]</code>代表截至到下标i之前有多少种方案，由此只需要$dp[i]+=dp[i-nums[j]], \forall{j\in{nums}}, \forall{i\in{\{nums[j], nums[j]+1, ..., S\}}}$ （注意内循环要反过来，和0-1背包一样）</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findTargetSumWays</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums.empty())</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> val: nums) &#123;</span><br><span class="line">            sum += val;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (sum &lt; target || ((sum + target) &amp; <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> findSubSet(nums, (sum + target) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findSubSet</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">dp</span><span class="params">(nums.size(), <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(target + <span class="number">1</span>, <span class="number">0</span>))</span></span>;</span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[<span class="number">0</span>] &lt;= target)</span><br><span class="line">            dp[<span class="number">0</span>][nums[<span class="number">0</span>]] += <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;nums.size(); i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;=target; j++) &#123;</span><br><span class="line">                dp[i][j] = dp[i - <span class="number">1</span>][j];</span><br><span class="line">                <span class="keyword">if</span> (j - nums[i] &gt;= <span class="number">0</span>)</span><br><span class="line">                    dp[i][j] += dp[i - <span class="number">1</span>][j - nums[i]];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp.back()[target];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>答案区里更简洁的代码</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">findTargetSumWays</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = accumulate(nums.begin(), nums.end(), <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> sum &lt; s || (s + sum) &amp; <span class="number">1</span> ? <span class="number">0</span> : subsetSum(nums, (s + sum) &gt;&gt; <span class="number">1</span>); </span><br><span class="line">    &#125;   </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">subsetSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> dp[s + <span class="number">1</span>] = &#123; <span class="number">0</span> &#125;;</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> n : nums)</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = s; i &gt;= n; i--)</span><br><span class="line">                dp[i] += dp[i - n];</span><br><span class="line">        <span class="keyword">return</span> dp[s];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="653-Two-Sum-IV-Input-is-a-BST"><a href="#653-Two-Sum-IV-Input-is-a-BST" class="headerlink" title="653. Two Sum IV - Input is a BST"></a>653. Two Sum IV - Input is a BST</h2><p>Given a Binary Search Tree and a target number, return true if there exist two elements in the BST such that their sum is equal to the given target.</p><p>核心思路两层循环，外循环遍历BST，内循环BST查找，复杂度$O(N\log{N})$</p><p>虽说也是水题，问题在于外循环因为每次要给内循环传根节点指针，所以不能用递归遍历，外层循环应该写成非递归——非递归的写法经常忘记，权当复习吧</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">findTarget</span><span class="params">(TreeNode* root, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="built_in">stack</span>&lt;TreeNode*&gt; st;</span><br><span class="line">        TreeNode *p = root, *q;</span><br><span class="line">        <span class="keyword">while</span> (p != <span class="literal">NULL</span> || !st.empty()) &#123;</span><br><span class="line">            <span class="keyword">while</span> (p != <span class="literal">NULL</span>) &#123;</span><br><span class="line">                st.push(p);</span><br><span class="line">                p = p-&gt;left;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (!st.empty()) &#123;</span><br><span class="line">                p = st.top();</span><br><span class="line">                q = findNode(root, k - p-&gt;val);</span><br><span class="line">                <span class="keyword">if</span> (q != <span class="literal">NULL</span> &amp;&amp; q != p)</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">                st.pop();</span><br><span class="line">                p = p-&gt;right;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">TreeNode* <span class="title">findNode</span><span class="params">(TreeNode* root, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (root == <span class="literal">NULL</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">        <span class="keyword">if</span> (root-&gt;val == k) &#123;</span><br><span class="line">            <span class="keyword">return</span> root;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (root-&gt;val &gt; k) &#123;</span><br><span class="line">            TreeNode *p = findNode(root-&gt;left, k);</span><br><span class="line">            <span class="keyword">if</span> (p != <span class="literal">NULL</span>)</span><br><span class="line">                <span class="keyword">return</span> p;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            TreeNode *p = findNode(root-&gt;right, k);</span><br><span class="line">            <span class="keyword">if</span> (p != <span class="literal">NULL</span>)</span><br><span class="line">                <span class="keyword">return</span> p;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="39-Combination-Sum"><a href="#39-Combination-Sum" class="headerlink" title="39. Combination Sum"></a>39. Combination Sum</h2><p>题意是给一个数组，找到这个数组中所有可以加和得到target的组合，每个数字可以使用无限次</p><p>很简单很基础的back-tracking题，复杂度为指数级别，然而我做了将近一个小时，动用了IDE来调bug才做出来，当前代码功力下降可见一斑</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">combinationSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; ans;</span><br><span class="line"><span class="keyword">if</span> (nums.empty())</span><br><span class="line"><span class="keyword">return</span> ans;</span><br><span class="line">sort(nums.begin(), nums.end());</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tmp;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.size(); i++) &#123;</span><br><span class="line">dfs(nums, target, i, tmp, ans);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target, <span class="keyword">int</span> begin, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; tmp, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; ans)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (target &lt; nums[begin])</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">tmp.push_back(nums[begin]);</span><br><span class="line"><span class="keyword">if</span> (target == nums[begin]) &#123;</span><br><span class="line">ans.push_back(tmp);</span><br><span class="line">tmp.pop_back();</span><br><span class="line"><span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = begin; i&lt;nums.size(); i++) &#123;</span><br><span class="line">dfs(nums, target - nums[begin], i, tmp, ans);</span><br><span class="line">&#125;</span><br><span class="line">tmp.pop_back();</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="40-Combination-Sum-II"><a href="#40-Combination-Sum-II" class="headerlink" title="40. Combination Sum II"></a>40. Combination Sum II</h2><p>上面那道combinationSum的代码只把DFS迭代的起始位置从begin改成begin+1即可，水题，代码略</p><h2 id="377-Combination-Sum-IV"><a href="#377-Combination-Sum-IV" class="headerlink" title="377. Combination Sum IV"></a>377. Combination Sum IV</h2><p>这道题的返回值从返回所有的组合变成了返回有多少种组合方式，这显然是一道DP。开一个长度为target+1的数组dp，其中dp[i]代表有dp[i]种方式可以加和得到i，很容易看到递推公式应该是</p><script type="math/tex; mode=display">dp[i]=\sum_{j=0}^{N}\mathbb{I}(nums[j]<=i)dp[i-nums[j]]</script><p>C++测试0ms代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">combinationSum4</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (nums.empty())</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    sort(nums.begin(), nums.end());</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">dp</span><span class="params">(target + <span class="number">1</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=target; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> val: nums) &#123;</span><br><span class="line">            <span class="keyword">if</span> (val &gt; i)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                dp[i] += dp[i - val];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[target];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="769-Max-Chunks-To-Make-Sorted"><a href="#769-Max-Chunks-To-Make-Sorted" class="headerlink" title="769. Max Chunks To Make Sorted"></a>769. Max Chunks To Make Sorted</h2><blockquote><p>Given an array arr that is a permutation of [0, 1, …, arr.length - 1], we split the array into some number of “chunks” (partitions), and individually sort each chunk.  After concatenating them, the result equals the sorted array.</p><p>What is the most number of chunks we could have made?</p></blockquote><p>最优解法时间复杂度$O(N)$，空间$O(1)$</p><ul><li>要找到数组中所有的下标i，使得对于所有$j<i,k>i$，都满足$arr[j]<arr[i]$且$arr[k]>arr[i]$——满足这样条件下标个数即为最终所求的目标</li><li>如何充分利用题意中数组arr是从0到N的permutation这一性质？可以想到，若下标i满足$i\geq{arr[j]},\forall{j}=0,1,…,i-1$，则i之前的数字一定可以单独分作一段</li><li>具体做法就是遍历数组中的每个数字，keep track of the maximum number。由于数组是从0到N-1的permutation，如果到下标i时最大值等于i，则说明之前的所有数字</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxChunksToSorted</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; arr)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> cnt = <span class="number">0</span>, mval = <span class="number">0x80000000</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;arr.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr[i] &gt; mval)</span><br><span class="line">            mval = arr[i];</span><br><span class="line">        <span class="keyword">if</span> (i == mval)</span><br><span class="line">            cnt ++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> cnt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="768-Max-Chunks-To-Make-Sorted-II"><a href="#768-Max-Chunks-To-Make-Sorted-II" class="headerlink" title="768. Max Chunks To Make Sorted II"></a>768. Max Chunks To Make Sorted II</h2><blockquote><p>This question is the same as <a href="#769-Max-Chunks-To-Make-Sorted">“Max Chunks to Make Sorted”</a> except the integers of the given array are not necessarily distinct, the input array could be up to length 2000, and the elements could be up to 10**8.</p></blockquote><p>看到这个答案无话可说，实实在在的智商差距</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxChunksToSorted</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; arr)</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">arr_cp</span><span class="params">(arr.begin(), arr.end())</span></span>;</span><br><span class="line">    sort(arr_cp.begin(), arr_cp.end());</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> ans = <span class="number">0</span>, sum1 = <span class="number">0</span>, sum2 = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;arr.size(); i++) &#123;</span><br><span class="line">        sum1 += arr[i];</span><br><span class="line">        sum2 += arr_cp[i];</span><br><span class="line">        <span class="keyword">if</span> (sum1 == sum2)</span><br><span class="line">            ans ++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Coding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>凌乱的工作计划</title>
      <link href="/2019/01/01/ImportantReferences/"/>
      <url>/2019/01/01/ImportantReferences/</url>
      
        <content type="html"><![CDATA[<h2 id="On-Exploration-of-Research-Intern"><a href="#On-Exploration-of-Research-Intern" class="headerlink" title="On Exploration of Research Intern"></a>On Exploration of Research Intern</h2><a id="more"></a><ul><li><a href="https://www.msra.cn/zh-cn/jobs" target="_blank" rel="noopener">MSRA: https://www.msra.cn/zh-cn/jobs</a></li><li><a href="https://campus.alibaba.com/talentPlanDetail.htm?spm=a1z3e1.11770841.0.0&amp;id=82" target="_blank" rel="noopener">Alibaba research intern: https://campus.alibaba.com/talentPlanDetail.htm</a></li><li><a href="https://join.qq.com/index.php" target="_blank" rel="noopener">腾讯实习生校园招聘</a></li><li><a href="https://job.bytedance.com/intern" target="_blank" rel="noopener">字节跳动: https://job.bytedance.com/intern</a></li></ul><h2 id="状态汇总"><a href="#状态汇总" class="headerlink" title="状态汇总"></a>状态汇总</h2><p><em>Note: 以下网页需要处于登录状态才可以正常查看</em></p><ul><li><a href="https://campus.alibaba.com/myJobApply.htm?spm=a1z3e1.11796652.0.0.7b5860d3YIJHPe" target="_blank" rel="noopener">Alibaba人才计划research intern</a></li><li><a href="https://join.qq.com/center.php" target="_blank" rel="noopener">腾讯深圳TEG</a></li><li><a href="https://job.bytedance.com/user/profile/" target="_blank" rel="noopener">ByteDance AML Machine Learning Platform Intern</a></li></ul><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><p><em>Note: first things first!</em></p><ul><li>Devided-and-Conquer</li><li>Re-implement GAIL-related works<ul><li>AIRL，只是把D网络的结构改成了reward-shaping的形式，但有证明</li><li>2017, Li <em>et al</em>, info-GAIL，解决multiple expert demonstration问题</li><li>ICLR-2019, Directed-info GAIL，由于涉及到一些option framework的内容，formulation还蛮复杂的</li></ul></li><li>Distributed DDPG的复现真是个悲伤的故事，即使只是在Pendulum-v0这样简单的任务上想复现出DDPG的效果也相当不容易（弃坑</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Personal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论面试官虐我的一百种方式</title>
      <link href="/2018/12/30/interview-summary/"/>
      <url>/2018/12/30/interview-summary/</url>
      
        <content type="html"><![CDATA[<h2 id="ByteDance抖音火山组"><a href="#ByteDance抖音火山组" class="headerlink" title="ByteDance抖音火山组"></a>ByteDance抖音火山组</h2><a id="more"></a><h3 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h3><ul><li>自我介绍</li><li>大致地聊一下简历，总结自己的研究方向，简述自己的文章工作，简述项目与之前的实习经历</li><li>Adam优化的原理与理解：momentum与RMSProp的结合，一阶矩滑动平均稳定迭代方向，二阶矩作为cache控制迭代步长，在带有噪声的loss surface上效果较好</li><li>手撕代码第一题：开根号，二分查找与牛顿法两种解法，注意迭代停止条件的设置</li><li>介绍Attention mechanism：soft attention、hard attention，self-attention，其中soft attention问题可拓展至seq2seq和LSTM等，self-attention拓展至CNN-based attention及其应用</li><li>介绍自己研究方向的几个主流方法及其优缺点</li></ul><h3 id="二面"><a href="#二面" class="headerlink" title="二面"></a>二面</h3><ul><li>手撕代码第二题：Resovior sampling</li><li>手撕代码第三题：二维数组找最长递增路径，DFS题</li><li>深度学习框架如TensorFlow如何实现并行：两个维度，GPU并行计算与计算图级别的并行，前者问题可以延伸至CUDA编程中的一些基础知识，后者可以延伸至PS架构</li><li>解释BP实现过程</li><li>问简历细节，dCRF和LevelSet原理与实现</li><li>重复问了Attention mechanism</li></ul><h3 id="三面"><a href="#三面" class="headerlink" title="三面"></a>三面</h3><ul><li>手撕代码第四题：finding the median in two sorted array，至少得写出log(M+N)级别的解法</li><li>不看简历，叙述以前的项目和比赛，如果方向不是强相关的话会直接打断</li><li>Random forest和GBDT，节点如何分裂，用什么metric来选特征，要求写出公式；进一步问题还可以延伸至feature importance怎么算，xgboost原理与实现等</li><li>介绍Attention mechanism <del>(我简历上都没写为啥这么喜欢问attention啊摔)</del></li><li>推荐搜索相关：优化搜索引擎时，有哪些指标可以作为评判用户满意程度与优化用户体验的依据</li></ul><h3 id="AML机器学习平台组"><a href="#AML机器学习平台组" class="headerlink" title="AML机器学习平台组"></a>AML机器学习平台组</h3><p><em>由于求职意向不符总共没聊几句，之后官网状态更新，头条以后再也不见</em></p><div>    <img src="http://img.itlun.cn/uploads/allimg/160416/1-160416140051.jpg" width="200px"></div><ul><li>详细问了之前实习中的内容，如何旁路log，如何提高性能</li><li>C++多态底层实现</li><li>一个线段随机分成三份，求三条子线段正好可以构成一个三角形的概率</li><li>旋转数组求最小值下标</li></ul><h2 id="Baidu-Feed流架构组"><a href="#Baidu-Feed流架构组" class="headerlink" title="Baidu Feed流架构组"></a>Baidu Feed流架构组</h2><h3 id="一面-1"><a href="#一面-1" class="headerlink" title="一面"></a>一面</h3><ul><li>自我介绍</li><li>重载与虚函数的底层实现机制</li><li>内存有哪几种，堆空间与栈空间的区别</li><li>手撕代码第一题：ThreeSum，要求bug free</li><li>哈希表如何解决冲突，拉链式与线性查找，两种方案的优缺点，缺点如何解决</li></ul><h3 id="二面-1"><a href="#二面-1" class="headerlink" title="二面"></a>二面</h3><ul><li>聊简历，聊论文的工作，为什么可以取得比较好的效果</li><li>有哪些可能会涉及到内存泄漏的场景，如何避免内存泄漏</li><li>手撕代码第二题：<code>char* strdump(const char* s)</code>，复制字符串，要求bug free</li><li>智能指针的原理与实现，开放式问题：工程中是否应该用智能指针</li><li>操作系统相关：C++11对多线程的支持，口头叙述生产者消费者读写buffer过程，虚拟内存与物理内存，Linux的top指令</li><li>简单问了下new operator原理，boost中内存池的原理</li><li>手写一个含有动态内存分配成员的类的各种函数签名，考点在于RAII、拷贝构造、右值拷贝与operator=</li><li>网络相关：TCP三次握手过程，浏览器浏览网页过程中涉及到的网络协议，select、poll和epoll</li></ul><h2 id="蚂蚁金服"><a href="#蚂蚁金服" class="headerlink" title="蚂蚁金服"></a>蚂蚁金服</h2><h3 id="一面-2"><a href="#一面-2" class="headerlink" title="一面"></a>一面</h3><ul><li>介绍论文的工作</li><li>linear regression，推进式地问了四个问题<ul><li>$Ax=b$ 欠定方程组求解</li><li>若$A\in{R^{m\times{n}}}$，求复杂度</li><li>如何降低求逆运算的复杂度 =&gt; 引出SVD或QR分解</li><li>在此问题中SVD为什么能够降低求逆复杂度 =&gt; 考察SVD细节</li></ul></li><li>算法也是渐进式地问了几个问题<ul><li>一个数组$[1, 2, 3, …, N]$随机打乱并从中去掉一个元素，如何找到这个元素，时间$O(N)$，空间$O(1)$</li><li>如果从中去掉两个元素如何求解，复杂度同上 =&gt; 分治</li><li>用数学方法求解出缺少的两个元素的公式</li></ul></li></ul><h3 id="二面-2"><a href="#二面-2" class="headerlink" title="二面"></a>二面</h3><ul><li>介绍论文的工作，会问到每种方法的复杂度和缺点，改进方法，在其他领域是否可以有应用</li><li>详细地问项目细节</li><li>RF、GBDT的区别，bagging和boosting的区别，xgboost如何并行，和RF比哪个快</li><li>有没有用过分布式框架 =&gt; gRPC，brpc =&gt; 开始问之前的实习，RPC如何工作，brpc有什么特性</li><li>乱序数组，找中位数 =&gt; Partition variant</li></ul><h3 id="三面-1"><a href="#三面-1" class="headerlink" title="三面"></a>三面</h3><ul><li>介绍论文，你的论文对于工业界来讲可以在哪些方面得到应用</li><li>xgboost和random forest的优缺点</li><li>如何处理离散特征（类别型特征）</li><li>特征工程中，若数据本身的维度为$N$，那么对特征两两进行交叉组合可以得到$N^{2}级别的特征量，如何快速筛选组合特征</li><li>如果进蚂蚁金服的话，你希望从事哪方面的工作？</li></ul><h3 id="四面"><a href="#四面" class="headerlink" title="四面"></a>四面</h3><p><em>和之前的面试相比，很多问题都感觉让人摸不着头脑，不知道面试官到底想问什么，感觉自己在答一些非技术问题的时候也踩了一些雷，不知道是交叉面还是HR面还是老板面</em></p><ul><li>自我介绍，讲论文，常规操作</li><li>介绍自己的三个优点与三个缺点 =&gt; <del>喵喵喵</del> =&gt; 面试官：看来以前没有自己想过这个问题哈</li><li>你希望自己从事偏科研还是偏工程的工作，是否有读博打算</li><li>你是如何学习机器学习的，介绍几个机器学习深度学习领域的代表人物</li><li>你的模型处于低偏差高方差的状态，应该采用什么<strong>算法</strong>来解决（不懂这个问题的点在哪里，我思考的逻辑如下<ul><li>决策树类的模型，post-pruning一类或许可以勉强称之为“算法”</li><li>神经网络这类capacity极大的模型，adversarial training应该也算是算法</li><li>其他诸如dropout、数据增强、加regularizer等等，都不能算得上是“算法”，且不指定“模型”是什么的前提下，完全没有办法回答用什么<strong>算法</strong>解决过拟合</li></ul></li><li>介绍什么是过拟合，如何解决</li><li>8个球长得一模一样，其中一个比其他球更重，给一个天平，最少多少次可以找到那个最重的球（每次问智商题都会暴露出智商的不足，正确答案是两次</li><li>一个搜索引擎，新算法上线做AB test，新算法线下测试的结果比旧算法好，但由于工程错误实际表现不如旧的算法，你认为可能是由于什么原因，如何理解这一类问题（<del>全然意味わからない</del></li></ul><h3 id="HR面"><a href="#HR面" class="headerlink" title="HR面"></a>HR面</h3><p><em>全程聊人生，面试时间在20min以内</em></p><ul><li>讲论文是怎么做的</li><li>你认为自己在之前的人生中做的最牛逼的一件事</li><li>你在之前的公司实习期间学到了些什么</li><li>你同时还投了哪些公司，你之所以投这些公司，是看中这些公司的哪些闪光点</li><li>你对你未来想要工作的团队的期望是什么，你希望自己可以通过实习达到什么样的目标</li></ul><h2 id="Tencent-TEG"><a href="#Tencent-TEG" class="headerlink" title="Tencent TEG"></a>Tencent TEG</h2><h3 id="一面-3"><a href="#一面-3" class="headerlink" title="一面"></a>一面</h3><ul><li>介绍简历上的工作，详细讲了一作的论文，非一作的直接跳过</li><li>介绍简历上的项目，说到CRF as RNN模型的时候，面试官问你们的训练数据量远少于语义分割，具体是如何训练以及对抗过拟合的，是否有用到pretrained model<ul><li>大部分做主流CV任务的网络参数量都太大了，不适合直接迁移</li><li>将VGG换成了UNet =&gt; 追问，为什么换</li><li>常规的数据增强 =&gt; 有哪几种</li><li>Adversarial training =&gt; 追问了具体实现方法 =&gt; 参考了<a href="https://arxiv.org/abs/1710.10571" target="_blank" rel="noopener">ICLR 2018 Sinha <em>et al</em></a>的工作</li></ul></li><li>针对王者荣耀任务设计强化学习算法框架（复盘时想起腾讯AILab在ICML2018上的论文上有一张王者荣耀比赛的图片，大概面试官是想听我说MCTS，可惜当时没有想到这一层，捂脸<ul><li>问面试官同时控制五个英雄会不会涉及到multi-agent的问题 =&gt; 先简化问题，认为只有一个英雄也可以</li><li>Hirachical RL，比如分开对线期和gank期子任务等等</li><li>大型MDP，baseline模型可以用DDPG或者distributed PPO</li><li>大量的人类操作数据，可以做imitation learning =&gt; 追问了具体该怎么做<ul><li>说到Inverse RL，讲了下DAgger</li><li>介绍了GAIL以及AIRL的一系列工作</li></ul></li></ul></li><li>平时玩游戏吗 =&gt; 玩 =&gt; 面试官说那么我们部门做的东西你应该蛮感兴趣的，你可以搜搜看KPL比赛的AI队伍就是我们做的</li></ul><h3 id="二面-3"><a href="#二面-3" class="headerlink" title="二面"></a>二面</h3><ul><li>怼项目，面试结束之后复盘发现，其实面试官问的有关项目的一连串问题，是希望我可以按照一篇学术论文的思路，将整个项目的历程组织起来的：<ul><li><strong>Background &amp; Motivation:</strong> 项目中用到的方法既然是参考别人的paper做的，那么你如何理解这次项目与别人paper中遇到的问题的差别？</li><li><strong>Related works:</strong> 传统上其他类似的视觉任务遇到这种问题是怎么做的 =&gt; 介绍了RNN as CRF，PSPNet，DeepLab</li><li><strong>Our approach:</strong> 既然你的问题与语义分割不同，为了解决task-specific problems你们做了哪些改进 =&gt; 为什么这样设计网络结构 =&gt; dense CRF具体怎么实现的</li><li><strong>Conclusion &amp; Future works:</strong> 做完这个项目你觉得里面还有什么问题是待解决或未解决的</li></ul></li><li>怼论文，之前有没有人做过类似的工作，与自己的方法比较，还有一些论文的琐碎细节问题</li><li>详细介绍深度学习网络结构的发展历程，从AlexNet到VGG再到ResNet再到DenseNet =&gt; 延伸问题：为什么DenseNet效果可以比ResNet更好</li><li>介绍深度学习优化方法的研究脉络与发展历程，从SGD到Momentum再到Adagrad和RMSProp，最后详细讲Adam</li><li>详细叙述SVM，如何解决线性不可分问题</li><li>两个数组取交集，讲算法，推复杂度（<del>貌似推错了</del></li><li>详细问了有关RL的一系列问题，分value-based方法和policy-based方法<ul><li>policy-based方法和value-based主要的区别在于哪里 =&gt; 从Bellman equation开始各种胡扯</li><li>value-based方法学习的目标是什么</li><li>讲policy-based方法的研究脉络与发展历程 =&gt; 从policy gradient theorem讲到REINFORCE，从DDPG讲到A2C，从TRPO讲到PPO<ul><li>追问一：value function在TRPO中的作用是什么</li><li>追问二：带value function的模型在优化时如何迭代</li><li>追问三：value function的loss可不可以和policy的loss放到同一个框架下</li><li>追问四：介绍PPO的两种objective function</li></ul></li></ul></li><li>聊人生，为什么做RL =&gt; 觉得有趣</li><li>重复问了一面的问题，平时玩游戏吗 =&gt; 玩 =&gt; 玩什么游戏 =&gt; Dota2（<del>被拉黑</del></li></ul><h3 id="HR面-1"><a href="#HR面-1" class="headerlink" title="HR面"></a>HR面</h3><ul><li>问我的具体研究方向 =&gt; RL强相关方向</li><li>由于我学校在上海，问我去深圳工作有没有问题</li><li>之前对腾讯有什么了解 =&gt; 看过AILab发的顶会文章，仰望大佬</li><li>有没有亲戚朋友之类的在深圳 =&gt; 没有 =&gt; 将来考虑留深圳吗 =&gt; 考虑</li><li>什么时候可以开始上班</li></ul><p><em>然后HR就直接口头通知说offer会在月底发到我手上，结束</em></p><h2 id="网易游戏"><a href="#网易游戏" class="headerlink" title="网易游戏"></a>网易游戏</h2><h3 id="一面-4"><a href="#一面-4" class="headerlink" title="一面"></a>一面</h3><p>电话面，聊的问题主要是实习期的工作以及RL饰演的一些细节，感觉面试官非常的nice，这场面是还是非常有收获的，这里省略所有实习相关的细节问题</p><ul><li>增加训练机器数量是否可以提高模型能力与泛化性能？ =&gt; positive，但通过提升训练量达到的上限有限 =&gt; 那么你认为这个上限落到数学层面，是什么问题导致的？</li><li>RL方法，从算法理论和解决的问题两个层面分类</li><li>DDPG中policy输出一般用tanh，但tanh并不能代表action空间真正的形状，这时应该怎么解决？</li><li>MCTS如何决定节点选择？ =&gt; UCB，score公式具体形式忘记了 (凉凉 X 1</li><li>TRPO的trust region是谁的trust region？为什么要在这个trust region内做优化？具体优化是如何做的？PPO和TRPO区别是什么？优化是如何做的 =&gt; clipped objective具体形式忘了 (凉凉 X 2</li><li>Prioritized Q-learning，sum tree具体是如何做的？为什么这样做？ =&gt; 忘记了 (凉凉 X 3</li><li>一个地方重男轻女，每家每户都秉承着只要生不出男孩就一直生直到生出男孩来为止的原则，那么长期下去这个地方的男女比例是多少？</li></ul><h3 id="二面-leader面"><a href="#二面-leader面" class="headerlink" title="二面+leader面"></a>二面+leader面</h3><p>杭州滨江现场面试，中规中矩偏聊天，问实习较多，依旧省略所有实习相关问题</p><ul><li>强化学习基础，介绍MDP和Bellman equation</li><li>手推Q和V之间的公式，相比一面更基础一些=，不过这还是第一次遇到需要手写RL公式的<ul><li>$V^{\pi}(s_{t})=\int_{\mathcal{A}}\pi(a_{t}|s_{t})Q(s_{t},a_{t})da_{t}$</li><li>$Q^{\pi}(s_{t},a_{t})=\mathbb{E}_{\pi}[R_{t}+\gamma V(s_{t+1})]$</li></ul></li><li>VAE和普通autoencoder有什么区别？VAE如何解决隐层无法求导的问题？</li><li>on-policy和off-policy的区别？TRPO和PPO是off-policy还是on-policy？<ul><li>这个问题的名堂在于，一般off-policy是指与环境直接交互的policy与正在优化的policy不同，on-policy则反之，从这个意义上讲TRPO和PPO是off-policy</li><li>问题在于TRPO和PPO的off-policy只能算是伪off-policy，因为TRPO的基础理论推导中要求每一步迭代满足 $D_{KL}(\pi_{old}||\pi)\leq\delta$</li><li>追问了与一面相同的问题：为什么要满足这个？不满足会怎样？TRPO和PPO都是用什么办法让模型满足这个条件的？</li><li>追问：既然TRPO和PPO只能算伪off-policy，列举一些你认为的真正意义上的off-policy算法</li></ul></li><li>一道DP题，m*n的int型矩阵，求最长递增路径，核心在于对于已经遍历过的点，其最优路径一定是最优的，所以要存下来每个点的最长递增路径长度，搜索过程中如果遇到已经遍历过的点直接返回</li><li>leader提出来的点，不算在提问之内，思考 imitation learning 与 reinforcement learning 结合的工作？如何结合？</li></ul><p>也有一些老生常谈的问题，比如</p><ul><li>分类问题中如何解决样本不均衡</li><li>介绍bagging和boosting的区别与联系</li><li>介绍牛顿法，优缺点</li><li>你给自己的定义是研究型还是应用型？</li></ul><h2 id="美团"><a href="#美团" class="headerlink" title="美团"></a>美团</h2><p>美团考了非常多的深度学习NLP题，印象比较深的一个问题是：对于一个维度为 [batch_size, sequence_length, dim] 的输入tensor，CNN中的卷积、LSTM的循环结构、以及基于CNN的self-attention，哪个计算速度最快？</p><p>编程题：有若干个桶，每个桶给定其尺寸大小 <code>b[i]</code> 和桶里已装的货物数量 <code>a[i]</code> ，每将一个桶中的一个货物挪到另一个桶中需要花掉1单位时间，要求在使用最少的桶的前提下，选择若干个桶，使得将其他桶中的货物装进这几个桶中花费的时间最少</p><h2 id="其他算法相关"><a href="#其他算法相关" class="headerlink" title="其他算法相关"></a>其他算法相关</h2><ul><li>一个长度为$N$的数组，$N$为偶数，将其平均分成两部分使得两部分和的乘积最大，背包题</li><li>给$2k+1$个硬币，问扔完之后正面比反面多的概率是多少<ul><li>$\frac{1}{2^{2k+1}}\sum_{i=k+1}^{2k+1}C_{2k+1}^{i}$</li><li>有公式$\sum_{i=0}^{N}C_{N}^{i}=2^{N}$，$C_{N}^{0}+C_{N}^{2}+…+C_{N}^{N}=2^{N-1}$，由于$C_{N}^{i}=C_{N}^{N-i}$，上式化简得答案1/2</li></ul></li><li>给定二维平面上点集$\{p_{1},…,p_{n} | p_{i}=(x_{i},y_{i})\}$，定义$p_{i}$为最大当且仅当对于所有的$p_{j},\ j\neq{i}$，都满足条件$x_{i}&gt;x_{j}\ \text{or}\ y_{i}&gt;y_{j}$，求最大点点集，复杂度$O(N\log(N))$</li></ul><h2 id="其他机器学习相关"><a href="#其他机器学习相关" class="headerlink" title="其他机器学习相关"></a>其他机器学习相关</h2><ul><li>HMM与CRF</li><li>Overfitting解决，除了常规的加正则，增加训练数据，数据增强，减少模型参数以外，还可以加adversarial training，把模型看作是PGM加hidden variable</li><li>推荐系统题：给十亿个用户特征向量，对于每个用户找到与其最相似的十个用户</li><li>xgboost相比GBDT和RF的优势</li><li>模型度量：AUC、ROC，recall</li><li>batch norm的原理与理解</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Personal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presentation slides download</title>
      <link href="/2018/11/12/slides/"/>
      <url>/2018/11/12/slides/</url>
      
        <content type="html"><![CDATA[<p>This page is for the AAAI 2019 paper <a href="https://arxiv.org/abs/1810.03806" target="_blank" rel="noopener">“adversarial attack and detection under the Fisher information metric”</a>.<br>The paper has been designated for oral representation on the conference. For the following period of time, as the first author of the paper, I will be preparing for the conference.<br><a id="more"></a></p><ul><li><a href="./slides.pdf">The full version of the slides (contains some detailed description about the motivation and formulation)</a></li><li><a href="./slides_formal.pdf">The slides used in AAAI 2019 12 minutes presentation</a></li><li><a href="./poster.pdf">The poster used in AAAI 2019</a></li></ul><p>The main purpose of this slides is to serve as the complementary material for the paper. As required by the AAAI paper formatting guideline, the paper is not allowed to exceed 8 pages long with the last page only containing references. Additional fee (hundreds of dollars) would be required for more pages in the publication. I have to remove some technical details to satisfy the requirement. However, I have to acknowledge that some of the details are essential for further understanding. As a matter of fact, some questions mentioned in the slide are raised by the reviewers, and have been thoroughly discussed during the reviewing, which really helps a lot to make the paper stronger.</p><p>Hopefully the slide can address most concerns of readers.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Public </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AAAI备忘</title>
      <link href="/2018/11/11/AAAI%E5%A4%87%E5%BF%98/"/>
      <url>/2018/11/11/AAAI%E5%A4%87%E5%BF%98/</url>
      
        <content type="html"><![CDATA[<h2 id="1-camera-ready相关"><a href="#1-camera-ready相关" class="headerlink" title="1. camera ready相关"></a>1. camera ready相关</h2><a id="more"></a><ul><li><b style="color:red;">重要！！！</b> Final AAAI archive due no later than Feb 21st, 2019 at 5:00 PM (PST)</li><li>Final AAAI archive submission url will be available via E-Mail contact soon</li><li>Arxiv传一个camera ready版本上去</li></ul><h2 id="2-签证相关"><a href="#2-签证相关" class="headerlink" title="2. 签证相关"></a>2. 签证相关</h2><ul><li><a href="http://www.ustraveldocs.com/cn_zh/cn-niv-ds160complete.asp" target="_blank" rel="noopener">DS160页面链接</a></li><li><a href="https://cgifederal.secure.force.com/?language=Chinese%20%28Simplified%29&amp;country=China" target="_blank" rel="noopener">签证申请链接</a></li><li><a href="https://ceac.state.gov/CEACStatTracker/Status.aspx" target="_blank" rel="noopener">查看签证状态</a></li></ul><h2 id="3-Oral-presentation相关"><a href="#3-Oral-presentation相关" class="headerlink" title="3. Oral presentation相关"></a>3. Oral presentation相关</h2><ul><li>演讲当天至少提前到会场10分钟，向session chair介绍自己</li><li>会场只有LCD投影仪，屏幕和麦克风</li><li>演讲时长15分钟，最后三分钟为Q&amp;A，slides中应包含summary，若最后时间不够可以选择性地跳过</li><li><a href="https://docs.google.com/spreadsheets/d/1mth3ZvbIn5jqvyWV2qsz1QoiR3Hase0i5GTyCO2m_PM/edit#gid=0" target="_blank" rel="noopener">AAAI-2019 Session Chair Form</a></li><li><a href="./Schedule.pdf">AAAI-2019会议日程</a></li></ul><h2 id="4-会议日程相关"><a href="#4-会议日程相关" class="headerlink" title="4. 会议日程相关"></a>4. 会议日程相关</h2><ul><li>Jan 27th, 2010. 6:30 PM: Student reception，自费，注册时未勾选</li><li>Jan 28th, 2019. Evening: Openning reception，费用包括在注册费中</li><li><strong>Jan 31th, 2019. 12:30-15:00: AAAI-19 job fair</strong>，可以去看看，记着提前打印好简历若干</li><li>Tutorial环节应该是可以免费入场的，提前交的10-20美刀可以保证入场肯定有座位</li><li>Jan 27-29, 2019. 7:30AM-5:00PM: 现场注册以及发放胸牌</li></ul><h2 id="5-费用记录"><a href="#5-费用记录" class="headerlink" title="5. 费用记录"></a>5. 费用记录</h2><div class="table-container"><table><thead><tr><th>时间</th><th style="text-align:center">内容</th><th style="text-align:center">金额</th><th style="text-align:center">金主</th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td>Nov 18th, 2018</td><td style="text-align:center">注册费</td><td style="text-align:center">524$</td><td style="text-align:center">小雪</td><td style="text-align:center"></td></tr><tr><td>Nov 26th, 2018</td><td style="text-align:center">签证费</td><td style="text-align:center">1120</td><td style="text-align:center">Me</td><td style="text-align:center"></td></tr><tr><td>Dec 6th, 2018</td><td style="text-align:center">资助部分机票钱</td><td style="text-align:center">5000</td><td style="text-align:center">Shen</td><td style="text-align:center"></td></tr><tr><td>Dec 14th, 2018</td><td style="text-align:center">机票：夏威夷→上海</td><td style="text-align:center">4859</td><td style="text-align:center">Me</td><td style="text-align:center"></td></tr><tr><td>Dec 23th, 2018</td><td style="text-align:center">机票：上海→夏威夷</td><td style="text-align:center">5016</td><td style="text-align:center">雪</td><td style="text-align:center"></td></tr><tr><td>Dec 23th, 2018</td><td style="text-align:center">出国救命钱</td><td style="text-align:center">10000</td><td style="text-align:center">父上</td><td style="text-align:center"></td></tr><tr><td>Jan 7th, 2019</td><td style="text-align:center">passport邮费</td><td style="text-align:center">24</td><td style="text-align:center">雪</td><td style="text-align:center">已还清</td></tr><tr><td>Jan 22th, 2019</td><td style="text-align:center">打印海报</td><td style="text-align:center">60</td><td style="text-align:center">Me</td><td style="text-align:center">没有发票</td></tr><tr><td>Jan 23th, 2019</td><td style="text-align:center">海报筒</td><td style="text-align:center">24</td><td style="text-align:center">Me</td><td style="text-align:center">没有发票</td></tr></tbody></table></div><h2 id="6-采购记录"><a href="#6-采购记录" class="headerlink" title="6. 采购记录"></a>6. 采购记录</h2><ul><li>王文：Dior一盒，一盒中有001和004两种色号，参考图片</li><li>母上：Estee Lauder (不知道怎么念)</li></ul><h2 id="7-吃喝玩乐"><a href="#7-吃喝玩乐" class="headerlink" title="7. 吃喝玩乐"></a>7. 吃喝玩乐</h2><p>仅限Oahu island和Maui island</p><div class="table-container"><table><thead><tr><th>地点</th><th style="text-align:center">地理信息</th><th style="text-align:center">费用信息</th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td>Diamond Head</td><td style="text-align:center">5.1 km from Hilton</td><td style="text-align:center"></td><td style="text-align:center">钻石头火山</td></tr><tr><td>Hanauma Bay</td><td style="text-align:center">The east end of the island</td><td style="text-align:center"></td><td style="text-align:center">恐龙湾</td></tr><tr><td>Lanai Lookout</td><td style="text-align:center">Besides Hanauma Bay</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td>Pearl Harbor</td><td style="text-align:center">17 km from Hilton</td><td style="text-align:center"></td></tr><tr><td>Kualoa Ranch</td><td style="text-align:center">Northwest in the island</td><td style="text-align:center"></td><td style="text-align:center">古兰尼牧场，保护区，电影取景地，公园</td></tr><tr><td>Road to Hana</td><td style="text-align:center">Maui岛自驾路线</td><td style="text-align:center"></td><td style="text-align:center">需要预留至少4h开完全程</td></tr><tr><td>Punaluu County Beach Park</td><td style="text-align:center">10.7 km from Kualoa Ranch</td><td style="text-align:center"></td></tr><tr><td>Haleakala National Park</td><td style="text-align:center">Maui岛中央</td><td style="text-align:center">15$ per car</td><td style="text-align:center">非自驾不可达</td></tr><tr><td>Lahaina</td><td style="text-align:center">The west end of Maui island</td><td style="text-align:center"></td><td style="text-align:center">拉海纳捕鲸镇</td></tr></tbody></table></div><h2 id="8-Oral-Reports-worth-Noting"><a href="#8-Oral-Reports-worth-Noting" class="headerlink" title="8. Oral Reports worth Noting"></a>8. Oral Reports worth Noting</h2><div class="table-container"><table><thead><tr><th>ID</th><th style="text-align:center">Time</th><th style="text-align:center">Title</th><th style="text-align:center">Session</th><th style="text-align:center">Location</th><th style="text-align:center">Author</th></tr></thead><tbody><tr><td>602</td><td style="text-align:center">10:25-11:25</td><td style="text-align:center">Distributionally Adversarial Attack</td><td style="text-align:center">Game Theory and Economic Paradigms 1</td><td style="text-align:center">Kahili Suite</td><td style="text-align:center">Tianhang Zheng</td></tr><tr><td>5666</td><td style="text-align:center">11:30-12:30</td><td style="text-align:center">Multi-Task Deep Reinforcement Learning with PopArt</td><td style="text-align:center">Transfer / Adaptation / Multi-task Learning 1</td><td style="text-align:center">Coral Ballroom 3-5</td><td style="text-align:center">Matteo Hessel</td></tr><tr><td>2360</td><td style="text-align:center">11:30-12:30</td><td style="text-align:center">AutoZOOM: Autoencoder Zeroth Order Optimization Method for Attacking Black-Box Neural Networks</td><td style="text-align:center">AI for Social Impact 2</td><td style="text-align:center">Coral 1</td><td style="text-align:center">Chun-Chen Tu</td></tr><tr><td>6751</td><td style="text-align:center">11:30-12:30</td><td style="text-align:center">Adversarial Actor-Critic Method for Task and Motion Planning Problems Using Planning Experience</td><td style="text-align:center">Cognitive Systems</td><td style="text-align:center">Lehua</td><td style="text-align:center">Beomjoon Kim</td></tr><tr><td>6269</td><td style="text-align:center">2:00-3:30</td><td style="text-align:center">Guided Dropout</td><td style="text-align:center">Deep Learning 1</td><td style="text-align:center">Coral Ballroom 3-5</td><td style="text-align:center">Rohit Keshari</td></tr><tr><td>4306</td><td style="text-align:center">2:00-3:30</td><td style="text-align:center">Graph Convolutional Networks Meet Markov Random Fields: Semi-Supervised Community Detection in Attribute Networks</td><td style="text-align:center">AI and Web 3</td><td style="text-align:center">South Pacific 1</td><td style="text-align:center">Di Jin</td></tr><tr><td>3231</td><td style="text-align:center">2:00-3:30</td><td style="text-align:center">Dialogue Generation: From Imitation Learning to Inverse Reinforcement Learning</td><td style="text-align:center">Coral Ballroom 3-5</td><td style="text-align:center">South Pacific 2</td><td style="text-align:center">Ziming Li</td></tr><tr><td>1336</td><td style="text-align:center">2:00-3:30</td><td style="text-align:center">Switch Active Deep Dyna-Q: Efficient Adaptive Planning for Task Completion Dialogue Policy Learning</td><td style="text-align:center">Coral Ballroom 3-5</td><td style="text-align:center">South Pacific 2</td><td style="text-align:center">Yuexin Wu</td></tr><tr><td>-</td><td style="text-align:center">Jan 30th, 2019, 8:45-9:45</td><td style="text-align:center">Adversarial Machine Learning</td><td style="text-align:center">Invited Talk</td><td style="text-align:center">Coral Ballroom</td><td style="text-align:center">Ian Goodfellow</td></tr><tr><td>-</td><td style="text-align:center">11:30-12:30</td><td style="text-align:center"><strong>ALL ORALS SHOULD BE NOTED</strong></td><td style="text-align:center">Adversarial Learning</td><td style="text-align:center">Coral Ballroom 3-5</td><td style="text-align:center">-</td></tr><tr><td>-</td><td style="text-align:center">3:35-4:35</td><td style="text-align:center"><strong>ALL ORALS SHOULD BE NOTED</strong></td><td style="text-align:center">Reinforcement Learnint 1</td><td style="text-align:center">Coral Ballroom 3-5</td><td style="text-align:center">-</td></tr><tr><td>5388</td><td style="text-align:center">2:00-3:30</td><td style="text-align:center">Temporal Planning with Temporal Metric Trajectory Constraints</td><td style="text-align:center">Planning, Routing, and Scheduling 3</td><td style="text-align:center">Lehua</td><td style="text-align:center">Andrea Micheli</td></tr><tr><td>1468</td><td style="text-align:center">3:35-4:45</td><td style="text-align:center">The Goldilocks Zone: Towards Better Understanding of Neural Network Loss Landscapes</td><td style="text-align:center">Deep Learning 3</td><td style="text-align:center">Coral 1</td><td style="text-align:center">Stanislav Fort</td></tr><tr><td>-</td><td style="text-align:center">Jan 31th, 2019, 5:15-6:15</td><td style="text-align:center">GDPR, Data Shortage and AI</td><td style="text-align:center">Invited Talk</td><td style="text-align:center">Coral Ballroom</td><td style="text-align:center">Yang Qiang</td></tr><tr><td>1405</td><td style="text-align:center">10:25-11:25</td><td style="text-align:center">Rethinking the Discount Factor in Reinforcement Learning: A Decision Theoretic Approach</td><td style="text-align:center">Reasoning under Uncertainty 1</td><td style="text-align:center">Sea Pearl Suite</td><td style="text-align:center">Silviu Pitis</td></tr><tr><td>5968</td><td style="text-align:center">11:30-12:30</td><td style="text-align:center">Bayesian Graph Convolutional Neural Networks for SemiSupervised Classification</td><td style="text-align:center">Bayesian Learning and Probabilistic Graphical Model 2</td><td style="text-align:center">Kahili Suite</td><td style="text-align:center">Mark Coates</td></tr><tr><td>1734</td><td style="text-align:center">11:30-12:30</td><td style="text-align:center">Verifying Robustness of Gradient Boosting Models</td><td style="text-align:center">Human-AI Collaboration 1</td><td style="text-align:center">Lehua</td><td style="text-align:center">Itai Segall</td></tr><tr><td>-</td><td style="text-align:center">2:00-3:30</td><td style="text-align:center"><strong>ALL SHOULD BE NOTED</strong></td><td style="text-align:center">Multiagent Systems 1</td><td style="text-align:center">Sea Pearl Suite</td><td style="text-align:center">-</td></tr><tr><td>-</td><td style="text-align:center">Feb 1st, 2019, 10:00-11:00</td><td style="text-align:center"><strong>ALL SHOULD BE NOTED</strong></td><td style="text-align:center">Reinforcement Learning 2</td><td style="text-align:center">Nautilus Suite</td><td style="text-align:center">-</td></tr><tr><td>-</td><td style="text-align:center">Feb 1st, 2019, 11:15-12:30</td><td style="text-align:center"><strong>ALL SHOULD BE NOTED</strong></td><td style="text-align:center">Reinforcement Learning 3</td><td style="text-align:center">Nautilus Suite</td><td style="text-align:center">-</td></tr></tbody></table></div>]]></content>
      
      
      
        <tags>
            
            <tag> Personal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction</title>
      <link href="/2018/01/20/Introduction/"/>
      <url>/2018/01/20/Introduction/</url>
      
        <content type="html"><![CDATA[<h2 id="Comming-up-soon…"><a href="#Comming-up-soon…" class="headerlink" title="Comming up soon…"></a>Comming up soon…</h2>]]></content>
      
      
      
        <tags>
            
            <tag> Public </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo使用记录</title>
      <link href="/2018/01/11/hexo%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/"/>
      <url>/2018/01/11/hexo%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h2 id="1-安装nodejs"><a href="#1-安装nodejs" class="headerlink" title="1. 安装nodejs"></a>1. 安装nodejs</h2><p><a href="https://nodejs.org/en/" target="_blank" rel="noopener">nodejs官网windows版本下载链接</a>，一路傻瓜式安装，记得把环境变量加上</p><h2 id="2-安装hexo"><a href="#2-安装hexo" class="headerlink" title="2. 安装hexo"></a>2. 安装hexo</h2><a id="more"></a><ul><li>先创建一个文件夹（用来存放所有blog），然后cd到该文件夹下</li><li>安装hexo: <del><code>npm i -g hexo</code></del></li></ul><p><strong>Updated in Jan 19th, 2019:</strong><br> 旧的hexo安装方法不再支持，如果使用旧指令的话会报错显示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ npm i -g hexo</span><br><span class="line">npm WARN deprecated titlecase@1.1.2: no longer maintained</span><br><span class="line"><span class="comment"># -------------------------</span></span><br><span class="line"><span class="comment"># Updated in Jan 19th, 2019</span></span><br><span class="line"><span class="comment"># New command to install hexo</span></span><br><span class="line">$ npm install hexo-cli -g</span><br></pre></td></tr></table></figure><ul><li>初始化hexo: <code>hexo init</code></li></ul><h2 id="3-创建github-io"><a href="#3-创建github-io" class="headerlink" title="3. 创建github.io"></a>3. 创建github.io</h2><ul><li>在自己的github账号下创建一个repo，名字叫<code>yourname.github.io</code>，其中yourname=github用户名</li><li><p>如果没有配置过这个github账号的话，要先在本地配置github账号信息</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">"yourname"</span></span><br><span class="line">git config --global user.email <span class="string">"your_email@youremail.com"</span></span><br><span class="line">ssh-keygen -t rsa -C “your_email@youremail.com”</span><br></pre></td></tr></table></figure><p>  然后会要求输入用户名密码，成功的话会在~/下生成.ssh文件夹，cd进去，<code>cat id_rsa.pub</code>，复制里面的key</p><p>  打开<a href="https://github.com/settings/keys" target="_blank" rel="noopener">github账号account settings下的ssh keys</a>，点new SSH key，title随便填，Key把刚才的内容黏贴进去</p></li></ul><h2 id="4-修改-config-yml配置"><a href="#4-修改-config-yml配置" class="headerlink" title="4. 修改_config.yml配置"></a>4. 修改_config.yml配置</h2><h3 id="1-通过hexo-deploy部署"><a href="#1-通过hexo-deploy部署" class="headerlink" title="1. 通过hexo deploy部署"></a>1. 通过hexo deploy部署</h3><p>用编辑器打开blog项目，修改_config.yml文件的配置 <span style="color:red;"><b> (注意冒号之后都是有一个半角空格!) </b></span>，做好这一步之后就可以直接通过<code>hexo d</code>免密码部署到自己的github主页了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">    type: git</span><br><span class="line">    repo: https:&#x2F;&#x2F;github.com&#x2F;YourgithubName&#x2F;YourgithubName.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure><h3 id="2-post加密"><a href="#2-post加密" class="headerlink" title="2. post加密"></a>2. post加密</h3><ul><li>运行 <code>npm install hexo-encrypt --save</code></li><li>在_config.yml中添加变量  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">encrypt: </span><br><span class="line">    password: UserDefinedPassWord</span><br></pre></td></tr></table></figure></li><li>通过 <code>hexo new post &quot;post_name&quot;</code> 生成的markdown文件最前面都会有变量栏，形如：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: xxx</span><br><span class="line">date: yy-mm-dd hour:min:sec</span><br><span class="line">tags: xxx</span><br><span class="line">---</span><br></pre></td></tr></table></figure>  在需要加密的markdown文件最前面加 <code>encrypt: true</code> 即可</li></ul><h3 id="3-修改hexo主题"><a href="#3-修改hexo主题" class="headerlink" title="3. 修改hexo主题"></a>3. 修改hexo主题</h3><p>网上讲解这个的博客很多，比如 <a href="https://www.jianshu.com/p/33bc0a0a6e90" target="_blank" rel="noopener">https://www.jianshu.com/p/33bc0a0a6e90</a></p><h3 id="4-TEX语法支持"><a href="#4-TEX语法支持" class="headerlink" title="4. TEX语法支持"></a>4. TEX语法支持</h3><p>作者在配置TEX语法支持功能的时候参考了若干篇博客，链接如下</p><ul><li><a href="https://www.jianshu.com/p/68e6f82d88b7" target="_blank" rel="noopener">https://www.jianshu.com/p/68e6f82d88b7</a> </li><li><a href="https://blog.csdn.net/crazy_scott/article/details/79293576" target="_blank" rel="noopener">https://blog.csdn.net/crazy_scott/article/details/79293576</a></li><li><a href="https://blog.csdn.net/u014792304/article/details/78687859" target="_blank" rel="noopener">https://blog.csdn.net/u014792304/article/details/78687859</a></li></ul><p>Trouble shooting:</p><ul><li>不能用默认的landscape主题，landscape对mathjax的支持很差</li><li>使用第三方主题时，注意第三方主题中的<code>_config.yml</code>文件中<code>mathjax</code>选项的配置<ul><li>mathjax: true # 在第三方theme的_config.yml中配置</li><li>mathjax: enable # 在根目录下的_config.yml中配置</li><li>per_page: false # false的时候需要在markdown中手动添加mathjax: true来启动mathjax</li></ul></li></ul><p>问题测试样例（如果看到这个公式显示异常的话就需要调整.yml文件）:</p><script type="math/tex; mode=display">\eta_*^\lambda=\sum_{i=0}^{T}D_{t_{e}^{s}}tk</script><h3 id="5-添加pdf文件"><a href="#5-添加pdf文件" class="headerlink" title="5. 添加pdf文件"></a>5. 添加pdf文件</h3><ul><li>在博客bash中执行  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-pdf</span><br></pre></td></tr></table></figure></li><li>在根目录下的_config.yml文件中设置<code>post_asset_folder</code>为<code>true</code></li><li>在markdown中需要用到pdf文件的地方加上<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% pdf http://some_url.com/some_pdf.pdf %&#125;</span><br></pre></td></tr></table></figure></li><li>在blog的source/_post路径下新建一个文件夹，<span style="color:red"><b>名字与引用pdf的markdown文件名相同</b></span>，放入要上传的pdf文件</li><li><code>hexo deploy</code>的时候会自动上传文件夹</li></ul><h2 id="6-修改NexT主题配置"><a href="#6-修改NexT主题配置" class="headerlink" title="6. 修改NexT主题配置"></a>6. 修改NexT主题配置</h2><ul><li>调整默认字体、字体大小与背景颜色等设定，定位到文件 <code>./newblog/themes/next/source/css/_variables/base.styl</code></li><li>修改代码块主题，找到<code>./themes/next/_config.yml</code>中的Code Highlight Theme，有下面四种可选项，个人经斟酌选择用night bright，并将selection调整为orange<ul><li>normal</li><li>night</li><li>night eighties</li><li>night blue</li><li>night bright</li></ul></li></ul><h2 id="7-让博客被Baidu-or-Google-收录"><a href="#7-让博客被Baidu-or-Google-收录" class="headerlink" title="7. 让博客被Baidu or Google 收录"></a>7. 让博客被Baidu or Google 收录</h2><h2 id="7-Hello-world-HEXO"><a href="#7-Hello-world-HEXO" class="headerlink" title="7. Hello world HEXO"></a>7. Hello world HEXO</h2><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h3 id="1-Create-a-new-post"><a href="#1-Create-a-new-post" class="headerlink" title="1. Create a new post"></a>1. Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="2-Run-server"><a href="#2-Run-server" class="headerlink" title="2. Run server"></a>2. Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="3-Generate-static-files"><a href="#3-Generate-static-files" class="headerlink" title="3. Generate static files"></a>3. Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="4-Deploy-to-remote-sites"><a href="#4-Deploy-to-remote-sites" class="headerlink" title="4. Deploy to remote sites"></a>4. Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Manual </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>ICLR-2020 OpenReview汇总</title>
      <link href="/conferences-collection/iclr-2020-openreview.html"/>
      <url>/conferences-collection/iclr-2020-openreview.html</url>
      
        <content type="html"><![CDATA[<p><strong>Updated in Nov 7th, 2019:</strong></p><a id="more"></a><p>审稿结果已出，今年评分结果有点出乎意料，之前看过的不少觉得还不错的文章都拿了低分，反而有一些觉得比较普通的文章拿了高分。。。</p><p>有github大佬已经做好了打分表，见 <a href="https://github.com/shaohua0116/ICLR2020-OpenReviewData#Data" target="_blank" rel="noopener">ICLR2020-OpenReviewData#Data</a>，以下主要扫一遍这个表中的高分paper</p><h1 id="High-rating-interesting-papers"><a href="#High-rating-interesting-papers" class="headerlink" title="High rating interesting papers"></a>High rating interesting papers</h1><h2 id="SQIL-Imitation-Learning-via-Reinforcement-Learning-with-Sparse-Rewards"><a href="#SQIL-Imitation-Learning-via-Reinforcement-Learning-with-Sparse-Rewards" class="headerlink" title="SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards"></a><a href="https://openreview.net/forum?id=S1xKd24twB" target="_blank" rel="noopener">SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards</a></h2><p>得分668</p><blockquote><p>This paper proposes an imitation learning approach via reinforcement learning. The imitation learning problem is transformed into an RL problem with a reward of +1 for matching an expert’s action in a state and a reward of 0 for failing to do so. This encourages the agent to return to “known” states from out-of-distribution states and alleviates the problem of compounding errors</p></blockquote><p>这篇本章提出的方法在代码实现层面非常简单，这里我们考虑用actor-critic的off-policy结构实现，假设算法一开始有一个expert model，每次agent与环境交互时，若agent的动作与expert的动作选择相同，则为这条样本赋+1的reward，反之则给0的reward</p><p><img src="./iclr-2020-openreview/sqil_algo.png" width="99%"></p><p>作者称他们的这套方法可以与任意off-policy RL算法结合，实质上相当于behavior cloning的正则化版本</p><p>文章中作者采用的基础off-policy RL算法是SAC，在SAC的背景下我们可以这样解读作者提出的方法：对于demonstration中出现过的state，鼓励模型以更高的概率访问这些state；而对于demonstration中未出现过的state，则给予模型negative feedback让模型以更低的概率访问该state，并最大化模型在这些state上的entropy</p><h2 id="Measuring-the-Reliability-of-Reinforcement-Learning-Algorithms"><a href="#Measuring-the-Reliability-of-Reinforcement-Learning-Algorithms" class="headerlink" title="Measuring the Reliability of Reinforcement Learning Algorithms"></a><a href="https://openreview.net/forum?id=SJlpYJBKvH" target="_blank" rel="noopener">Measuring the Reliability of Reinforcement Learning Algorithms</a></h2><p>提出了一个新的evaluation metric来衡量RL算法的reliability</p><h2 id="Observational-Overfitting-in-Reinforcement-Learning"><a href="#Observational-Overfitting-in-Reinforcement-Learning" class="headerlink" title="Observational Overfitting in Reinforcement Learning"></a><a href="https://openreview.net/forum?id=HJli2hNKDH" target="_blank" rel="noopener">Observational Overfitting in Reinforcement Learning</a></h2><blockquote><p>A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP).</p></blockquote><p>与我毕业论文正在做的工作强相关，作者声称状态转移动态对于policy的泛化能力有non-trivial的影响，基于深度学习的RL模型很容易将state中的某些特征与reward错误地联系起来，产生题目中的ovservational overfitting效应；本文主要是发现了这种现象并通过实验说明为什么这种现象是non-trivial的，并没有提出新的解决方案，但实验部分比较充分，值得学习借鉴</p><h2 id="Is-a-Good-Representation-Sufficient-for-Sample-Efficient-Reinforcement-Learning"><a href="#Is-a-Good-Representation-Sufficient-for-Sample-Efficient-Reinforcement-Learning" class="headerlink" title="Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?"></a><a href="https://openreview.net/forum?id=r1genAVKPB" target="_blank" rel="noopener">Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?</a></h2><blockquote><p>The authors challenge the idea that good representation in RL lead are sufficient for learning good policies with an interesting negative result — they show that there exist MDPs which require an exponential number of samples to learn a near-optimal policy even if a good-but-not-perfect representation is given to the agent for both value-based and policy-based learning.</p></blockquote><p>一篇理论RL的文章，提出的观点很有趣，作者证明存在一个exponential lower bound for policy-based and value-based RL algorithms with function approximation</p><h2 id="Robust-Reinforcement-Learning-for-Continuous-Control-with-Model-Misspecification"><a href="#Robust-Reinforcement-Learning-for-Continuous-Control-with-Model-Misspecification" class="headerlink" title="Robust Reinforcement Learning for Continuous Control with Model Misspecification"></a><a href="https://openreview.net/forum?id=HJgC60EtwB" target="_blank" rel="noopener">Robust Reinforcement Learning for Continuous Control with Model Misspecification</a></h2><p>这篇文章已经写过，出自deepmind之手，个人认为这篇文章处理环境动态的distributional shift的方式略显不妥，虽然前面作者有证明这种robust operator是一个contraction，但实验是直接用hand-selected hyperparameter来让模型在不同环境中训练的，几个reviewer也有提出类似的问题：</p><blockquote><p>If there are, e.g., 20 parameters, and one picks 3 values for each, there are 3^20 variations of the environment. This exponential growth seems problematic, which is also mentioned by R3.</p></blockquote><p>值得一提的是reviewer 3给出了一份长长的reference list，大部分研究都与解决RL robustness to action stochasticity与robustness to dynamics相关，值得学习，这里复制过来方便后面参考</p><blockquote><p>Robustness to Dynamics:</p><ul><li>Bart van den Broek, Wim Wiegerinck, and Hilbert J. Kappen.  Risk sensitive path integral control. In UAI, 2010.</li><li>Arnab Nilim and Laurent El Ghaoui.   Robust control of markov decision processes with uncertain transition matrices.Operations Research, 53(5):780–798, 2005</li><li>Wolfram Wiesemann, Daniel Kuhn, and Berc  Rustem.  Robust markov decision processes. Mathematics of Operations Research, 38(1):153–183, 2013.</li><li>Lars Peter Hansen and Thomas J Sargent.Robustness. Princeton university press, 2008</li><li>Yun Shen, Michael J Tobia, Tobias Sommer, and Klaus Obermayer. Risk-sensitive reinforcement learning. Neural computation, 26(7):1298–1328, 2014.</li><li>Yinlam  Chow,  Aviv  Tamar,  Shie  Mannor,  and  Marco  Pavone.   Risk-sensitive  and  robust decision-making: a cvar optimization approach.   In Advances in Neural Information Pro-cessing Systems, pages 1522–1530, 2015.</li></ul><p>Robustness to Action Stochasticity:</p><ul><li>Roy Fox, Ari Pakman, and Naftali Tishby.  G-learning: Taming the noise in reinforcement learning via soft updates.arXiv preprint arXiv:1512.08562, 2015.</li><li>Jonathan Rubin, Ohad Shamir, and Naftali Tishby.  Trading value and information in mdps.In Decision Making with Imperfect Decision Makers, pages 57–74. Springer, 2012.</li><li>Daniel A Braun, Pedro A Ortega, Evangelos Theodorou, and Stefan Schaal.  Path integral control  and bounded  rationality.   In Adaptive  Dynamic  Programming  And  ReinforcementLearning (ADPRL), 2011 IEEE Symposium on, pages 202–209. IEEE, 2011.</li></ul><p>Combination of both:</p><ul><li>Grau-Moya, Jordi, et al. “Planning with information-processing constraints and model uncertainty in Markov decision processes.” Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2016.</li></ul></blockquote><h2 id="Improving-Generalization-in-Meta-Reinforcement-Learning-using-Neural-Objectives"><a href="#Improving-Generalization-in-Meta-Reinforcement-Learning-using-Neural-Objectives" class="headerlink" title="Improving Generalization in Meta Reinforcement Learning using Neural Objectives"></a><a href="https://openreview.net/forum?id=S1evHerYPr" target="_blank" rel="noopener">Improving Generalization in Meta Reinforcement Learning using Neural Objectives</a></h2><p>我之前很早就有考虑过meta-learning对于学习具有泛化能力的RL模型应该是一个有效的方法，但由于meta learning本身可以说是纯粹的实验科学，缺乏理论支撑，要想说明这个思路的非平凡性也并不容易</p><blockquote><p>This paper proposes a meta-RL algorithm that learns an objective function whose gradients can be used to efficiently train a learner on entirely new tasks from those seen during meta-training.</p></blockquote><hr><h1 id="A-RL-part"><a href="#A-RL-part" class="headerlink" title="A. RL part"></a>A. RL part</h1><h2 id="Implementation-Matters-in-Deep-RL-A-Case-Study-on-PPO-and-TRPO"><a href="#Implementation-Matters-in-Deep-RL-A-Case-Study-on-PPO-and-TRPO" class="headerlink" title="Implementation Matters in Deep RL: A Case Study on PPO and TRPO"></a><a href="https://openreview.net/forum?id=r1etN1rtPB" target="_blank" rel="noopener">Implementation Matters in Deep RL: A Case Study on PPO and TRPO</a></h2><blockquote><p>We investigate the consequences of <strong>“code-level optimizations” in TRPO and PPO</strong>: algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO’s gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function.</p></blockquote><p><strong>Updated after the reviewing:</strong> 审稿结果已出，结果啪啪打脸，所有的审稿人都认为这篇文章会对RL community的reproductivity有很大好处</p><p>这次的结果让我想起有位知乎老哥写了一篇文章<a href="https://zhuanlan.zhihu.com/p/78652777" target="_blank" rel="noopener">吐槽OpenAi Gym MuJoCo</a>，其中提到Humanoid的alive bonus被设置到了极其不科学的5，去掉alive bonus的话PPO也能得到8000分以上的高分。2019年以来，很多迹象表明虽然RL社区一片热闹非凡的景象，但相比于2017年来说model-free RL并没有非常本质性的突破，之后的一些算法可能只是overfitting到了benchmark的一些特殊的结构特性上，并没有能够真正带来更低的sample complexity或更好的generalization abiity</p><p>我还记得我作为RL初心者入行的时候学习到multi-modal policy是当前RL的一个重大难题，但对比SAC与SQL的实验结果也不难发现一个令人丧气的事实：理论上严谨的SVGD并没有带来更好的实验结果，反而是Gaussian policy训练加上deterministic evaluation的SAC已经足以在Humanoid上达到6000分以上的表现。这样的事实可能对于RL还有点新鲜，但在CV或NLP中早就被广泛承认：炼丹再次战胜了严谨的分析与推导</p><p>目前的RL还远不够成熟，MuJoCo作为主流benchmark的时代应当逐渐走远，未来的RL社区需要更硬核的benchmark，需要更科学的evaluation metric以及面对随机性与distributional shift更加robust的算法</p><hr><p>abstract中非常直白地发牢骚，作者指出PPO相比TRPO的performance gain主要来源于论文里的一些细节，而不是PPO文章中claim的那些改进，如果将PPO实现中的一些代码细节同等地放在TRPO中，那么TRPO可以达到至少是与PPO同等的水平，甚至多数情况下还会更高</p><p>我觉得这个结果完全不值得惊讶，当前的RL社区Berkeley+OpenAI几乎独占了半壁江山，以每年二三十篇的速度产出顶会论文，每篇文章都会claim说自己在benchmark上刷到了sota；然而事实是很多算法细节与代码实现都受到了不同层面的质疑</p><h2 id="A-Generalized-Training-Approach-for-Multiagent-Learning"><a href="#A-Generalized-Training-Approach-for-Multiagent-Learning" class="headerlink" title="A Generalized Training Approach for Multiagent Learning"></a><a href="https://openreview.net/forum?id=Bkl5kxrKDr" target="_blank" rel="noopener">A Generalized Training Approach for Multiagent Learning</a></h2><blockquote><p>This paper investigates a population-based training regime based on game-theoretic principles called Policy-Spaced Response Oracles (PSRO).<br>Here, we extend the theoretical underpinnings of PSRO by considering an alternative solution concept, α-Rank, which is unique (thus faces no equilibrium selection issues, unlike Nash) and tractable to compute in general-sum, many-player settings.</p><p>We also carry out an initial empirical validation in <strong>MuJoCo soccer</strong>, illustrating the feasibility of the proposed approach in another complex domain.</p></blockquote><p>最后的实验是MuJoCo Soccer，应该可以排除这篇文章是DeepMind的工作的可能性。之前DeepMind已经有工作指出PSRO+population-based training可以得到two-player zero-sum game的Nash equilibrium，本文研究的问题是环境中存在超过两个agent时（general-sum game）如何解Nash equilibrium，作者提出的方法在general-sum games问题上有收敛性保障</p><h2 id="Robust-Reinforcement-Learning-via-Adversarial-Training-with-Langevin-Dynamics"><a href="#Robust-Reinforcement-Learning-via-Adversarial-Training-with-Langevin-Dynamics" class="headerlink" title="Robust Reinforcement Learning via Adversarial Training with Langevin Dynamics"></a><a href="https://openreview.net/forum?id=BJl7mxBYvB" target="_blank" rel="noopener">Robust Reinforcement Learning via Adversarial Training with Langevin Dynamics</a></h2><blockquote><p>We re-think the Two-Player Reinforcement Learning (RL) as an instance of a distribution sampling problem in infinite dimensions. Using the powerful Stochastic Gradient Langevin Dynamics, we propose a new two-player RL algorithm, which is a sampling variant of the two-player policy gradient method. Our new algorithm consistently outperforms existing baselines, <strong>in terms of generalization across differing training and testing conditions</strong>, on several MuJoCo environments.</p></blockquote><p>看标题就被吸引了，感觉是非常有趣的一篇文章，作者声称自己的方法在generalization问题上outperform已有的baseline，最后实验效果做出来也很不错，可以关注一波后续reviewer的评价</p><ul><li>问题一：如何度量现有模型的robustness呢？文中的方法是在MuJoCo的实验中调整参数 <code>relative mass</code></li><li>问题二：Langevin dynamic是什么？作者idea的核心是Langevin dynamic，这是非凸优化领域一种性质比较好的解minimax问题的算法，本质是梯度加高斯噪声，有收敛保障</li><li>问题三：baseline是什么？作者在实验中对比的baseline是不加噪声的普通adversarial training with RMSProp optimizer</li><li>问题四：adversarial training内层的maximization目标是什么？这篇文章中的设定是action space同时受到两个player的影响，假设两个agent分别为 $\mu_{\theta}$ 和 $v_{w}$ ，那么最后实际与环境交互的action为 $a_{t}=\delta\mu_{\theta}(s_{t})+(1-\delta)v_{w}(s_{t})$ ,文章中用的基础算法是DDPG，梯度可以直接通过critic传递到 $\mu_{\theta}$ 和 $v_{w}$ 的参数中</li></ul><h2 id="Rularization-Matters-in-Policy-Optimization"><a href="#Rularization-Matters-in-Policy-Optimization" class="headerlink" title="Rularization Matters in Policy Optimization"></a><a href="https://openreview.net/forum?id=B1lqDertwr" target="_blank" rel="noopener">Rularization Matters in Policy Optimization</a></h2><p>这是一篇benchmark性质的文章，全文基本只有empirical evaluation，但实验非常solid，研究的问题是传统机器学习与深度学习中的正则化方法对于强化学习算法的影响</p><p>对比supervised learning (SL)与reinforcement learning (RL)，我们可以看到很多传统的正则化方式，包括$\ell_{2}$、dropout，weight clipping等方法，在SL中都有非常广泛的应用，如最早版本的ResNet和RCNN都是默认加$\ell_{2}$正则的，transformer中加了dropout正则，而之后的BERT模型中dropout换成了$\ell_{2}$，应当说，目前的绝大多数sota方法中都或多或少存在这些传统正则化的影子</p><p>相比之下，RL算法中很少可以看到这一类的正则化，取而代之最常用的是entropy正则，在policy-based方法中，为了避免模型过早过拟合到某种suboptimal的策略上，研究者们常常采用entropy正则来避免模型执行某一个action的概率过高或过低；PPO方法中的entropy正则体现在损失函数中直接explicitly出现了policy的entropy项，而SQL与SAC方法中entropy则直接成为了模型优化的最终目标一部分</p><p>为什么RL领域中很少用到SL里的正则化方法呢？作者认为其中一个可能的原因在于目前的RL方法训练环境与测试环境完全相同，因而RL的模型不需要像SL的模型一样具备在不同的测试环境中的泛化能力。本文的研究中，作者发现传统的正则化方法往往可以对复杂任务的performance有很大提升，通过比较传统的$\ell_{2}$正则与entropy正则，作者发现$\ell_{2}$正则往往可以的得到更好的<strong>泛化性能</strong>，且对训练的超参更加不敏感</p><p>这里总结记录一些文章中的empirical conclusion</p><ul><li>Only regularizing the policy network is typically enough, regularizing $V^{\pi}$ or $Q^{\pi}$ does not help on the performance</li><li>$\ell_{2}$ regularization outperforms entropy regularization in many cases</li><li>$\ell_{1}$ and weight clipping can sometimes boost performance</li><li>Dropout and batch norm tend to bring improvements on only on off-policy SAC</li></ul><h1 id="B-Adversarial-learning-part"><a href="#B-Adversarial-learning-part" class="headerlink" title="B. Adversarial learning part"></a>B. Adversarial learning part</h1><h2 id="Global-Adversarial-Robustness-Guarantees-for-Neural-Networks"><a href="#Global-Adversarial-Robustness-Guarantees-for-Neural-Networks" class="headerlink" title="Global Adversarial Robustness Guarantees for Neural Networks"></a><a href="https://openreview.net/forum?id=BJgyn1BFwS" target="_blank" rel="noopener">Global Adversarial Robustness Guarantees for Neural Networks</a></h2><blockquote><p>Given a neural network f we investigate the global adversarial robustness properties of f, showing how these can be computed up to any a priori specified statistical error. </p><p>We empirically observe that robustness and accuracy tend to be negatively correlated for networks trained via stochastic gradient descent and with iterative pruning techniques, while a positive trend is observed between them in Bayesian settings.</p></blockquote><h2 id="Invariance-vs-Robustness-of-Neural-Networks"><a href="#Invariance-vs-Robustness-of-Neural-Networks" class="headerlink" title="Invariance vs Robustness of Neural Networks"></a><a href="https://openreview.net/forum?id=HJxp9kBFDS" target="_blank" rel="noopener">Invariance vs Robustness of Neural Networks</a></h2><blockquote><p>Previous work has studied generalization to natural geometric transformations (e.g., rotations) as invariance, and generalization to adversarial perturbations as robustness.<br>In this paper, <strong>we examine the interplay between invariance and robustness</strong>. We empirically study the following two cases:(a) change in adversarial robustness as we improve only the invariance using equivariant models and training augmentation, (b) change in invariance as we improve only the adversarial robustness using adversarial training. We observe that the rotation invariance of equivariant models (StdCNNs and GCNNs) improves by training augmentation with progressively larger rotations but while doing so, their adversarial robustness does not improve, or worse.</p></blockquote><p>简单来说，这篇文章用前人提出的equivariant model来测试对抗样本性质的旋转变换等操作，发现虽然模型的invariance随训练而提升，但模型对人为优化出的旋转变换对抗样本的鲁棒性反而下降了</p><h2 id="Fast-is-better-than-free-Revisiting-adversarial-training"><a href="#Fast-is-better-than-free-Revisiting-adversarial-training" class="headerlink" title="Fast is better than free: Revisiting adversarial training"></a><a href="https://openreview.net/forum?id=BJx040EFvH" target="_blank" rel="noopener">Fast is better than free: Revisiting adversarial training</a></h2><blockquote><p>FGSM-based adversarial training, with randomization, works just as well as PGD-based adversarial training: we can use this to train a robust classifier in 6 minutes on CIFAR10, and 12 hours on ImageNet, on a single machine.</p></blockquote><p>这篇文章的评论区目前已经是非常热闹，很多人在问通过作者给的源码复现不出来作者文章中的结果</p><h2 id="BayesOpt-Adversarial-Attack"><a href="#BayesOpt-Adversarial-Attack" class="headerlink" title="BayesOpt Adversarial Attack"></a><a href="https://openreview.net/forum?id=Hkem-lrtvH" target="_blank" rel="noopener">BayesOpt Adversarial Attack</a></h2><blockquote><p>We propose a query-efficient black-box attack which uses Bayesian optimisation in combination with Bayesian model selection to optimise over the adversarial perturbation and the optimal degree of search space dimension reduction.<br>We demonstrate empirically that our method can <strong>achieve comparable success rates with 2-5 times fewer queries compared to previous state-of-the-art black-box attacks.</strong></p></blockquote><p>可以比之前的方法减少2-5倍的query次数还是很厉害的，扫了一眼实验也比较solid，但还是感觉现在纯做attack不好发文章，即使是balck-box attack</p><h1 id="C-大佬们的文章"><a href="#C-大佬们的文章" class="headerlink" title="C. 大佬们的文章"></a>C. 大佬们的文章</h1><h2 id="Truth-or-backpropaganda-An-empirical-investigation-of-deep-learning-theory"><a href="#Truth-or-backpropaganda-An-empirical-investigation-of-deep-learning-theory" class="headerlink" title="Truth or backpropaganda? An empirical investigation of deep learning theory"></a><a href="https://openreview.net/forum?id=HyxyIgHFvr" target="_blank" rel="noopener">Truth or backpropaganda? An empirical investigation of deep learning theory</a></h2><p>初看标题时查了好半天backpropaganda的意思，很久以后才有点明白过来，大概不是英语母语者很难瞬间get到这个词中包含的笑点吧2333</p><blockquote><p>We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike.  We study the prevalence of local minima in loss landscapes, whether small-norm parameter vectors generalize better (and whether this explains the advantages of weight decay), whether wide-network theories (like the neural tangent kernel) describe the behaviors of classifiers, and whether the rank of weight matrices can be linked to generalization and robustness in real-world networks.</p></blockquote><p>如果文章没有大的纰漏，感觉会是一篇在deep learning theory领域引起巨大反响的文章，从往年来看，一般这类反响剧烈的文章都是奔着best paper去的，应该是一篇不错的文章，可以一读</p><h2 id="Information-Geometry-of-Orthogonal-Initializations-and-Training"><a href="#Information-Geometry-of-Orthogonal-Initializations-and-Training" class="headerlink" title="Information Geometry of Orthogonal Initializations and Training"></a><a href="https://openreview.net/forum?id=rkg1ngrFPr" target="_blank" rel="noopener">Information Geometry of Orthogonal Initializations and Training</a></h2><blockquote><p>Recently mean field theory has been successfully used to analyze properties of wide, random neural networks. It gave rise to a prescriptive theory for initializing feed-forward neural networks with orthogonal weights, which ensures that <strong>both the forward propagated activations and the backpropagated gradients are near `2 isometries</strong> and as a consequence <strong>training is orders of magnitude faster</strong>.</p></blockquote><p>大佬的工作连abstract读起来都吃力，应该是一篇很硬核的文章，作者做的工作主要包括</p><ul><li>作者发现了loss surface上最大曲率 (which is measured by FIM) 是如何与spectral radius of the input-output Jacobian相关的，这在某种程度上解释了为什么orthogonal initialization训练速度比其他方法快几个数量级</li><li>已知正交初始化可以保障gradient的norm在训练前期几乎不发生改变，作者实验研究了在训练中维持参数空间正交的好处，得出结论：manifold optimization of weights performs well regardless of the smoothness of the gradients</li><li>作者最后还联系了Fisher geometry与最近提出的neural tangent kernel之间的联系</li></ul><h2 id="Neural-tangent-kernels-transportation-mappings-and-universal-approximation"><a href="#Neural-tangent-kernels-transportation-mappings-and-universal-approximation" class="headerlink" title="Neural tangent kernels, transportation mappings, and universal approximation"></a><a href="https://openreview.net/forum?id=HklQYxBKwS" target="_blank" rel="noopener">Neural tangent kernels, transportation mappings, and universal approximation</a></h2><blockquote><p>The NTK linearization is a universal approximator, even when looking arbitrarily close to initialization.</p></blockquote><p>不熟悉neural tangent kernel，拜就对了</p><h1 id="233-Researchers迷惑行为大赏"><a href="#233-Researchers迷惑行为大赏" class="headerlink" title="233. Researchers迷惑行为大赏"></a>233. Researchers迷惑行为大赏</h1><h2 id="I-love-your-chain-mail-Making-knights-smile-in-a-fantasy-game-world"><a href="#I-love-your-chain-mail-Making-knights-smile-in-a-fantasy-game-world" class="headerlink" title="I love your chain mail! Making knights smile in a fantasy game world"></a><a href="https://openreview.net/forum?id=BJxRrlBFwB" target="_blank" rel="noopener">I love your chain mail! Making knights smile in a fantasy game world</a></h2><blockquote><p>Agents interact (speak, act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue.</p></blockquote><p><img src="https://pic.ruiwen.com/allimg/201706/46-1F6021P952-51.jpg?x-oss-process=style/qr.ruiwen" width="20%"></p><h2 id="Learning-to-Prove-Theorems-by-Learning-to-Generate-Theorems"><a href="#Learning-to-Prove-Theorems-by-Learning-to-Generate-Theorems" class="headerlink" title="Learning to Prove Theorems by Learning to Generate Theorems"></a><a href="https://openreview.net/forum?id=BJxiqxSYPB" target="_blank" rel="noopener">Learning to Prove Theorems by Learning to Generate Theorems</a></h2><blockquote><p>We consider the task of automated theorem proving, a key AI task. Deep learning has shown promise for training theorem provers, but there are limited human-written theorems and proofs available for supervised learning. To address this limitation, we propose to learn a neural generator that automatically synthesizes theorems and proofs for the purpose of training a theorem prover. Experiments on real-world  tasks demonstrate that synthetic data from our approach significantly improves the theorem prover and <strong>advances the state of the art of automated theorem proving in Metamath</strong>.</p></blockquote><p>以前就听说有做物理的人用deep learning来模拟流体力学方程，学习PDE等等，直到看到这篇文章发现原来还能这样</p><h2 id="How-to-0wn-the-NAS-in-Your-Spare-Time"><a href="#How-to-0wn-the-NAS-in-Your-Spare-Time" class="headerlink" title="How to 0wn the NAS in Your Spare Time"></a><a href="https://openreview.net/forum?id=S1erpeBFPB" target="_blank" rel="noopener">How to 0wn the NAS in Your Spare Time</a></h2><p>注：这里的NAS指的是neural architecture search</p><p>乱码文章标题始祖</p><blockquote><p>We implement our algorithm on PyTorch and Tensorflow. We demonstrate experimentally that we can reconstruct MalConv, a novel data pre-processing pipeline for malware detection, and ProxylessNAS-CPU, a novel network architecture for the ImageNet classification optimized to run on CPUs, without knowing the architecture family. <strong>In both cases, we achieve 0% error.</strong></p></blockquote><h2 id="BREAKING-CERTIFIED-DEFENSES-SEMANTIC-ADVERSARIAL-EXAMPLES-WITH-SPOOFED-ROBUSTNESS-CERTIFICATES"><a href="#BREAKING-CERTIFIED-DEFENSES-SEMANTIC-ADVERSARIAL-EXAMPLES-WITH-SPOOFED-ROBUSTNESS-CERTIFICATES" class="headerlink" title="BREAKING CERTIFIED DEFENSES: SEMANTIC ADVERSARIAL EXAMPLES WITH SPOOFED ROBUSTNESS CERTIFICATES"></a><a href="https://openreview.net/forum?id=HJxdTxHYvB" target="_blank" rel="noopener">BREAKING CERTIFIED DEFENSES: SEMANTIC ADVERSARIAL EXAMPLES WITH SPOOFED ROBUSTNESS CERTIFICATES</a></h2><blockquote><p>Defenses against adversarial attacks can be classified into certified and non-certified. Certifiable defenses make networks robust within a certain $\ell_p$-bounded radius, so that it is impossible for the adversary to make adversarial examples in the certificate bound. We present an attack that <strong>maintains the imperceptibility property of adversarial examples while being outside of the certified radius</strong>.</p></blockquote><p>标题党，本来以为作者是说certified defense在实践中会出现实际不符合预期的情况，实际只是说在semantic的意义上做优化，最终得到的adversarial examples既可以骗过神经网络又可以跑到 $\ell_{p}$ bound之外</p><p>那么这样做的意义是什么？和姚明比较说姚明打三国杀不如我？</p><h2 id="ARTIFICIAL-DESIGN-MODELING-ARTIFICIAL-SUPER-INTELLIGENCE-WITH-EXTENDED-GENERAL-RELATIVITY-AND-UNIVERSAL-DARWINISM-VIA-GEOMETRIZATION-FOR-UNIVERSAL-DESIGN-AUTOMATION-FAR-FROM-THE-MADDING-CROWD-HOW-TO-BE-SHALLOW-TO-AVOID-GOT-LOST-IN-DEEP-FOREST"><a href="#ARTIFICIAL-DESIGN-MODELING-ARTIFICIAL-SUPER-INTELLIGENCE-WITH-EXTENDED-GENERAL-RELATIVITY-AND-UNIVERSAL-DARWINISM-VIA-GEOMETRIZATION-FOR-UNIVERSAL-DESIGN-AUTOMATION-FAR-FROM-THE-MADDING-CROWD-HOW-TO-BE-SHALLOW-TO-AVOID-GOT-LOST-IN-DEEP-FOREST" class="headerlink" title="ARTIFICIAL DESIGN: MODELING ARTIFICIAL SUPER INTELLIGENCE WITH EXTENDED GENERAL RELATIVITY AND UNIVERSAL DARWINISM VIA GEOMETRIZATION FOR UNIVERSAL DESIGN AUTOMATION (FAR FROM THE MADDING CROWD: HOW TO BE SHALLOW TO AVOID GOT LOST IN DEEP FOREST?)"></a><a href="https://openreview.net/pdf?id=SyxQ_TEFwS" target="_blank" rel="noopener">ARTIFICIAL DESIGN: MODELING ARTIFICIAL SUPER INTELLIGENCE WITH EXTENDED GENERAL RELATIVITY AND UNIVERSAL DARWINISM VIA GEOMETRIZATION FOR UNIVERSAL DESIGN AUTOMATION (FAR FROM THE MADDING CROWD: HOW TO BE SHALLOW TO AVOID GOT LOST IN DEEP FOREST?)</a></h2><p>题目振聋发聩，广义相对论，泛达尔文主义，deep forest，黎曼几何，超级人工智能，Figure 1的两张子图分别是道德经和太极</p><p>目前该文章被怀疑是由BERT生成，其训练集见Appendix一节</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>ICLR-2019论文整理</title>
      <link href="/conferences-collection/iclr-papers.html"/>
      <url>/conferences-collection/iclr-papers.html</url>
      
        <content type="html"><![CDATA[<p><del>今年RL的论文怎么这么多。。。</del></p><h2 id="Key-words"><a href="#Key-words" class="headerlink" title="Key words"></a>Key words</h2><a id="more"></a><ul><li>RL $\times$ Adersarial learning</li><li>Guided-policy search (Model-based RL)</li><li>GAIL (IRL)</li><li>Multi-agent RL</li><li>Soft Q-learning (PGM)</li></ul><h2 id="Adversarial-learning"><a href="#Adversarial-learning" class="headerlink" title="Adversarial learning"></a>Adversarial learning</h2><h6 id="Excessive-Invariance-Causes-Adversarial-Vulnerability"><a href="#Excessive-Invariance-Causes-Adversarial-Vulnerability" class="headerlink" title="Excessive Invariance Causes Adversarial Vulnerability"></a><a href="https://openreview.net/forum?id=BkfbpsAcF7" target="_blank" rel="noopener">Excessive Invariance Causes Adversarial Vulnerability</a></h6><p>只看标题觉得结论还是比较有趣的，先mark一下</p><h6 id="The-Limitations-of-Adversarial-Training-and-the-Blind-Spot-Attack"><a href="#The-Limitations-of-Adversarial-Training-and-the-Blind-Spot-Attack" class="headerlink" title="The Limitations of Adversarial Training and the Blind-Spot Attack"></a><a href="https://openreview.net/forum?id=HylTBhA5tQ" target="_blank" rel="noopener">The Limitations of Adversarial Training and the Blind-Spot Attack</a></h6><p>文章研究了adversarial training中存在的blind-spot现象，作者在abstract中如此解释blind-spot attack：</p><blockquote><p>Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the “blind-spot attack”, where the input images reside in “blind-spots” (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold.</p><p>The existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data.</p></blockquote><p>此外作者还发现了2018年Kolter &amp; Wong以及Sinha <em>et al.</em>的adversarial training方法都存在blind-spot问题</p><h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><h6 id="Variational-Discriminator-Bottleneck-Improving-Imitation-Learning-Inverse-RL-and-GANs-by-Constraining-Information-Flow"><a href="#Variational-Discriminator-Bottleneck-Improving-Imitation-Learning-Inverse-RL-and-GANs-by-Constraining-Information-Flow" class="headerlink" title="Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow"></a><a href="https://openreview.net/forum?id=HyxPx3R9tm" target="_blank" rel="noopener">Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow</a></h6><ul><li>UCB大佬集团出品，Pieter Abbeel和Sergey Levine都在作者名单里，</li><li>Variational information bottleneck并非首创，早在2016年<a href="https://arxiv.org/abs/1612.00410" target="_blank" rel="noopener">Alemi <em>et al.</em>的工作</a>就已经将其用于supervised learning任务，以此来reduce model variance；相比之下本文的创新在于将这种思路拓展至几个经典的需要使用discriminator/critic的场景：<ul><li>GAN生成图像，unsupervised learning，文章后面的实验中放了几张生成的$1024\times{}1024$的人脸大图</li><li>GAIL，第一篇为基于GAN做imitation learning提供理论基础的文章</li><li><a href="https://arxiv.org/abs/1710.11248" target="_blank" rel="noopener">AIRL</a>，将GAIL拓展至inverse RL的工作</li></ul></li><li>直接constraint mutual information的upper bound，训练时用了截断的dual gradient descent，此外实验一节也提出了若干trick (加GP等)，目测复现比较困难</li></ul><h6 id="Temporal-Difference-Variational-Auto-Encoder"><a href="#Temporal-Difference-Variational-Auto-Encoder" class="headerlink" title="Temporal Difference Variational Auto-Encoder"></a><a href="https://openreview.net/forum?id=S1x4ghC9tQ" target="_blank" rel="noopener">Temporal Difference Variational Auto-Encoder</a></h6><p>Motivation很有说服力，作者认为一个agent的experience modeling应该具有三个性质：</p><ul><li>model应当学习到MDP中state空间的抽象表示，而不仅仅局限于根据observation做判断</li><li>model应当学习belief state</li><li>mode应当能够学习到<strong>时域上的抽象</strong>，从而使得agent的planning可以更长远</li></ul><h6 id="Rigorous-Agent-Evaluation-An-Adversarial-Approach-to-Uncover-Catastrophic-Failures"><a href="#Rigorous-Agent-Evaluation-An-Adversarial-Approach-to-Uncover-Catastrophic-Failures" class="headerlink" title="Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures"></a><a href="https://openreview.net/forum?id=B1xhQhRcK7" target="_blank" rel="noopener">Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures</a></h6><p>选题很新潮，Motivation是在safety-critical场景下evaluate agent，审稿人的summary简洁而准确：</p><blockquote><p>Proposes an importance sampling approach to sampling failure cases for RL algorithms. The proposal distribution is based on a function learned via a neural network on failures that occur during agent training. The method is compared to random sampling on two problems where the “true” failure probability can be approximated through random sampling. The IS method requires substantially fewer samples to produce failure cases and to estimate the failure probability.</p></blockquote><p>此外也是用adversarial learning的思路来做RL，可以看出这种思路将成为一种大的趋势</p><h6 id="Woulda-Coulda-Shoulda-Counterfactually-Guided-Policy-Search"><a href="#Woulda-Coulda-Shoulda-Counterfactually-Guided-Policy-Search" class="headerlink" title="Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search"></a><a href="https://openreview.net/forum?id=BJG0voC9YQ" target="_blank" rel="noopener">Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search</a></h6><p>Guided policy search系列的工作，需要稍微查一下之前的文献才能看懂</p><h6 id="CEM-RL-Combining-evolutionary-and-gradient-based-methods-for-policy-search"><a href="#CEM-RL-Combining-evolutionary-and-gradient-based-methods-for-policy-search" class="headerlink" title="CEM-RL: Combining evolutionary and gradient-based methods for policy search"></a><a href="https://openreview.net/forum?id=BkeU5j0ctQ" target="_blank" rel="noopener">CEM-RL: Combining evolutionary and gradient-based methods for policy search</a></h6><blockquote><p>In this paper, we propose a different combination scheme using the simple cross-entropy<br>method (<strong>CEM</strong>) and Twin Delayed Deep Deterministic policy gradient (<strong>TD3</strong>), another off-policy deep RL algorithm which improves over DDPG.</p></blockquote><p>审稿人1的评论</p><blockquote><p>This paper combines two different types of existing optimization methods, CEM/CMA-ES and DDPG/TD3, for policy optimization. The approach resembles ERL but demonstrates good better performance on a variety of continuous control benchmarks.  Although I feel the novelty of the paper is limited, the provided promising results may justify the acceptance of the paper.</p></blockquote><h6 id="Directed-Info-GAIL-Learning-Hierarchical-Policies-from-Unsegmented-Demonstrations-using-Directed-Information"><a href="#Directed-Info-GAIL-Learning-Hierarchical-Policies-from-Unsegmented-Demonstrations-using-Directed-Information" class="headerlink" title="Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information"></a><a href="https://openreview.net/forum?id=BJeWUs05KQ" target="_blank" rel="noopener">Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information</a></h6><p>Imitation learning领域的新鲜血液，2017年NIPS上<a href="http://papers.nips.cc/paper/6971-infogail-interpretable-imitation-learning-from-visual-demonstrations.pdf" target="_blank" rel="noopener">info-GAIL</a>的延伸</p><div align="center"><img src="./iclr-papers/paper1.png" width="90%"></div><ul><li>解决的问题：<ul><li>Learning a macro-policy from unsegmented expert demonstrations</li><li>Unsupervised inference of subtask-specific latent variables</li></ul></li><li>提出的方法<ul><li>将info-GAIL的PGM推广为sequence of latent variable的形式，优化latent variable与state-action pair的directed mutual information</li><li>采用Gumbel trick来解决latent variable离散的问题</li><li>用expert demonstrations预训练了一个VAE用于从latent variable的先验分布中采样</li></ul></li><li>作者分析了提出的方法与options framework的关联</li><li>实验中有若干离散latent variable可视化的例子，作者以此来证明他们方法中latent variable的interpretability，审稿人认为实验比较弱，没有大型continuous control任务上的evaluation</li><li>个人认为与infoGAN的inference相似，由于复杂任务的subtask表示本身具有某种程度的复杂性，这种inference很难在复杂任务上学习到对人类有意义的latent variable representation</li></ul><h6 id="Hindsight-policy-gradients"><a href="#Hindsight-policy-gradients" class="headerlink" title="Hindsight policy gradients"></a><a href="https://openreview.net/forum?id=Bkg2viA5FQ" target="_blank" rel="noopener">Hindsight policy gradients</a></h6><p><em>Jürgen Schmidhuber出现在了通讯作者的位置上，审稿人认为contribution有限，但AC力排众议给了这篇文章很高的评价</em></p><p>Metareview:</p><blockquote><p>The paper generalizes the concept of “hindsight”, i.e. the recycling of data from trajectories in a goal-based system based on the goal state actually achieved, to policy gradient methods.</p></blockquote><p>Review 1:</p><blockquote><p>The authors present HPG, which applies the hindsight formulation already applied to off-policy RL algorithms (hindsight experience replay, HER, Andrychowicz et al., 2017) to policy gradients.</p></blockquote><h6 id="Probabilistic-Planning-with-Sequential-Monte-Carlo-methods"><a href="#Probabilistic-Planning-with-Sequential-Monte-Carlo-methods" class="headerlink" title="Probabilistic Planning with Sequential Monte Carlo methods"></a><a href="https://openreview.net/forum?id=ByetGn0cYX" target="_blank" rel="noopener">Probabilistic Planning with Sequential Monte Carlo methods</a></h6><blockquote><p>Sequential Monte Carlo (SMC) has since its inception some 25 years ago proved to be a powerful and generally applicable tool. The authors of this paper continue this development in a very interesting and natural way by showing how SMC can be used to solve challenging planning problems. This is a enabled by reformulating the planning problem as an inference problem via the recent trend referred to as “control as inference”. </p></blockquote><h6 id="Learning-to-Understand-Goal-Specifications-by-Modelling-Reward"><a href="#Learning-to-Understand-Goal-Specifications-by-Modelling-Reward" class="headerlink" title="Learning to Understand Goal Specifications by Modelling Reward"></a><a href="https://openreview.net/forum?id=H1xsSjC9Ym" target="_blank" rel="noopener">Learning to Understand Goal Specifications by Modelling Reward</a></h6><p>文章讨论了NLP任务中的RL应用，当reward没有良好定义的时候，如何用一个discriminator D来生成pseudo rewards。感觉motivation蛮有趣的，可以一读。</p><h6 id="Adversarial-Imitation-via-Variational-Inverse-Reinforcement-Learning"><a href="#Adversarial-Imitation-via-Variational-Inverse-Reinforcement-Learning" class="headerlink" title="Adversarial Imitation via Variational Inverse Reinforcement Learning"></a><a href="https://openreview.net/forum?id=HJlmHoR5tQ" target="_blank" rel="noopener">Adversarial Imitation via Variational Inverse Reinforcement Learning</a></h6><p>Metareview给了这篇文章很高的评价</p><p>作者在abstract中claim的contribution：</p><blockquote><p>Our proposed method builds on the framework of generative adversarial networks and introduces the empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies. Empowerment-based regularization prevents the policy from overfitting to expert demonstrations, which advantageously leads to more generalized behaviors that result in learning near-optimal rewards.</p></blockquote><p>Reviewer在评论区claim的contribution：</p><blockquote><p>This paper builds on the AIRL framework (Fu et al., 2017) by combining the empowerment maximization objective for optimizing both the policy and reward function. Algorithmically, the main difference is that this introduces the need to optimize a inverse model (q), an empowerment function (Phi) and alters the AIRL updates to the reward function and policy. This paper presents experiments on the original set of AIRL tasks, and shows improved performance on some tasks.</p></blockquote><h6 id="The-Laplacian-in-RL-Learning-Representations-with-Efficient-Approximations"><a href="#The-Laplacian-in-RL-Learning-Representations-with-Efficient-Approximations" class="headerlink" title="The Laplacian in RL: Learning Representations with Efficient Approximations"></a><a href="https://openreview.net/forum?id=HJlNpoA5YQ" target="_blank" rel="noopener">The Laplacian in RL: Learning Representations with Efficient Approximations</a></h6><p>Abstract非常吸引我，感觉讨论的问题很有趣，且和我之前的工作似乎有一点点的关联，需要仔细研究下</p><blockquote><p>The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning.</p></blockquote><p>所有审稿人中，审稿人2起初的意见相对比较negative，质疑主要围绕两点展开:</p><ul><li>作者只使用random policy来学习state representation，这在比较大的MDP上显然不能对state space进行有效的探索</li><li>作者在文中指出，Laplacian representation的一个应用在于reward-shaping，但审稿人对sample efficiency提出了质疑，认为文章中的实验并没有公正地反映出sample efficiency</li></ul><p>经过rebuttal审稿人接受了作者的说法</p><h6 id="Marginal-Policy-Gradients-A-Unified-Family-of-Estimators-for-Bounded-Action-Spaces-with-Applications"><a href="#Marginal-Policy-Gradients-A-Unified-Family-of-Estimators-for-Bounded-Action-Spaces-with-Applications" class="headerlink" title="Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications"></a><a href="https://openreview.net/forum?id=HkgqFiAcFm" target="_blank" rel="noopener">Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications</a></h6><blockquote><p>With the marginal policy gradients family of estimators we present a unified analysis of the variance reduction properties of APG and CAPG; our results provide a stronger guarantee than existing analyses for CAPG.</p></blockquote><p>又是来自Tencent AILab大佬Han Liu组的paper，研究的是在RTS游戏背景下的policy gradient方差问题，RTS游戏中action space往往由几个连续的变量来表示，连续变量的值往往表示英雄移动的方向或施放技能的方向等。此前RL方面还没有类似的研究RTS特定背景的的工作，值得一读。</p><h6 id="Soft-Q-Learning-with-Mutual-Information-Regularization"><a href="#Soft-Q-Learning-with-Mutual-Information-Regularization" class="headerlink" title="Soft Q-Learning with Mutual-Information Regularization"></a><a href="https://openreview.net/forum?id=HyEtjoCqFX" target="_blank" rel="noopener">Soft Q-Learning with Mutual-Information Regularization</a></h6><p>Soft Q-learning的思路来源于概率图模型，之前的研究表明soft Q-learning中的entropy regularizer可以很有效地提高exploration的效率与policy的robustness，因此被广泛应用在很多RL任务中。而根据作者在abstract中的claim：</p><blockquote><p>However, entropy regularization might be undesirable when actions have significantly different importance.</p><p>We propose a theoretically motivated framework that dynamically weights the importance of actions by using the mutual-information.</p><p>This regularizer encourages the policy to be close to a non-uniform distribution that assigns higher probability mass to more important actions.</p></blockquote><p>文章的formulation看起来还是很漂亮的，可以仔细研究下。</p><h6 id="Deep-reinforcement-learning-with-relational-inductive-biases"><a href="#Deep-reinforcement-learning-with-relational-inductive-biases" class="headerlink" title="Deep reinforcement learning with relational inductive biases"></a><a href="https://openreview.net/forum?id=HkxaFoC9KQ" target="_blank" rel="noopener">Deep reinforcement learning with relational inductive biases</a></h6><p>划重点，文章在星际2的几个小型任务上刷到了SOTA</p><blockquote><p>We introduce an approach for augmenting model-free deep reinforcement learning agents with a mechanism for relational reasoning over structured representations, which improves performance, learning efficiency, generalization, and interpretability. Our architecture encodes an image as a set of vectors, and applies an iterative message-passing procedure to discover and reason about relevant entities and relations in a scene.</p></blockquote><h6 id="Preferences-Implicit-in-the-State-of-the-World"><a href="#Preferences-Implicit-in-the-State-of-the-World" class="headerlink" title="Preferences Implicit in the State of the World"></a><a href="https://openreview.net/forum?id=rkevMnRqYQ" target="_blank" rel="noopener">Preferences Implicit in the State of the World</a></h6><p>Pieter Abbeel组的第二篇文章，文章的主旨可以用一句话概括：</p><blockquote><p>Inferring preferences from the initial state of an environment.</p></blockquote><p>文章中放出的代码地址<a href="https://github.com/HumanCompatibleAI/rlsp" target="_blank" rel="noopener">https://github.com/HumanCompatibleAI/rlsp</a></p><h2 id="Multi-agent-RL"><a href="#Multi-agent-RL" class="headerlink" title="Multi-agent RL"></a>Multi-agent RL</h2><h6 id="M-3RL-Mind-aware-Multi-agent-Management-Reinforcement-Learning"><a href="#M-3RL-Mind-aware-Multi-agent-Management-Reinforcement-Learning" class="headerlink" title="M^3RL: Mind-aware Multi-agent Management Reinforcement Learning"></a><a href="https://openreview.net/forum?id=BkzeUiRcY7" target="_blank" rel="noopener">M^3RL: Mind-aware Multi-agent Management Reinforcement Learning</a></h6><p>Multi-agent的工作，mark一下，有时间可以仔细看看</p><h6 id="Learning-to-Schedule-Communication-in-Multi-agent-Reinforcement-Learning"><a href="#Learning-to-Schedule-Communication-in-Multi-agent-Reinforcement-Learning" class="headerlink" title="Learning to Schedule Communication in Multi-agent Reinforcement Learning"></a><a href="https://openreview.net/forum?id=SJxu5iR9KQ" target="_blank" rel="noopener">Learning to Schedule Communication in Multi-agent Reinforcement Learning</a></h6><blockquote><p>In this paper, we study a practical scenario when (i) the communication bandwidth is limited and (ii) the agents share the communication medium so that only a restricted number of agents are able to simultaneously use the medium, as in the state-of-the-art wireless networking standards.</p></blockquote><h6 id="Multi-Agent-Dual-Learning"><a href="#Multi-Agent-Dual-Learning" class="headerlink" title="Multi-Agent Dual Learning"></a><a href="https://openreview.net/forum?id=HyGhN2A5tm" target="_blank" rel="noopener">Multi-Agent Dual Learning</a></h6><p>将dual learning的思路与multi-agent结合，传统的dual learning一般是两个模型互相利用对方的对偶性质来进行学习，这篇文章将idea拓展至multi-agent环境下的多个目标之间的互相交互，并且在machine translation任务上刷到了SOTA。</p><p>Metareview:</p><blockquote><p>A paper that studies two tasks: machine translation and image translation. The authors propose a new multi-agent dual learning technique that takes advantage of the symmetry of the problem. The empirical gains over a competitive baseline are quite solid.</p></blockquote><h2 id="Deep-learning"><a href="#Deep-learning" class="headerlink" title="Deep learning"></a>Deep learning</h2>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>ICML-2019 论文整理</title>
      <link href="/conferences-collection/icml-2019-summary.html"/>
      <url>/conferences-collection/icml-2019-summary.html</url>
      
        <content type="html"><![CDATA[<p>ICML-2019 accepted paper list已经放出来一个月了，知乎上<a href="https://www.zhihu.com/question/321210381/answer/724436093" target="_blank" rel="noopener">有关ICML-2019的问题</a>下还一片火热，让周末摸鱼的我感受到了强烈的peer pressure，开始后知地后觉滚去公司刷paper</p><p>扫了一眼accepted list，感觉今年很多文章都很有趣，值得深入研究一下</p><a id="more"></a><h2 id="Machine-learning"><a href="#Machine-learning" class="headerlink" title="Machine learning"></a>Machine learning</h2><h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><h3 id="Off-Policy-Deep-Reinforcement-Learning-without-Exploration"><a href="#Off-Policy-Deep-Reinforcement-Learning-without-Exploration" class="headerlink" title="Off-Policy Deep Reinforcement Learning without Exploration"></a><a href="https://arxiv.org/abs/1812.02900" target="_blank" rel="noopener">Off-Policy Deep Reinforcement Learning without Exploration</a></h3><p>问题背景和之前看过的[Tencent AILab 那篇 imitation learning]的文章基本一致：你有一批<strong>固定的</strong>数据，可以用这一堆数据去进行off-policy的学习，但是训练过程中不允许有更多的exploration，也不允许收集更多的数据</p><p>这种场景在定价系统或推荐系统中较为常见，前者explore的代价太大，一不小心explore到很糟糕的地方就血本无归；而在大部分做推荐搜索的公司内部，万恶的PM是绝对不会允许你拿用户做实验的，万一一个explore不好就流失了一个用户；因此这类场合下的RL往往是先收集一波数据，然后拿这批固定的数据做off-policy的训练，这样在训练期线上只需跑一个安全且保守的策略即可</p><p>作者首先提出了一个很惊人的结论——在以上问题设定的大背景下，由于 <strong>extrapolation error</strong> 的存在，一些传统的RL算法，包括DQN、DDPG等，是不会收敛的</p><blockquote><p>In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting.</p></blockquote><p>根本原因在于，如此设定下的off-policy RL优化的MDP实际上是batch data中的数据所代表的MDP，而不是真实环境的MDP，这其中会产生一个不能被bound的误差</p><p>对于state s和acton a，我们首先定义这个误差函数 $\epsilon(s,a)$ 为off-policy收集到的数据数据所代表的MDP与真实环境MDP之间的误差，而 $\epsilon$ 在state和action下的积分为extrapolation error：</p><script type="math/tex; mode=display">\epsilon(s,a)=Q^{\pi}(s,a)-Q^{\mathcal{B}}(s,a) \\e_{MDP}=\int_{\mathcal{S}}d^{\pi}(s)\int_{\mathcal{A}}\pi(a|s)\epsilon(s,a)\ dads</script><p>把两个Q function用Bellman equation展开，可以发现$\epsilon(s,a)$也可以写成类似Bellman equation的递推式形式：</p><p><img src="./icml-2019-summary/off-policy-1.png" width="100%"></p><p>以加号为界将两部分分隔开，这个递推式由两部分组成，左边这部分的核心是 $p_{M}(s’|s,a)-p_{\mathcal{B}}(s’|s,a)$，可以理解为MDP的model bias，右半部分是一个Bellman operator。已知Bellman operator一定是contraction mapping，那么问题就在于前半部分的model bias是不是可以被bound</p><p>文章指出至少有三种情况下，这个model bias可以是非常大的，这里用自己的语言总结一下</p><ul><li><strong>Model bias:</strong> batch size比较小，那么batch中的数据对真实MDP的表达方差会非常大，实际上相当于在学一个non-stationary的MDP</li><li><strong>Absent data:</strong> 这点在连续state-action space的MDP中比较常见，连续空间中，用神经网络拟合从 $\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ 本质是用数据中已有的state和action学习到整个state-action space空间的插值，那么对于某个特定的state，如果target policy访问到这个state的概率很高，但由behavior policy收集到的数据中恰好缺少接近这个state的样本，那么对于这个state的value estimation error就会任意大</li><li><strong>Training mismatch:</strong> 即使数据量足够，由于从dataset中的采样是均匀的，如果behavior policy收集到的数据分布和target policy对应的数据分布不一样，那么误差也可以很大（<em>这是不是反过来又证明了TRPO/PPO类算法的先进性？</em>）</li></ul><p>作者在实验中对比了三种场景的off-policy训练</p><ol><li><strong>Final buffer:</strong> 从零开始训练一个DDPG，将这个DDPG从开始训练到收敛所有用到的样本收集起来训练off-policy DDPG</li><li><strong>Concurrent:</strong> 训练一个DDPG，将其训练数据同时feed给另一个完全异步同时启动的off-policy DDPG</li><li><strong>Imitation:</strong> 拿一个已经训练好的DDPG来跑expert demonstration给off-policy DDPG来训练</li></ol><p>实验结果发现1和2的off-policy DDPG比普通DDPG差很多，3则完全不收敛，且value出现了爆炸现象</p><blockquote><p>这个value爆炸的现象，我做实验也遇到过，当时以为是哪里写错把policy loss的梯度传到Q-function里面了，后来分析了一下，其实是因为DDPG训练的loss是一个正反馈系统，如果有某些访问概率很大的state的value估计很差的话，这个误差会随着Bellman递推越来越大。虽然提高起来蛮不可思议，但确实即使没有gradient指导，value也会出现爆炸的现象</p></blockquote><p><img src="./icml-2019-summary/off-policy-2.png" width="80%"></p><p>作者提出的解决方案是，对于上面的递推式，只要保证MDP是deterministic的，即可使得左半部分的model bias为0，作者还证明了对于任意state-action pair， $e_{MDP}=0$ 的充要条件是model-bias为0</p><p>deterministic MDP其实是个很强的条件，绝大多数RL问题都不满足，因此作者假设对于任意state-action pair，它与batch data中的数据相似度可以用一个conditional marginal likelihood $p_{\mathcal{B}}(a|s)$ 学出来。实际作者是用VAE来学的，这样就可以直接从这个分布中采样，采样后在所有action中选择value最大的那个action作为Q value的估计即可。可以想到随着训练迭代，VAE采样出的数据分布会asymptotically follow真实MDP的数据分布</p><p><em>这里还需要check作者前作 <a href="https://arxiv.org/abs/1802.09477" target="_blank" rel="noopener">Clipped Double Q learning</a>，还是有点复杂的</em></p><p><img src="./icml-2019-summary/off-policy-3.png" width="85%"></p><h3 id="Actor-Attention-Critic-for-Multi-Agent-Reinforcement-Learning"><a href="#Actor-Attention-Critic-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Actor-Attention-Critic for Multi-Agent Reinforcement Learning"></a><a href="https://arxiv.org/abs/1810.02912" target="_blank" rel="noopener">Actor-Attention-Critic for Multi-Agent Reinforcement Learning</a></h3><p>见<a href="https://chenshawn.github.io/2019/09/09/2019-fall-review/#toc-heading-6" target="_blank" rel="noopener">Multi-Agent问题汇总Actor-Attention-Critic</a></p><h3 id="Open-ended-Learning-in-Symmetric-Zero-sum-Games"><a href="#Open-ended-Learning-in-Symmetric-Zero-sum-Games" class="headerlink" title="Open-ended Learning in Symmetric Zero-sum Games"></a><a href="http://proceedings.mlr.press/v97/balduzzi19a/balduzzi19a.pdf" target="_blank" rel="noopener">Open-ended Learning in Symmetric Zero-sum Games</a></h3><p>见<a href="https://chenshawn.github.io/2019/09/09/2019-fall-review/#toc-heading-8" target="_blank" rel="noopener">Multi-Agent问题汇总#Actor-Attention-Critic</a></p><h3 id="Action-Robust-Reinforcement-Learning-and-Applications-in-Continuous-Control"><a href="#Action-Robust-Reinforcement-Learning-and-Applications-in-Continuous-Control" class="headerlink" title="Action Robust Reinforcement Learning and Applications in Continuous Control"></a><a href="https://arxiv.org/abs/1901.09184" target="_blank" rel="noopener">Action Robust Reinforcement Learning and Applications in Continuous Control</a></h3><p>如题，本文研究的是连续空间中的robust RL控制问题</p><p>在此之前，虽然已经有大量文献对robust MDP问题进行了充分的研究，但他们分析的场景的大多都是tabular case或者linear function approximator，这些研究结果很难拓展到非线性的连续state-action space中；<a href="https://arxiv.org/pdf/1703.02702.pdf" target="_blank" rel="noopener">2017年Pinto等人发表于ICML2017的文章</a>中，作者曾提出了一种基于对抗博弈优化RL模型robustness的方法，该方法得到了很好的实验结果，但其背后缺乏理论保障</p><p>本文中作者研究的主要是对于action空间的鲁棒性，文章以minimax对抗博弈作为基础，设定了两种博弈情况</p><ul><li>模型预测得到的action与实际执行的action不同，有一个adversary试图通过修改实际执行的action来让agent拿到尽可能低的reward，即 $a=\delta a^{+}+(1-\delta)a^{-}$，这种设定叫做NR-MDP</li><li>存在$\alpha$的概率使得模型与环境交互的时候，实际执行的是另一个完全不同的动作；否则执行模型的输出，这种设定叫做PR-MDP</li></ul><p>针对两种设定，本文在Bellman equation的层面上提出了统一的minimax鲁棒训练解决方案，算法分两步</p><ul><li>给定一个adversarial strategy，计算optimal counter strategy</li><li>基于当前的value做一步Bellman propagation</li></ul><p>文章还指出非soft的收敛速度会比non-soft得快，但soft面对错误更robust</p><p><img src="./icml-2019-summary/action-robust.png" width="98%"></p><p>实验中作者采用的是类似DDPG的形式</p><ul><li>PR-MDP: $\pi^{mix}(u|s;\theta,\bar{\theta})=(1-\alpha)(u-\mu_{\theta}(s))+\alpha(u-\bar{\mu_{\theta}}(s))$</li><li>NR_MDP: $\pi^{mix}(u|s;\theta,\bar{\theta})=u-[(1-\alpha)\mu_{\theta}(s)+\alpha\bar{\mu_{\theta}}(s)]$</li></ul><h3 id="Distributionally-Robust-Reinforcement-Learning"><a href="#Distributionally-Robust-Reinforcement-Learning" class="headerlink" title="Distributionally Robust Reinforcement Learning"></a><a href="https://arxiv.org/abs/1902.08708" target="_blank" rel="noopener">Distributionally Robust Reinforcement Learning</a></h3><p>本文出自今年ICML-2019的workshop RL4RealLife，主要解决的问题是agent探索过程中的safety问题，作者在MDP的层面提出了一种新的Bellman operator，并证明了该operator</p><ul><li>is a valid Bellman operator, i.e., a monotone $\ell_{\infty}$-norm $\gamma$ contraction on value functions</li><li>has polynomial convergence rate intead of exponential</li><li>is distributionally robust, i.e., has safety guarantee</li></ul><p>从intuition的层面来讲，本文的核心思想是这样的：做policy iteration时，对于当前agent的policy $\pi$，如果我们希望$\pi$在explore的时候可以避免探索可能会带来非常差value的state，我们需要使用一个在能够带来最差value的adversarial policy $\tilde{\pi}$来做policy evaluation，即估计value的值；同时在policy improving时让$\pi$沿着用$\tilde{\pi}$估计出的value的greedy方向前进</p><p>如何定义这个adversarial policy $\tilde{\pi}$呢？我们可以定义它在一个以$\pi$为球心的$\epsilon$-ball中；距离定义的话文中用的是KL divergence（KL作为metric不满足对称性，它的Hessian倒是可以作为一个合法的Riemannian metric tensor，不过本文中并没有严格地追求这一点上的严谨）</p><script type="math/tex; mode=display">\tilde{\pi}=\arg\min_{D_{KL}(\pi,\tilde{\pi})\leq{\epsilon}}\mathbb{E}_{\tilde{\pi}}[R_{t}+\gamma V(s_{t+1})]</script><p>比较容易想到的一个问题在于，这种用worst case adversarial policy来做policy evaluation的思路会导致agent在短期内采取很保守的探索策略，每步都这样explore的话势必会影响收敛速度，因此作者又提出这种方法可以和SAC结合，因为SAC的设定中每步都是包含有entropy正则的，模型可以自己去学习长期的exploration entropy取舍问题</p><p>本文实验部分对比的主要baseline算法是SAC，实验环境为PyBullet（原RobotSchool环境，免费下载使用，不像MuJoCo需要licence），Figure 1中可以看到，虽然在average return指标上作者提出的方法对比SAC提升不是很显著，但从降低方差的层面来说，DRSAC提升现还是比较明显的</p><p><img src="./icml-2019-summary/drsac.png" width="98%"></p><p>由于本文是OpenReview公开审稿，审稿人#2给了一些建设性的意见：在更多的环境下做实验，也可以分析简单实验环境下agent的行为特点，这两点经验非常值得学习</p><h3 id="On-the-Generalization-Gap-in-Reparameterizable-Reinforcement-Learning"><a href="#On-the-Generalization-Gap-in-Reparameterizable-Reinforcement-Learning" class="headerlink" title="On the Generalization Gap in Reparameterizable Reinforcement Learning"></a><a href="https://arxiv.org/abs/1905.12654" target="_blank" rel="noopener">On the Generalization Gap in Reparameterizable Reinforcement Learning</a></h3><p>To the best of my knowledge, this is the first paper comprehensively discussing and analyzing the <strong>generalization issues</strong> of reinforcement learning from a theoretic approach.</p><p>作者指出，要对on-policy RL做具体的理论分析有两个方面的难点</p><ul><li>第一，训练过程中episode的distribution一直在随policy的变化而变化，这为finite sample analysis带来了难度</li><li>第二，当前的RL研究普遍混淆了两种不同意义的generalization，作者称这两种generalization gap实际上是有本质区别的<ul><li>由训练环境本身带来的randomness，例如很多RL模型无法在不同的random seed下得到consistent performance，对于这种generalization gap作者称之为intrinsic error，与supervised learning中的generalization意义是类似的</li><li>由于环境变化所带来的distribution shift，例如RARL中在默认MuJoCo环境下训练模型，在不同的relative mass下测试模型性能，得到模型缺乏鲁棒性的结论，作者将这种generalization gap称为external error，并认为这种error比较类似于transfer learning中的误差</li></ul></li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Machine Learning Conferences Paper Reading Collection</title>
      <link href="/conferences-collection/index.html"/>
      <url>/conferences-collection/index.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://aaai.org/Conferences/AAAI-19/wp-content/uploads/2017/09/AAAI-19-Hawaii-Beach-Shade.png" alt=""></p><ul><li><a href="./nips-2019-summaries.html">NeurIPS 2019</a></li><li><a href="./icml-2019-summary.html">ICML 2019</a></li><li><a href="./iclr-papers.html">ICLR 2019</a></li><li><a href="./iclr-2020-openreview.html">ICLR 2020</a></li></ul><p>Comming next sooooooooooooon:</p><ul><li>ICML 2020</li><li>NeurIPS 2020</li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>NeurIPS-2019 论文整理</title>
      <link href="/conferences-collection/nips-2019-summaries.html"/>
      <url>/conferences-collection/nips-2019-summaries.html</url>
      
        <content type="html"><![CDATA[<p>NeurIPS-2019 还在审稿中，但是很多文章已经在ArXiv上release出来了，本文记录一些最近看过的NeurIPS文章</p><a id="more"></a><h2 id="Machine-learning"><a href="#Machine-learning" class="headerlink" title="Machine learning"></a>Machine learning</h2><h3 id="When-Does-Label-Smoothing-Help"><a href="#When-Does-Label-Smoothing-Help" class="headerlink" title="When Does Label Smoothing Help?"></a><a href="https://arxiv.org/abs/1906.02629" target="_blank" rel="noopener">When Does Label Smoothing Help?</a></h3><p>Hinton组有关label smoothing和distillation的新文章，文章总体偏实验，empirical results有以下几点发现：</p><ol><li>作者提出了一种新的神经网络可视化方法，将神经网络倒数第二层的神经元输出进行可视化后，发现加了label smoothing的model类内间距要比不加label smoothing的小</li><li>label smoothing implicitly calibrates learned models so that the confidences of their predictions are more aligned with the accuracies of their predictions.</li><li>label smoothing在缩小类内间距，降低不同类别的耦合程度的同时，从而提高网络的准确性，也抹消掉了类间的的一些semantic information，所以做distillation的时候，teacher网络用加了label smoothing的反而比不加的更差，i.e., teacher网络的acc高不是student蒸馏效果好的充分条件</li></ol><p>下图是1的实验</p><p><img src="./nips-2019-summaries/hinton-exp1.png" alt="placeholder"></p><p>2的实验（个人感觉结果没有很significant，不过实验的setting上确实挑不出刺来）</p><p><img src="./nips-2019-summaries/hinton-exp2.png" alt="abc"></p><p>为了说明结论3，作者用一些近似的方法可视化了输入样本与倒数第二层的softmax logits之间的互信息，发现加了label smoothing的teacher互信息比不加label smoothing的要低很多，因此得出结论</p><blockquote><p>The figure shows that using these better performing teachers is no better, and sometimes worse, than training the student directly with label smoothing, as the relative information between logits is “erased” when the teacher is trained with label smoothing.</p><p>Therefore, a teacher with better accuracy is not necessarily the one that distills better.</p></blockquote><p><img src="./nips-2019-summaries/hinton-exp3.png" alt="safa"></p><h3 id="You-Only-Propagate-Once-Accelerating-Adversarial-Training-via-Maximal-Principle"><a href="#You-Only-Propagate-Once-Accelerating-Adversarial-Training-via-Maximal-Principle" class="headerlink" title="You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle"></a><a href="https://arxiv.org/pdf/1905.00877v5.pdf" target="_blank" rel="noopener">You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle</a></h3><p>大佬Yiping Lu的文章，题目模仿了YOLO的风格，主要解决的问题是加速基于PGD的adversarial training过程</p><p>对抗攻击问题可以写作</p><script type="math/tex; mode=display">\min_{\theta}\max_{\eta\leq{\epsilon}}\mathbb{E}_{x}[J(x+\eta,y;\theta)]</script><p>可以看出主要的计算量瓶颈在于内层循环的maximization，作者的方法是通过chain rule分解，<strong>近似认为</strong> adversarial perturbation $\eta$ 只与神经网络第一层有关，假设n层的神经网络映射表示为</p><script type="math/tex; mode=display">J(f_{n}(\dots f(f_{2}(f_{1}(f_{0}(x+\eta,\theta_{0}),\theta_{1}),\theta_{2}),\dots,\theta_{n}))</script><p>则 $\nabla_{\eta}J(.)=\nabla_{f_{0}}J(.)\nabla_{\eta}f(x+\eta,\theta_{0})$，作者的思路是fix前面一项，认为 $p=\nabla_{f_{0}}J(.)$，在inner loop中只算一次 $p$ ，剩余时间都只back-propagate第一层，由此减少PGD的计算量</p><p>显然这是一种approximation，因为网络后面的层的响应是会受到前面 $x+\eta$ 影响的，那么问题就来了</p><ul><li>这种approximation是否有bound？</li><li>这种优化得到的最优解是否唯一？</li><li>这种优化是否可以收敛？</li></ul><p>作者在theorem 1中证明：如果将神经网络每层的映射看成一个离散ode，定义Hamiltonian为</p><script type="math/tex; mode=display">H_{t}(x,p,\theta_{t})=pf_{t}(x,\theta_{t})-\frac{1}{B}R_{t}(x,\theta_{t})</script><p>那么只有第一层会包含 $\eta$ 项，满足性质</p><p><img src="./nips-2019-summaries/yiping1.png" width="99%"></p><p>因此这种优化方式存在最优解</p><ul><li>文章证明涉及 Pontryagin’s Maximum Principal ，这里的背景不是很懂</li><li>Theorem 1中 $\{ f_{t}(x,\theta):\theta\in{\Theta} \}$ convex w.r.t. $t$ ，这个条件在实际应用中ResNet是否满足？</li></ul><h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><h3 id="Imitation-learning-without-interacting-with-environment"><a href="#Imitation-learning-without-interacting-with-environment" class="headerlink" title="Imitation learning without interacting with environment"></a>Imitation learning without interacting with environment</h3><p>出自NeurIPS 2018的文章<a href="https://papers.nips.cc/paper/7866-exponentially-weighted-imitation-learning-for-batched-historical-data" target="_blank" rel="noopener">Exponentially Weighted Imitation Learning for Batched Historical Data</a>，由AILab独立完成，作者列表里Han Liu与前主任Tong Zhang榜上有名</p><p>文章研究的背景非常贴合我们的业务，目前主流的imitation learning方法，无论是早期的DAgger还是后来的各种GAIL类方法，训练的过程中都必须要和environment做交互，然而实践中很多情况下与环境交互的成本是非常高的，举几个例子</p><ul><li>OpenAI的Dota2，DeepMind的AlphaGo系列，一个是不完全信息的多智体博弈，一个是完全信息的MCTS模拟，采用的与环境交互方式都是self-play，因此不需要任何人类数据，agent可以真正意义上做到在与environment的交互中学习——为此付出的代价是学习过程非常的漫长，无论是OpenAI还是DeepMind都需要相当于人类上万年的学习时间来达到一个能看得过去的结果</li><li>Robotics领域中，现实场景下做交互不能像在simulator里一样随心所欲，机械臂还是很容易坏掉的，以往robotics的人经常考虑的是用model-based learning方法进行学习，然而model-based的方法在复杂环境、大型MDP下复杂度极高</li></ul><p>这篇文章的background setting如题所述：我们只有batched historical data，而不能用任何方法与environment做直接交互</p><p><strong>Q1:</strong> How about directly train a policy model with supervised learning (behavior cloning)?</p><p>首先一个显而易见的问题在于distributional shift，其次我们的historical batched data中，每个state都会对应一个reward，这是reinforcement learning与supervised learning的一个重大的差异</p><p>If we can not make full use of the reward signal in data, the performance of behavior cloning will certainly be upper bouned by the batched historical data.</p><p>This paper tries to address the problem of <strong>learning a better policy</strong> than the expert policy with only batched historical data.</p><h4 id="Algorithm-and-analysis"><a href="#Algorithm-and-analysis" class="headerlink" title="Algorithm and analysis"></a>Algorithm and analysis</h4><p><img src="./algo1.png" width="95%"></p><p>算法本身非常简单，具体实现时除了policy network $\pi_{\theta}$，我们还需要用supervised learning训练一个$V_{w}(s_{t})$，之前的paper中一般的做法是用cumulated reward $R_{t}$ 算: $A(s_{t},a_{t})=\mathbb{E}_{\pi}[R_{t}-V_{w}(s_{t})]$，文中作者表示他们的做法是$A(s_{t},a_{t})=(R_{t}-V_{w}(s_{t}))/c$, where $c$ is the exponential moving average norm of the advantage, to make the scale of $\beta$ stable across different environments，这种做法计算复杂度也相对较低</p><p><img src="./nips-2019-summaries/theory1.png" width="95%"></p><p>这段分析在看的时候就很头大，看rebuttal记录时发现reviewer和我提出了类似的疑问，主要有两点</p><blockquote><p>Regarding the state distribution $d(s)$, I assume it is always referring to the state distribution of the behavior policy $\pi$. However, below line 146, when you say that it is imitating a new hypothetic policy $\tilde{\pi}$, shouldn’t the state distribution becomes the state-distribution resulting from $\tilde{\pi}$? The state distribution of $\tilde{\pi}$ and $\pi$ could be different.</p></blockquote><p>我暂时没证出来$d^{\tilde{\pi}}(s)\propto{d^{\pi}(s)\exp(-C(s))}$</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>2017年交大Yann Lecun讲座的照片数张</title>
      <link href="/hello-world/YanLecun.html"/>
      <url>/hello-world/YanLecun.html</url>
      
        <content type="html"><![CDATA[<p>换电脑的时候收拾旧电脑里的东西，偶然翻出了2017年在交大参加Yann Lecun讲座时拍的PPT照片，讲的基本都是2017年FAIR的最新研究。回想2017虽然时间不算长却也恍如隔世，重新翻看这些照片时竟有种莫名的新鲜感。</p><a id="more"></a><p><img src="./YanLecun/1.jpg" width="500px"><br><img src="./YanLecun/2.jpg" width="500px"><br><img src="./YanLecun/3.jpg" width="500px"><br><img src="./YanLecun/4.jpg" width="500px"><br><img src="./YanLecun/5.jpg" width="500px"></p><p>时至如今，当时的最新研究或已成为某个领域无法绕开的里程碑，或已被新的state-of-the-art超越，或被人遗忘，或写进教科书。</p><ul><li>2017年初Arjovski组提出<a href="https://arxiv.org/abs/1701.07875" target="_blank" rel="noopener">Wasserstein GAN</a>，一举使得之前鲜有人问津的GAN推向如日中天的程度</li><li>2017年10月<a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">Proximal policy optimization</a>开始挂在ArXiv，用简单有效的实践把从前晦涩小众的<a href="https://arxiv.org/abs/1502.05477v4" target="_blank" rel="noopener">Trust region policy gradient</a>转为benchmark型的算法</li><li>从不同的角度出发，但与PPO解决了同一问题的<a href="https://arxiv.org/abs/1708.05144" target="_blank" rel="noopener">ACKTR</a></li><li>2017年计算机传统视觉任务开始成熟，新出现的文章基本是在前人摸索出的大框架下修修补补，具体来说<ul><li>语义分割：Google的DeepLab组发了<a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener">一篇PAMI</a>，宣告这个领域主要的重大问题基本都已解决，DeepLab的ResNet + Atrous convolution + CRF一统天下</li><li>目标检测：<a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask RCNN</a>与<a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="noopener">YOLO v3</a>平分秋色，二者都成为很多下游计算机视觉领域的标准基础模型</li><li>目标识别：最后一篇受到比较广泛关注的文章，可能是MSRA的这篇<a href="https://ieeexplore.ieee.org/document/8099959" target="_blank" rel="noopener">Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-Grained Image Recognition</a>，之后该领域已经完全被ResNet攻克，从前做recognition的组开始转型</li></ul></li><li>Yann Lecun在FAIR的一个组一直在做graph spectral convolution，其中用到的一些理论和实践都非常漂亮，虽然2018年底graph convolution突然备受关注，但相比更受人关注的spatial convolution，我打心底还是很欣赏spectral convolution这一系列工作的</li><li>对我个人来讲最最重要的工作，是在年底的ICLR 2018上，其中很多oral文章的质量让我佩服不已不已，其中最重要的两篇分别是<a href="https://arxiv.org/abs/1801.02613" target="_blank" rel="noopener">Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality</a>和Stanford大佬Sinha的<a href="https://arxiv.org/abs/1710.10571v4" target="_blank" rel="noopener">Certifying Some Distributional Robustness with Principled Adversarial Training</a>，这两篇文章完全改变了我对对抗样本领域heuristic堆trick没有theoretical backup的印象，从此将我带进对抗样本的大坑中</li></ul><p><img src="./YanLecun/7.jpg" width="500px"><br><img src="./YanLecun/8.jpg" width="500px"><br><img src="./YanLecun/9.jpg" width="500px"><br><img src="./YanLecun/10.jpg" width="500px"></p><hr><p>上面的很多研究，虽然一一道来有种煮酒论英雄的舒爽，但我也有点意识到，2017年已经算是蛮遥远的曾经了。</p><p>我也从2016年初一路繁忙至今，鲜有时间回顾下四围的光景。</p><p>想想2018年年底出现在各大会议上的工作自己基本都没怎么follow，即使只说自己领域内也顿然想不到有什么新鲜物事，至关重要的NIPS 2018和ICLR 2019也至今没来得及刷一遍。</p><p>早就搞不清楚是自己在有充分计划与信心地追求某个目标，还是在被巨大的车轮迫使着向前。</p><p>开始可以看到自己的upper bound，那里有无论如何努力都无法跨越的隔阂。</p><p>越发感觉到时间对于自己是越发沉重的成本，每次在岔路口的选择都需要百十倍的小心翼翼。</p><p>觉察到造物者开始回收我年轻永远用不完的体力，就像修复一个本不该存在的bug，每一次熬夜都会付出比从前大得多的成本。</p><hr><p>大概不只有我一个人，会在晚上睡觉之前，脑子里浮光掠影地闪过自己人生的其他可能性吧。</p><p>如果我初中学萨克斯坚持下来了，真的走上了音乐的路，会怎么样？</p><p>如果本科真的动了心，放弃考研出去玩乐队，会遇到好的队友吗？能做出漂亮的音乐吗？</p><p>如果我高考没有来到上海，而是去了我报志愿最多的山东，会遇到什么人？遇到什么事？</p><p>如果考研时没有将选择限定在华东地区范围以内，而是选择了北京的学校，人生会如何发展？如果当初怂了，选择直接读USST的光学研究生，今天的我是不是会后悔到死？反之，如果最终报学校的时候脑子一发热，直接报了上海交大，最后就一定会落榜吗？现在又会如何？</p><p>很神奇的一点是，每每想到其他种种可能性，想到那些不一样的自己交错在不同的平行宇宙的情形，即使知道那些路远比自己想象中更加凶险，但总会不由自主地往好的方面想，以至有时真的会半夜做梦笑出来。</p><p>可能我只是向往，但并非真的羡慕那些平行宇宙中的自己。</p><p>可以想象到未来充满荆棘与极大的不确定性，但至少，未来不会很无聊。</p><script type="math/tex; mode=display">\mathcal{F}in.</script>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Diaries Written during AAAI-2019</title>
      <link href="/hello-world/aaai-2019-japan.html"/>
      <url>/hello-world/aaai-2019-japan.html</url>
      
        <content type="html"><![CDATA[<h2 id="Jan-26th-2019"><a href="#Jan-26th-2019" class="headerlink" title="Jan 26th, 2019"></a>Jan 26th, 2019</h2><a id="more"></a><p><img src="/2019/02/10/aaai-2019-japan/Narita.jpg" width="100%" style="vertical-align:middle"></p><h2 id="Jan-27th-2019-in-Tokyo"><a href="#Jan-27th-2019-in-Tokyo" class="headerlink" title="Jan 27th, 2019, in Tokyo"></a>Jan 27th, 2019, in Tokyo</h2><p><img src="/2019/02/10/aaai-2019-japan/Kinsei.jpg" width="100%" style="vertical-align:middle"></p><p><img src="/2019/02/10/aaai-2019-japan/Tokyo.jpg" width="100%" style="vertical-align:middle"></p><h2 id="Jan-27th-2019-in-Oahu-island"><a href="#Jan-27th-2019-in-Oahu-island" class="headerlink" title="Jan 27th, 2019, in Oahu island"></a>Jan 27th, 2019, in Oahu island</h2><!-- more --><p><img src="/2019/02/10/aaai-2019-japan/Hawaii_1.jpg" width="100%" style="vertical-align:middle"></p><h2 id="Jan-28th-2019"><a href="#Jan-28th-2019" class="headerlink" title="Jan 28th, 2019"></a>Jan 28th, 2019</h2><ul><li>Pearl harbor.</li><li>Opening reception.</li><li>Hanging out with Yang and Steve at night, having some Hawaiian (Japanese) spam.</li></ul><h2 id="Jan-29th-2019"><a href="#Jan-29th-2019" class="headerlink" title="Jan 29th, 2019"></a>Jan 29th, 2019</h2><ul><li>The first day of the conference.</li><li>Walking on the Waikiki beach.</li></ul><h2 id="Jan-30th-2019"><a href="#Jan-30th-2019" class="headerlink" title="Jan 30th, 2019"></a>Jan 30th, 2019</h2><ul><li>The day for my oral presentation.</li><li>Invited talk given by Ian Goodfellow.</li><li>Hawaiian food for lunch.</li></ul><h2 id="Jan-31th-2019"><a href="#Jan-31th-2019" class="headerlink" title="Jan 31th, 2019"></a>Jan 31th, 2019</h2><ul><li>Kualoa ranch.</li><li>Swimming pool in Illikai.</li></ul><h2 id="Feb-1st-2019"><a href="#Feb-1st-2019" class="headerlink" title="Feb 1st, 2019"></a>Feb 1st, 2019</h2><ul><li>Self-driving to the east coast of Oahu. </li><li>Hanauma bay snorkling.</li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Hello Worlds</title>
      <link href="/hello-world/index.html"/>
      <url>/hello-world/index.html</url>
      
        <content type="html"><![CDATA[<p><img src="http://yorimoi.com/images/top-logo.png" alt=""></p><ul><li><a href="./YanLecun.html">Yann Lecun上海交大老照片数张</a></li><li><a href="./aaai-2019-japan.html">Diaries Written in AAAI-2019</a></li><li><a href="./yugoslavia.html">Itinerary to Yugoslavia</a></li><li><a href="https://www.douban.com/note/716090012/" target="_blank" rel="noopener">Douban diary written by Schoko</a></li><li><a href="./philipines.html">Grand blue in the south</a></li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Grand blue in the south</title>
      <link href="/hello-world/philipines.html"/>
      <url>/hello-world/philipines.html</url>
      
        <content type="html"><![CDATA[<p>Grand blue in the south</p><meta name="referrer" content="never"><a id="more"></a><h1 id="Day-1-Oct-20th"><a href="#Day-1-Oct-20th" class="headerlink" title="Day 1: Oct 20th"></a>Day 1: Oct 20th</h1><p>到达菲律宾是当地清晨，坐了一夜红眼航班，看着东边的天从蒙蒙亮的灰色变成清爽的蓝色，2019年已经是第二次来到一个热带小岛，这次的目的地是宿务&amp;薄荷岛</p><p><img src="https://img9.doubanio.com/view/photo/l/public/p2575642196.webp" alt=""></p><p>一离开机场，马上热浪袭来，一群小伙伴冲进卫生间换半袖短裤</p><p><img src="https://img1.doubanio.com/view/photo/l/public/p2575642188.webp" alt="airport"></p><p>这次菲律宾之行是某人心血来潮的结果，在我还不知情的情况下，某人飞快地买完了机票，然后微信我道：去宿务吧</p><p>我：宿务是哪？？？ #黑人问号</p><p><img src="https://cn.bing.com/th?id=OIP.F_0E3p93sRVRsa1RTHooBwHaHa&pid=Api&rs=1" width="45%"></p><p>一番搜索，发现宿务并不算是个大众化的旅游目的地，直到2018年宿务才开通了机场，目的似乎是为了促进其旅游业发展，在那之前的更长一段时间里，宿务是一个对大众而言籍籍无名但在潜水圈里盛名在外的地方，很多潜水论坛里的人会推荐在这里考OW或AOW证</p><p>我之前并没有潜过水，听闻过潜水【蓝色鸦片】的大名，之前曾经很喜欢异步叫做grand blue的硬核潜（饮）水（酒）漫画，这一点还是让我很感兴趣的</p><p>出发，在机场门口联系小黑叫了一辆van前往码头，上了车就开始与司机师傅各种聊天问路</p><p>相比于美帝极其热情会给你讲black jokes的出租车司机，或者是相比黑山共和国20多年车龄的大佬话唠车神司机，菲律宾的大部分司机并没有很强的沟通愿望，部分原因在于，大部分菲律宾普通人的英语水平仅能达到基本日常沟通的程度，所以基本上你只能用一些简单的单词来问，司机通常也会用最简单的词汇回答</p><h2 id="Tubigon-port"><a href="#Tubigon-port" class="headerlink" title="Tubigon port"></a>Tubigon port</h2><p>不得不提，进入码头前的那条马路上贫民区光景触目惊心，用废纸板搭成的低矮棚户占满了马路边各个角落，时而有骨瘦如柴的小孩从废纸板搭成的小棚子里向外窥视，没敢拍照，快速离开；到达码头后人流也是非常混乱，很多小黑抢着给我们带路，然后买完票就伸手要tips（一开始就应该假装听不懂英语来着</p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575641935.webp" alt=""><br><img src="https://img1.doubanio.com/view/photo/l/public/p2575641927.webp" alt=""></p><p>Tubigon port是以当地人为主的码头，风景不错，天气炎热。我们在这里包了一辆van，从Tubigon port前往我们的住所Tagbilaran，砍价之后3500 Peso成交，其中1000 Peso是加油的费用，考虑路程长度应该说还蛮划算的，途经景点包括以下几个地方：</p><ul><li>Cholocate hills</li><li>眼镜猴动物园（Tarsier</li><li>一个纯粹用来拍照的吊桥</li><li>Man-made forest（一条树比较多的林荫小道</li><li>Python养殖（并没有进去</li><li>不晓得啥名字的Cathedral church（据说是菲律宾最古老的教堂</li></ul><p>大致行程如图<br><img src="https://img9.doubanio.com/view/photo/l/public/p2575641956.webp" alt=""></p><h2 id="Chocolate-hills"><a href="#Chocolate-hills" class="headerlink" title="Chocolate hills"></a>Chocolate hills</h2><p>巧克力山是Harry Potter电影的取景地之一，山丘隆起形如一片片巧克力</p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575642143.webp" alt=""><br><img src="https://img9.doubanio.com/view/photo/l/public/p2575642134.webp" alt=""></p><p>路上看到到处都是隆起的小山包，问司机大叔：Are we heading towards the chocolate hill now?</p><p>司机大叔：Chocolate hill<strong>S</strong></p><p>emmmmm，所以巧克力山指的不是某一座山，而是这样一堆巧克力状的小山包的集合，按照观景台的介绍，这里的地貌是由于特殊的地质原因形成的——以前这里曾经是大海，然后经过balabala复杂的地质运动balabala</p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575642115.webp" alt=""><br><img src="https://img1.doubanio.com/view/photo/l/public/p2575642128.webp" alt=""></p><p>在一群日本人的围观中完成了这组照片的拍摄</p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575642123.webp" alt=""></p><p>我们到Chocolate hills的时候差不多正是中午，热带的日光直射之下我们没过一会就想回车里吹空调了，看到山下居然还有人玩ATV，我只想知道他们的散热系统是啥牌子的？我们不一样？</p><h2 id="Fruit-market"><a href="#Fruit-market" class="headerlink" title="Fruit market"></a>Fruit market</h2><p>路上有人突然提议去当地水果市场买点水果，司机大叔爽快地给我们找了一个<strong>真</strong>local的菜市场</p><p>local到什么程度？我们钻进菜市场时迎接我们的是菜市场里全体当地村民的注视，他们面面相觑，带有墙裂新鲜感的目光仿佛在说：OMG u see those Chinese? What the hell are they doing here?</p><p><img src="https://img1.doubanio.com/view/photo/l/public/p2575642037.webp" alt=""></p><p>最后买了一小把香蕉，照片里看来卖相不怎么好看，没想到这里的香蕉味道和我们在国内日常可以吃到的香蕉非常不一样，果肉很有弹性，味道酸酸甜甜略带酸奶的质感，意外地很赞</p><h2 id="Tarsier"><a href="#Tarsier" class="headerlink" title="Tarsier"></a>Tarsier</h2><p>眼镜猴，号称菲律宾的国宝级小动物，刚进去的时候问那边的工作人员：How do you call that tiny little monkey with very special eyes?（捂脸）</p><p>工作人员：Tarsier</p><p>我：大耳希尔（模仿中。。。）</p><p>工作人员：Tarrrrrsier（用重音强调了第一个r的小舌音）</p><p>我：。。。</p><p>这样的对话已经是本次行程中第二次发生了，第一次在机场问路怎么去Tagbilaran，工作人员回复我时字正腔圆地念了一遍这个城市名：Tagbila~~~rrrrrrrran</p><p><img src="https://img1.doubanio.com/view/photo/l/public/p2575641949.webp" alt=""></p><p>门口一个男的里面一个女的，不敢妄谈是不是开车现场，可能和当地某种宗教信仰相关</p><p>说是眼镜猴动物园，实际上是一个包括了各种菲律宾当地特有动物的动物园。园内人不多，每个笼子附近都有工作人员负责讲解，讲解员对各种当地动物的介绍中包含了几个共同点</p><ul><li>危险（有毒/带电/攻击性强）</li><li>成年以后会长到比人还大</li></ul><p>主角tarsier在最里面，工作人员示意轻声进去轻声拍照，最终终于见到了主角</p><p><img src="https://img1.doubanio.com/view/photo/l/public/p2575641939.webp" alt=""></p><p>有人觉得长的很恶心，有人觉得很可爱，见仁见智吧，我觉得挺可爱的</p><h2 id="Church"><a href="#Church" class="headerlink" title="Church"></a>Church</h2><p>人造森林、吊桥比较无聊不表，Python没人敢下去看，司机大叔听了很惊诧：You should see this! This is very interesting!</p><p>Church的位置在Bohol临近Panglao的地方，已经可以看到一望无垠碧波万顷的菲律宾菲律宾海</p><p><img src="https://img1.doubanio.com/view/photo/l/public/p2575642107.webp" alt=""><br><img src="https://img3.doubanio.com/view/photo/l/public/p2575642100.webp" alt=""></p><p>有点意外，全菲律宾最古老的教堂并没有被作为文物保存起来，而是现在还在使用，教堂里牧师正在讲经，几乎没人发现我们几个不速之客的闯入，楼顶候鸟振翅而飞，无论你是否信这种教义，这里都是个能静下心来的好地方</p><h2 id="The-night-in-Panglao"><a href="#The-night-in-Panglao" class="headerlink" title="The night in Panglao"></a>The night in Panglao</h2><p>下午回到酒店后就没什么安排了，菲律宾作为一个热带国家，正午时分的太阳极其毒辣，一般来说如果要出去玩，比较好的时间安排是早上早早起床出门浪，中午之前回酒店歇着，等下午或者晚上太阳不那么毒辣之后再出门</p><p><img src="https://img1.doubanio.com/view/photo/l/public/p2575641957.webp" alt=""></p><p>酒店的泳池里一群台湾人在打水球，一群韩国人拿着gopro在水里各种拍，池水清爽，游完泳之后躺在泳池边的躺椅上，发现这里的星空还是很干净明亮的，不同星座层次分明。大城市里的星空是奢侈品，印象里上一次看到类似的星空已经是很久很久以前的事了，可惜游泳没带手机没有拍照</p><h1 id="Day-2-Oct-21st"><a href="#Day-2-Oct-21st" class="headerlink" title="Day 2: Oct 21st"></a>Day 2: Oct 21st</h1><h2 id="The-way-to-Balicasag"><a href="#The-way-to-Balicasag" class="headerlink" title="The way to Balicasag"></a>The way to Balicasag</h2><p>这一天的主要行程是出海跳岛，目的地是一个叫做Balicasag的小岛，我们四个人包了一艘螃蟹船，内容主要包括</p><ul><li>看海豚（必须早上很早才能看到，需要碰运气</li><li>Balicasag大断层深潜（非常有名，可以说是本次行程中最有名的一个点</li><li>Balicasag浮潜，看海龟（也很有名，国人望文生义薄荷岛Bohol island的近海是薄荷的颜色，这种说法盖源于此</li><li>Church island or Virgin island二选一，前者岛上是一个教堂，后者是一个海上的海鲜市集，我们选择了church island</li></ul><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575641955.webp" alt=""></p><p>还是讲讲我们联系小黑包船的经过。我们住在靠近Alona beach的市镇上，镇中心的麦当劳附近到处都是小黑，随便找一个谈价格即可，一般人多一些会比较好砍价，包船会比和别人拼一艘船要贵一些；我们遇到的小黑会极力推销Balicasag的深潜和沙丁鱼风暴两个项目，大概是因为这两项他的提成比较多，当他极力想推荐你让你接受某个项目时，一般会说</p><blockquote><p>This one is beautiful you <strong>HAVE TO</strong> go! I give you a good price my friend（掏出手机打开计算器，按一个价格）. I give you X千块钱一个人（突然开始用浓重的口音讲中文）. You go 浮潜, you go 深潜 or 沙丁鱼. This is a good price you <strong>HAVE TO</strong> go.</p></blockquote><p>我们谈价格的过程非常曲折困难，小黑极力想让我们在大断层深潜，我们嫌大断层深潜太贵尝试砍价，双方都不肯让步磨了很长时间，最后我都快要放弃砍价时，似乎小黑丧失了继续谈的耐心，打电话给他老板之后，以折合人民币200多一个人的价格成交（低价的代价，潜水时没有脚蹼）</p><p>出发时间是早上五点，清晨的Alona beach海风和阳光都很舒服，大量的螃蟹船停靠在岸附近</p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575642020.webp" alt=""></p><p>螃蟹船上开始各种拍照<br><img src="https://img3.doubanio.com/view/photo/l/public/p2575642171.webp" alt=""><br><img src="https://img9.doubanio.com/view/photo/l/public/p2575642174.webp" alt=""><br><img src="https://img9.doubanio.com/view/photo/l/public/p2575642164.webp" alt=""></p><p>自拍若干<br><img src="https://img9.doubanio.com/view/photo/l/public/p2575642004.webp" alt=""><br><img src="https://img9.doubanio.com/view/photo/l/public/p2575641986.webp" alt=""><br><img src="https://img3.doubanio.com/view/photo/l/public/p2575641981.webp" alt=""></p><h2 id="Diving-experience"><a href="#Diving-experience" class="headerlink" title="Diving experience"></a>Diving experience</h2><p>终于到了最期待的diving！</p><hr><blockquote><p>所谓体验潜水，就是一种给没有潜水证的人体验潜水的方式，由教练带着下水并保证潜水者的安全；在岸上时，教练会简单讲解有关潜水的一些最基本的知识，这部分知识性命攸关，教练会给一份中文材料（简体中文）确保你完全理解了上面的意思</p><ul><li>regulator，呼吸用的，背着气瓶呼吸会比正常呼吸费力一些</li><li>regulator进水如何排水，直接按一下regulator背面就行</li><li>耳压平衡：在水下每下潜一段水压都会升高，如果硬下的话耳膜会很痛，甚至有鼓膜损伤的风险，必须要平衡完耳压向教练确认OK才能继续下潜，方法就是微微捏住鼻子，然后使劲用鼻子吹气，吹到耳朵里有吱吱的声音就对了</li><li>面镜排水：个人经验感觉抬头看东西时面镜容易进水，方法也很简单就是按住面镜上侧，然后用鼻子使劲呼气，基本上一次就能把水完全排掉；考OW时会要求把面镜完全摘掉，然后再重新戴上面镜做排水，貌似是很多人的噩梦，眼睛接触到盐度很高的海水会有点扎扎的感觉</li><li>各种潜水手势：为了在水下交流用的，体验潜水基本上只需要会OK和problem两个就够了，对于新手来说最大的问题就是耳压平衡，比OK教练就会往更深的地方走，比problem教练就会带着上浮</li></ul></blockquote><hr><p>下水前的各种准备，Balicasag是岸潜，貌似菲律宾没有无证的体验船潜？</p><p><img src="https://img1.doubanio.com/view/photo/l/public/p2575642079.webp" alt=""><br><img src="https://img9.doubanio.com/view/photo/l/public/p2575642064.webp" alt=""></p><p>这张图里已经能从海水的颜色上明显地看出大断层的位置，所谓大断层其实就是海里的悬崖，网上盗一张Balicasag的图感受一下</p><p><img src="https://cn.bing.com/th?id=OIP.OCF5NAc29rixzaLKGTIDsAHaE8&pid=Api&rs=1"></p><p>其实大多数海岛都会有这样的断层，但Balicasag很棒的一点在于大断层离岸非常近，落差几百米深不见底，断崖上珊瑚茂密鱼类丰富</p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575642052.webp" alt=""></p><p>遗憾的是这次并没有带诸如gopro一类能在水下拍照的设备，所以潜水全程没有拍照记录。即使如此，还是要说，第一次潜水的体验太太太太太震撼了～～绝对可以称得上是毕生难忘</p><p>应该如何形容呢？海里的风景美到不想上浮，教练拉着我上浮的时候感觉好像只过了五分钟；身体被海水包裹的感觉奇妙到难以用语言形容；内心激动不已的同时，又感觉世界上的一切都极其安静，只能听到自己的呼吸声</p><hr><p>负责我的教练讲完科普完理论与安全常识之后之后，带着我走到上图的断层边缘处。此时，饶是我会游泳且以前一直笃信自己没有深海恐惧症，真的站在大断层前时，看着面前深不见底的万丈深渊还是有点害怕的：卧槽我这万一脚一滑腰上还带着配重马上就沉下去了捞都捞不回来啊，此处应有【害怕.jpg】</p><p>我背上了气瓶之类的装备，教练问道：have a try here! Can you breath with your regulator?</p><p>我戴上面镜，咬着regulator钻进水里：卧槽水下和水上是两个世界啊！卧槽珊瑚好漂亮还有好多鱼！卧槽前面好深！诶我可以呼吸！</p><p>从水里站起来，教练问are you okay? 回曰OK。我心里想着潜水真有意思，接下来大概教练会让我在这里把刚才教的regulator排水面镜排水啥的都模拟一遍吧。。。</p><p>万万没有想到，教练听到我说OK，二话不说直接拉着我下水，在我还没反应过来之前，教练脚蹼轻轻一蹬，我感觉到一股力量带着我向前，身体开始不受控制地向大断层下的深渊下沉。。。</p><p>内心OS：NO! I WAS NOT OKAY!</p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575672555.webp" width="30%"></p><p>一抬头发现水面已经离我大约两三米远了，在水里用氧气瓶呼吸要比正常呼吸费力一些，我开始用力地大口呼吸，人生中从未感觉呼吸如此困难过，在感觉自己要完蛋的时候，突然又感觉身体缓过来了一些，我开始有意识地深呼吸来调整：嗯！？？好像可以！？？我还活着！？？</p><p>教练打手势问我是否OK，我做了肯定回复，教练开始带着我下到更深处的位置，耳朵开始感觉到水压的变化，试了下耳压平衡：emmmmm!?? it works!!!</p><p>接下来就是看风景，下沉，耳压平衡，循环往复；悬崖上的珊瑚五颜六色非常漂亮，这时我意识到一件事情，如果你在网上看到类似这样的潜水照</p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575674442.webp"></p><p>实际上珊瑚的颜色是要比照片中鲜艳很多的，只是阳光中除蓝色以外的大部分波段都被海水过滤掉了，所以照片整体颜色偏蓝，如果自己亲身体验的话，看到的风景远远要比照片更美</p><p>最后教练带着我缓缓降落到悬崖上一个类似洞穴一样的地方，这个地方也很奇妙，头顶是各种珊瑚海葵，脚下是一片松软的白沙滩，无数小鱼对我们几个人类熟视无睹从身边穿过，向悬崖下看还能看到比较深的位置有很多蓝色的会发光的鱼，类似这种感觉</p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575674552.webp" alt=""></p><p>教练示意我这里已经十二米深了不能再下潜，我才注意到这里的光线要比上面暗很多，水温也要低一些；虽然字面意义上我是【站】在白沙滩上的，但其实在水里感觉不到任何重力，当我吸气的时候身体就微微上浮，反之呼气的时候身体就微微下沉（游泳的时候也干过类似的事情）</p><p>这时几个小伙伴和他们的教练也潜到了洞穴附近，教练开始教我们如何打水环，在水里的视觉效果非常漂亮，可惜我试了很多次也没打出来hhh</p><p>五米三分钟停留时，靠近我们的悬崖上有一个海葵里面有小丑鱼，这是我第一次在海里见到小丑鱼！真的和海底总动员的Nemo一毛一样！教练让我把手放在海葵旁边（千万别碰），里面的小丑鱼从海葵中探出头来，以极快的速度在我的手上啄了一下，然后又缩回海葵中，简直是太可爱了～～</p><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575674553.webp" alt="Nemo"></p><p>结果在我得意忘形的时候没注意到离悬崖太近，脚旁边有个珊瑚被蛰了一下，没有伤口，挺疼</p><h2 id="Church-island"><a href="#Church-island" class="headerlink" title="Church island"></a>Church island</h2><p>Church island去得人很少，这个岛的面积很小，非常适合拿无人机航拍，因为整个岛在海里的形状就是一个逗号，逗号的长度取决于涨潮or落潮。可以沿着逗号的尾巴走到逗号的尾端，这时周围270度都是海，是一番蛮别样的体验</p><p>小岛上有一个耶稣像，动作神态与里约热内卢的那个完全相同，可以看做是缩小版的巴西耶稣像。耶稣像后面是一个小院子，院子里很多果树，沙地只见一条小道，颇有些曲径通幽的感觉，很多鸽子在沙地上觅食。进到院子里以后，这里的一个工作人员给了我一把饲料，示意我摊开手，几乎是我这样做的一瞬间，几只鸽子就已经飞到我的胳膊上开始啄食</p><p><img src="https://img1.doubanio.com/view/photo/l/public/p2575641977.webp" alt=""></p><p>搞笑的是某人也尝试这样喂食时，居然没有鸽子飞上来wwww，连鸽子的主人都笑了</p><blockquote><p>It happens sometimes. They like friendly people. The girl is not friendly. Bad girl.<br>The boy is friendly. Good boy and bad girl.<br>（笑完以后） I’m just joking.</p></blockquote><p><img src="https://img3.doubanio.com/view/photo/l/public/p2575641963.webp" alt=""></p><p>当然，最后在工作人员的指导下，unfriendly girl也成功地让鸽子飞到了她的手里</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Itinerary to East Europe</title>
      <link href="/hello-world/yugoslavia.html"/>
      <url>/hello-world/yugoslavia.html</url>
      
        <content type="html"><![CDATA[<h2 id="Schedule"><a href="#Schedule" class="headerlink" title="Schedule"></a>Schedule</h2><a id="more"></a><div class="table-container"><table><thead><tr><th>Date</th><th style="text-align:center">Time</th><th style="text-align:center">Activities</th><th style="text-align:center">Accommodation</th><th style="text-align:center">Remarks</th><th style="text-align:center">Map</th></tr></thead><tbody><tr><td>April 7th - 8th</td><td style="text-align:center">23:00 - 5:20</td><td style="text-align:center">TK071 HK-&gt;İstanbul</td><td style="text-align:center"></td><td style="text-align:center">出发：香港T1</td><td style="text-align:center"></td></tr><tr><td>April 8th</td><td style="text-align:center">7:40 - 8:20</td><td style="text-align:center">TK1081 İstanbul-&gt;Belgrade</td><td style="text-align:center"></td><td style="text-align:center">1h 45min</td><td style="text-align:center"></td></tr><tr><td>April 8th</td><td style="text-align:center">全天</td><td style="text-align:center">Topčider Park -&gt; Sveti Sava Temple -&gt; 共和国广场 -&gt; Belgrade Fortress -&gt; Great War Island</td><td style="text-align:center"><a href="https://zh.airbnb.com/rooms/18431092" target="_blank" rel="noopener">Terazije Central Apartment</a>, Belgrade</td><td style="text-align:center">第一天可能会由于时差比较累</td><td style="text-align:center"><a href="https://www.google.com/maps/dir/%E8%B4%9D%E5%B0%94%E6%A0%BC%E8%8E%B1%E5%BE%B7%E5%B0%BC%E5%8F%A4%E6%8B%89%C2%B7%E7%89%B9%E6%96%AF%E6%8B%89%E6%9C%BA%E5%9C%BA+Aerodrom+Beograd+59,+Beograd+11180%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/%D0%A1%D0%B2%D0%B5%D1%82%D0%BE%D0%B3+%D0%A1%D0%B0%D0%B2%D0%B5,+Beograd,+%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A%E8%B4%9D%E5%B0%94%E6%A0%BC%E8%8E%B1%E5%BE%B7+Trg+republike,+%E5%85%B1%E5%92%8C%E5%9B%BD%E5%B9%BF%E5%9C%BA/Belgrade+Fortress,+%E8%B4%9D%E5%B0%94%E6%A0%BC%E8%8E%B1%E5%BE%B7%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/Great+War+Island,+%E8%B4%9D%E5%B0%94%E6%A0%BC%E8%8E%B1%E5%BE%B7%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/@44.8197693,20.3673847,13z/data=!4m32!4m31!1m5!1m1!1s0x475a688a5214cf59:0xc3184c4bc7f300f9!2m2!1d20.291691!2d44.820498!1m5!1m1!1s0x475a700bb3b702cf:0x8c4fe8311c3f97e5!2m2!1d20.46787!2d44.8004214!1m5!1m1!1s0x475a7ab2453c4ba1:0xb81037438bba0036!2m2!1d20.4603199!2d44.8162551!1m5!1m1!1s0x475a6549015c2c53:0x160b799f3d48d2b2!2m2!1d20.450903!2d44.822633!1m5!1m1!1s0x475a65144b7d88b7:0x3e2055c8a0b8499e!2m2!1d20.4325128!2d44.8304371!3e0" target="_blank" rel="noopener">Google Map View</a></td></tr><tr><td>April 9th</td><td style="text-align:center">全天</td><td style="text-align:center">Belgrade -&gt; Krusedol monastery -&gt; Novi Sad -&gt; Subotica -&gt; Novi Sad</td><td style="text-align:center"><a href="https://www.google.com/maps/place/%C4%90ure+Jak%C5%A1i%C4%87a+15,+Novi+Sad,+%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/@45.2582485,19.8453618,17z/data=!4m5!3m4!1s0x475b1067f0d8d81b:0x9edd190951d170dc!8m2!3d45.2582411!4d19.8447001" target="_blank" rel="noopener">Đure Jakšića 15, Novi Sad</a></td><td style="text-align:center">租车, 4h 31min or more</td><td style="text-align:center"><a href="https://www.google.com/maps/dir/Belgrade+Nikola+Tesla+Airport,+%E8%B4%9D%E5%B0%94%E6%A0%BC%E8%8E%B1%E5%BE%B7%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/Kru%C5%A1edol+Monastery,+313,+%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/Sremski+Karlovci,+%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/Petrovaradin+Fortress,+Beogradska,+%E5%BD%BC%E5%BE%97%E7%BD%97%E7%93%A6%E6%8B%89%E4%B8%81%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/%E8%AF%BA%E5%A8%81%E8%90%A8%E4%B8%BB%E6%95%99%E5%BA%A7%E5%A0%82/Synagoge,+Trg+Jakaba+i+Komora,+%E8%8B%8F%E5%8D%9A%E8%92%82%E5%AF%9F%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/@45.500022,20.3031191,8.42z/data=!4m38!4m37!1m5!1m1!1s0x475a688a5214cf59:0xc3184c4bc7f300f9!2m2!1d20.291691!2d44.820498!1m5!1m1!1s0x475b01766c65b369:0xc682914f77cca9c7!2m2!1d19.9400407!2d45.1194434!1m5!1m1!1s0x475b04a43adf8bdd:0x9cf011f0e8a1b3ae!2m2!1d19.9364586!2d45.2037357!1m5!1m1!1s0x475b1077bfacf56b:0xa7ff4e0e190a1b33!2m2!1d19.8633667!2d45.252901!1m5!1m1!1s0x475b106858fe513d:0xb816502dc083f616!2m2!1d19.8457101!2d45.2556759!1m5!1m1!1s0x474366c60a83872f:0x84818b887e7d51c3!2m2!1d19.6614422!2d46.1014059!3e0" target="_blank" rel="noopener">Google Map View</a></td></tr><tr><td>April 10th</td><td style="text-align:center">全天</td><td style="text-align:center">Novi Sad -&gt; Tara National Park (Perućac)</td><td style="text-align:center"><a href="https://www.google.com/maps/place/Bungalows+Viola+and+Detelina/@43.95182,19.4217149,13.75z/data=!4m5!3m4!1s0x4759b2f68c58d965:0xf586511ce8287c56!8m2!3d43.9570806!4d19.424817" target="_blank" rel="noopener">Bungalow Detelina </a>, Perućac</td><td style="text-align:center">4h 51min or more</td><td style="text-align:center"><a href="https://www.google.com/maps/dir/Novi+Sad,+%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/Peru%C4%87ac,+%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/Tara+national+park,+Mokra+Gora,+%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/@43.8783054,19.5441863,11.75z/data=!4m20!4m19!1m5!1m1!1s0x475b10613de93455:0xb6f7d683724fe28!2m2!1d19.8335496!2d45.2671352!1m5!1m1!1s0x4759b3a9e1709a6f:0x13125de029752c13!2m2!1d19.4316836!2d43.9573692!1m5!1m1!1s0x47584bdafcc3a383:0xa28ba6bb1c340a84!2m2!1d19.4581188!2d43.8483396!3e0" target="_blank" rel="noopener">Google Map View</a></td></tr><tr><td>April 11th</td><td style="text-align:center">全天</td><td style="text-align:center">Perucac -&gt; Mecavnik -&gt; Uzice -&gt; 查查克 -&gt; Belgrade</td><td style="text-align:center"><a href="https://www.google.com/maps/search/?api=1&amp;query=Pariska%201%2C%20%E8%B4%9D%E5%B0%94%E6%A0%BC%E8%8E%B1%E5%BE%B7%2C%2011271%2C%20%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A" target="_blank" rel="noopener">Belgrade Airport neighborhood</a></td><td style="text-align:center">4h 49min</td><td style="text-align:center"><a href="https://www.google.com/maps/dir/Peru%C4%87ac,+%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/Drvengrad+-+Mecavnik,+Mokra+Gora,+%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A%E4%B9%8C%E6%97%A5%E7%AD%96/%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A%E6%9F%A5%E6%9F%A5%E5%85%8B/Belgrade,+%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/@44.2048115,20.2216701,11.37z/data=!4m32!4m31!1m5!1m1!1s0x4759b3a9e1709a6f:0x13125de029752c13!2m2!1d19.4316836!2d43.9573692!1m5!1m1!1s0x47583632ed05ba15:0xdd78e2331a9d3100!2m2!1d19.5074369!2d43.7959892!1m5!1m1!1s0x4759d34266ef8b59:0x58e8ff863b5b6aab!2m2!1d19.842471!2d43.8555729!1m5!1m1!1s0x4757723fdaf79e2d:0x46460a390c51b8ce!2m2!1d20.3501652!2d43.8914144!1m5!1m1!1s0x475a7aa3d7b53fbd:0x1db8645cf2177ee4!2m2!1d20.4489216!2d44.786568!3e0" target="_blank" rel="noopener">Google Map View</a></td></tr><tr><td>April 12th</td><td style="text-align:center">13:35 - 14:25</td><td style="text-align:center">Flight JU172: Belgrade -&gt; Podgorica. Car: Podgorica -&gt; Kotor</td><td style="text-align:center">Kotor</td><td style="text-align:center">1h 33min from Podgorica to Kotor</td><td style="text-align:center"><a href="https://www.google.com/maps/dir/Podgorica+Airport+Passenger+Terminal,+Golubovci,+%E9%BB%91%E5%B1%B1/Kotor,+%E9%BB%91%E5%B1%B1/@42.4448683,18.6503703,9.08z/data=!4m14!4m13!1m5!1m1!1s0x134de97ac7d3e81b:0xca9b66f8b356630c!2m2!1d19.2465409!2d42.3676005!1m5!1m1!1s0x134c33063d70c91b:0x7a73f15e212e9306!2m2!1d18.771234!2d42.424662!3e0" target="_blank" rel="noopener">Google Map View</a></td></tr><tr><td>April 13th-14th</td><td style="text-align:center">全天</td><td style="text-align:center">因黑山面积较小，可以随意安排，景点可参考<a href="#Sights">Sights in Montenegro</a></td><td style="text-align:center"><a href="https://www.booking.com/hotel/me/apartmani-zlatne-njive.en-gb.html?aid=1664454;sid=96e55f8590c1e329e381f2996c1c4fdc;all_sr_blocks=54002904_118560005_1_0_0;checkin=2019-04-12;checkout=2019-04-14;dest_id=-85411;dest_type=city;dist=0;group_adults=1;group_children=0;hapos=1;highlighted_blocks=54002904_118560005_1_0_0;hpos=1;no_rooms=1;req_adults=1;req_children=0;room1=A;sb_price_type=total;sr_order=popularity;srepoch=1553618640;srpvid=fe7075a747ca0257;type=total;ucfs=1&amp;" target="_blank" rel="noopener">Apartment Luka</a>, near <a href="https://www.google.com/maps/place/Apartments+Luka/@42.4309215,18.7667139,15.5z/data=!4m5!3m4!1s0x134c330f972bd087:0x23087a33abc7d156!8m2!3d42.429947!4d18.76992" target="_blank" rel="noopener">Kotor beach</a></td><td style="text-align:center">建议买14号中午的机票回贝城</td><td style="text-align:center"></td></tr><tr><td>April 14th</td><td style="text-align:center">15:20 - 16:05</td><td style="text-align:center">Flight Ju173: podgorica -&gt; Belgrade</td><td style="text-align:center">Belgrade</td><td style="text-align:center">只是推荐选项，也可以选其他的</td><td style="text-align:center"></td></tr><tr><td>April 15th</td><td style="text-align:center">白天时间</td><td style="text-align:center">Belgrade市内吃吃喝喝逛逛</td><td style="text-align:center"></td><td style="text-align:center">6:20动身去机场应该来得及</td><td style="text-align:center"></td></tr><tr><td>April 15th</td><td style="text-align:center">20:20 - 23:05</td><td style="text-align:center">TK1084 Belgrade-&gt;İstanbul</td><td style="text-align:center"></td><td style="text-align:center">出发：Nikola Tesla Airport (<em>aka</em> Belgrade Airport) T1</td><td style="text-align:center"></td></tr><tr><td>April 16th</td><td style="text-align:center">1:25 - 16:45</td><td style="text-align:center">TK026 İstanbul-&gt;Shanghai</td><td style="text-align:center"></td><td style="text-align:center">到达：浦东T2</td><td style="text-align:center"></td></tr></tbody></table></div><h2 id="Sights"><a href="#Sights" class="headerlink" title="Sights"></a>Sights</h2><h3 id="Novi-Sad-amp-Subotica"><a href="#Novi-Sad-amp-Subotica" class="headerlink" title="Novi Sad &amp; Subotica"></a>Novi Sad &amp; Subotica</h3><p><em>以下按照路过先后顺序组织</em></p><ul><li>Krusedol Manastery: 一座巴洛克式的中世纪修道院，<a href="https://s.weibo.com/weibo/Kru%25C5%25A1edol%2520Monastery?topnav=1&amp;wvr=6&amp;b=1" target="_blank" rel="noopener">来自微博网友推荐</a>，按照<a href="https://en.wikipedia.org/wiki/Kru%C5%A1edol_Monastery" target="_blank" rel="noopener">Wikipedia的介绍</a>：<blockquote><p>The Krušedol Monastery is a Serbian Orthodox monastery, the legacy of the last Serbian despot family of Syrmia - Branković. Dedicated to the Annunciation to the Blessed Virgin Mary, it has been described as the “spiritual beacon” of Fruška Gora and “Second Studenica”.</p></blockquote></li><li>Sremski Karlovci: 进入Novi Sad之前路过的最后一个小镇，盛产一种叫做Bermet的葡萄酒，有超过500年的历史，小镇中央巴洛克式喷泉Four Lions是地标建筑。根据<a href="https://en.wikipedia.org/wiki/Sremski_Karlovci" target="_blank" rel="noopener">Wikipedia的介绍</a>，这座小镇在古代曾长期是Habsburg Monarchy与奥匈帝国的精神、政治以及文化中心</li><li>The Peterovaradin fortress: 多瑙河东岸，过桥就是Novi Sad，著名打卡圣地，根据<a href="https://en.wikipedia.org/wiki/Petrovaradin_Fortress" target="_blank" rel="noopener">Wikipedia介绍</a>，前身可追溯到罗马帝国，于1235年匈牙利国王King Bela四世重修，并在16-17实际奥地利与土耳其的百年战争中数次被毁重修</li><li>Novi Sad主教堂 (Church of the Name of Mary): 打卡圣地之二，旁边就是Liberty Square，Dunavska Street以及Serbia National Theatre，网上大多Novi Sad的旅游照都来自于这附近</li><li>Palic Lake: 进Subotica前最后一站，度假胜地</li><li>Subotica的景点集中在市政厅(City Hall)附近，包括Synagogue, Modern Art Gallery与City Museum Subotica</li></ul><h3 id="Montenegro"><a href="#Montenegro" class="headerlink" title="Montenegro"></a>Montenegro</h3><p><em>黑山面积比较小，每个景点之间距离也很近，所以基本可以随便安排</em></p><ul><li>Podgorica：黑山首都</li><li>Skadar Lake National Park：从Podgorica到Kotor必经之地，与Albania接壤</li><li>Ulcinj：一个地中海风情的小镇，前南斯拉夫休假圣地，在<a href="https://www.lonelyplanet.com/montenegro" target="_blank" rel="noopener">Lonely Planet</a>和穷游网上的评分都比较高</li><li>Sveti Stefan：黑山打卡圣地</li><li>Njegoš Mausoleum：Njegos <del>(不认识这个人是谁)</del> 的陵墓，爬山看风景，据<a href="https://www.tripadvisor.cn/Attraction_Review-g304076-d552685-Reviews-Njegos_Mausoleum-Cetinje_Cetinje_Municipality.html" target="_blank" rel="noopener">猫途鹰吃瓜群众</a>说爬到山顶风景很好</li><li>Budva：又一个地中海风情小镇，距离Kotor很近</li><li>Kotor：可能是在这里住宿</li><li>Perast：从Kotor走路就可以到</li><li>The Kotor-Lovcen Road：从Kotor走路就能到，打卡圣地</li><li>Ostrog Monastery：又一个修道院，距离上面所有的景点都较远，不沿海，国内游客鲜有光顾，<a href="https://www.lonelyplanet.com/montenegro/central-montenegro/attractions/ostrog-monastery/a/poi-sig/1526215/360151" target="_blank" rel="noopener">Lonely Planet</a>上评价很高</li></ul><h2 id="Information"><a href="#Information" class="headerlink" title="Information"></a>Information</h2><h3 id="租车相关"><a href="#租车相关" class="headerlink" title="租车相关"></a>租车相关</h3><ul><li>租车可以提前在<a href="https://w.zuzuche.com/list.php?id=84278867&amp;driver_age=24" target="_blank" rel="noopener">租租车</a>上看，自动挡3-4人车型约200-300￥</li><li>Serbia油价约10￥/L，加油方式与美帝类似，信用卡自助</li><li>按照<a href="http://www.zuzuche.com/article/art-746-22423.html" target="_blank" rel="noopener">网上的说法</a>，租车时主驾驶员名下要有3000$的信用卡额度，也有1000欧元的说法，<strong>去塞尔维亚之前或需要联系招行临时提高信用额度</strong></li><li>Belgrade的旧城区基本大部分停车位需要发短信缴纳停车费（机场买的卡如果是流量卡的话没有短信电话功能）,划黄线的车位、画着轮椅的车位，以及车库门口不要停</li><li>Belgrade和Novi Sad市区有的报亭有卖停车票，一小时一张摆在车窗前即可，但有的报亭可能听不懂英语</li><li><a href="./yugoslavia/driving.pdf">Serbia交规.pdf</a></li><li>机场租车门店位于T2航站楼地下一层</li><li>关于保险，<a href="https://bbs.qyer.com/thread-2848098-1.html" target="_blank" rel="noopener">某大佬的文章中</a>建议购买全险，此外大佬还提醒Serbia有部分车行的部分车型有单日最大里程数限制，需要留意</li></ul><h3 id="生活相关"><a href="#生活相关" class="headerlink" title="生活相关"></a>生活相关</h3><ul><li>需要给小费，一般数额在10%左右</li><li>使用220V双圆孔插座，需要欧标转换头</li><li>出门基本没有公厕</li><li>网上普遍反映在当地市区/机场买电话卡比较好，几十块人民币，卡里没有任何套餐，需要在营业厅或书报亭充值</li><li>Belgrade附近网红餐厅CASA NOVA，需要提前邮件预约，人均80rmb，基本是塞尔维亚最贵的餐厅</li><li>若在黑山有上网要求，可以在当地买Telenor电话卡，有效期15天，10GB流量，价格9.95欧元</li></ul><h3 id="路线相关"><a href="#路线相关" class="headerlink" title="路线相关"></a>路线相关</h3><ul><li>网上很多人表示Google Map导航大致方向没什么问题，但在Serbia不够精确，且山区容易没信号，推荐下载<strong>探途地图</strong>作为补充，可以预先用离线地图功能缓存</li><li>有<a href="http://www.mafengwo.cn/i/8440858.html" target="_blank" rel="noopener">马蜂窝吃瓜群众</a>表示Tara公园制高点Vidikovac Banjska stena观景台可以看到Serbia和Bosnia Herzegovina边界的Perucac湖，风景应该不错，但路况较差</li><li>Tara national park附近除了<a href="https://club.autohome.com.cn/bbs/thread/45f1a39d4ba9abb4/67945365-1.html" target="_blank" rel="noopener">北京论坛吃瓜群众</a>说的Perućac可以住宿以外，还有一个叫做兹拉蒂博尔的小镇可以住宿</li><li>有住黑山的吃瓜群众表示黑山的飞机比较小，遇到恶劣天气容易取消</li><li><strong>Updated in March 26th:</strong> 按照<a href="#References">参考文献[3]</a>的说法，从乌日策回Belgrade有两种走法，其中一条路程较近，但完全是山路，路况也比较差，另一条路要绕远一些，但以高速为主，因此将4月11日的路线修正为Perucac -&gt; Mecavnik -&gt; 乌日策 -&gt; 查查克 -&gt; Belgrade</li></ul><h3 id="住宿相关"><a href="#住宿相关" class="headerlink" title="住宿相关"></a>住宿相关</h3><ul><li>April 8-9th, Terazije Central Apartment, Belgrade: <a href="https://www.google.com/maps/place/Terazije+Central+Apartment/@44.8102722,20.4639564,14.5z/data=!4m5!3m4!1s0x475a7b8517691449:0x287c960c06419ad5!8m2!3d44.8114533!4d20.4617163" target="_blank" rel="noopener">[Google Map]</a> <a href="./index/terazije.jpg">[further info]</a></li><li>April 9-10th, Đure Jakšića 15, Novi Sad: <a href="https://www.google.com/maps/place/%C4%90ure+Jak%C5%A1i%C4%87a+15,+Novi+Sad,+%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A/@45.2582485,19.8453618,17z/data=!4m5!3m4!1s0x475b1067f0d8d81b:0x9edd190951d170dc!8m2!3d45.2582411!4d19.8447001" target="_blank" rel="noopener">[Google map]</a></li><li>April 10-11th, Perucac Tara, Drina, Bungalow Detelina: <a href="https://www.google.com/maps/place/Bungalows+Viola+and+Detelina/@43.95182,19.4217149,13.75z/data=!4m5!3m4!1s0x4759b2f68c58d965:0xf586511ce8287c56!8m2!3d43.9570806!4d19.424817" target="_blank" rel="noopener">[Google Map]</a> <a href="https://www.airbnb.cn/trips/v1/e7a74a72-4016-4a35-9956-1b7a533acaeb/2019-04-10" target="_blank" rel="noopener">[Airbnb link]</a></li><li>April 12-13th, Apartments Luka, Kotor: <a href="https://www.google.com/maps/place/Apartments+Luka/@42.4309215,18.7667139,15.5z/data=!4m5!3m4!1s0x134c330f972bd087:0x23087a33abc7d156!8m2!3d42.429947!4d18.76992" target="_blank" rel="noopener">[Google Map]</a> <a href="https://www.booking.com/searchresults.zh-cn.html?aid=1664454;sid=96e55f8590c1e329e381f2996c1c4fdc;checkin=2019-04-12;checkout=2019-04-14;city=-85411;highlighted_hotels=540029;hlrd=with_av;keep_landing=1;redirected=1;source=hotel;srpvid=fe7075a747ca0257&amp;" target="_blank" rel="noopener">[Booking link]</a></li><li>April 14-15th, Belgrade Airport neighborhood: <a href="https://www.google.com/maps/search/?api=1&amp;query=Pariska%201%2C%20%E8%B4%9D%E5%B0%94%E6%A0%BC%E8%8E%B1%E5%BE%B7%2C%2011271%2C%20%E5%A1%9E%E5%B0%94%E7%BB%B4%E4%BA%9A" target="_blank" rel="noopener">[Google Map]</a></li></ul><h2 id="Travel-Maps"><a href="#Travel-Maps" class="headerlink" title="Travel Maps"></a>Travel Maps</h2><ul><li><a href="./yugoslavia/belgrade.jpeg">Serbia Belgrade</a></li><li><a href="./yugoslavia/tara.jpeg">Tara National Park</a></li><li><a href="./yugoslavia/Novi-Sad.jpeg">Novi Sad</a></li><li><a href="./yugoslavia/kotor.jpeg">Montenegro Kotor</a></li><li><a href="./yugoslavia/monasteries.jpeg">Monasteries collection</a></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="http://www.mafengwo.cn/gonglve/ziyouxing/147259.html" target="_blank" rel="noopener">去塞尔维亚自驾，你需要这篇指南</a></li><li><a href="https://club.autohome.com.cn/bbs/thread/45f1a39d4ba9abb4/67945365-1.html" target="_blank" rel="noopener">命途多舛，我曾叫“南斯拉夫”——塞尔维亚深度自驾之旅</a></li><li><a href="https://bbs.qyer.com/thread-2869536-1.html" target="_blank" rel="noopener">温暖人心的巴尔干之旅-塞尔维亚自驾游记（９天５城２万字：贝尔格莱德－泽蒙－诺维萨德－苏博蒂察－乌日策）</a></li><li><a href="https://you.autohome.com.cn/details/116959" target="_blank" rel="noopener">一日三城记—塞尔维亚自驾游的一天</a></li><li><a href="https://bbs.qyer.com/thread-2848098-1.html" target="_blank" rel="noopener">时光多么细致——你不知道的塞尔维亚（塞尔维亚黑山超深度超全面游记攻略）【完】</a></li><li><a href="http://www.zuzuche.com/article/art-746-22423.html" target="_blank" rel="noopener">塞尔维亚自驾旅游注意事项</a></li><li><a href="http://www.mafengwo.cn/i/8440858.html" target="_blank" rel="noopener">记录我的第一次异国自驾——1100公里跨越塞尔维亚四城（超实用自驾信息）</a></li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>9102秋招信息汇总</title>
      <link href="/schedule/2019_fall_never_give_up.html"/>
      <url>/schedule/2019_fall_never_give_up.html</url>
      
        <content type="html"><![CDATA[<p>Never give up…</p><a id="more"></a><h2 id="复习资料汇总"><a href="#复习资料汇总" class="headerlink" title="复习资料汇总"></a>复习资料汇总</h2><ul><li><a href="https://chenshawn.github.io/tags/Coding/" target="_blank" rel="noopener">春招刷过的题</a></li><li><a href="https://www.nowcoder.com/tutorial/95/92c2f259572344f69f64137a3cf10c4c" target="_blank" rel="noopener">牛客网算法面试宝典</a></li></ul><h2 id="状态汇总"><a href="#状态汇总" class="headerlink" title="状态汇总"></a>状态汇总</h2><p>[empty]</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>gossip-about-gfw</title>
      <link href="/schedule/gossip-about-gfw.html"/>
      <url>/schedule/gossip-about-gfw.html</url>
      
        <content type="html"><![CDATA[<p>Nothing here…</p><a id="more"></a><h2 id="Blocked"><a href="#Blocked" class="headerlink" title="Blocked"></a>Blocked</h2><p>Social media:</p><ul><li><a href="https://twitter.com/" target="_blank" rel="noopener">https://twitter.com/</a></li><li><a href="https://www.facebook.com/" target="_blank" rel="noopener">https://www.facebook.com/</a></li><li><a href="https://www.instagram.com/" target="_blank" rel="noopener">https://www.instagram.com/</a></li><li><a href="https://line.me/en/" target="_blank" rel="noopener">https://line.me/en/</a></li><li><a href="https://www.whatsapp.com/" target="_blank" rel="noopener">https://www.whatsapp.com/</a></li></ul><p>BBS and Forums:</p><ul><li><a href="https://www.reddit.com/" target="_blank" rel="noopener">https://www.reddit.com/</a></li><li><a href="https://gist.github.com/mine" target="_blank" rel="noopener">https://gist.github.com/mine</a></li><li><a href="https://www.quora.com/" target="_blank" rel="noopener">https://www.quora.com/</a></li></ul><p>Video:</p><ul><li><a href="https://www.nicovideo.jp/" target="_blank" rel="noopener">https://www.nicovideo.jp/</a></li><li><a href="https://www.youtube.com/" target="_blank" rel="noopener">https://www.youtube.com/</a></li><li><a href="http://www.pixiv.net/" target="_blank" rel="noopener">http://www.pixiv.net/</a></li><li><a href="https://www.pornhub.com/" target="_blank" rel="noopener">https://www.pornhub.com/</a></li></ul><p>Daily tools:</p><ul><li><a href="http://www.google.com.hk/" target="_blank" rel="noopener">http://www.google.com.hk/</a></li><li><a href="https://maps.google.com/" target="_blank" rel="noopener">https://maps.google.com/</a></li><li><a href="http://earth.google.com/" target="_blank" rel="noopener">http://earth.google.com/</a></li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>TODO list</title>
      <link href="/schedule/index.html"/>
      <url>/schedule/index.html</url>
      
        <content type="html"><![CDATA[<h2 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h2><ul><li><a href="/schedule/2019_fall_never_give_up.html">2019_fall_never_give_up</a></li><li><a href="/schedule/week-report.html">Week report (probably deprecated)</a></li><li><a href="/schedule/robust-rl.html">Improving the generalization of reinforcement learning in continuous control</a></li><li><a href="/schedule/gossip-about-gfw.html">Gossips</a></li></ul><h2 id="High-priority"><a href="#High-priority" class="headerlink" title="High priority"></a>High priority</h2><ul><li>写完论文（最高优</li><li>论文补实验</li><li>Bali trip planning（包车，Tulamben，etc</li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Review of robust reinforcement learning</title>
      <link href="/schedule/robust-rl.html"/>
      <url>/schedule/robust-rl.html</url>
      
        <content type="html"><![CDATA[<p>A review of robust and generalization of reinforcement learning<br><a id="more"></a></p><p>link: <a href="/schedule/index/images.tgz">images.tgz</a></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul><li>Silver D, Hubert T, Schrittwieser J, et al. Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.[J]. arXiv: Artificial Intelligence, 2017.</li><li>Kober J, Bagnell J A, Peters J, et al. Reinforcement learning in robotics: A survey[J]. The International Journal of Robotics Research, 2013, 32(11): 1238-1274.</li><li>Hu Y, Da Q, Zeng A, et al. Reinforcement Learning to Rank in E-Commerce Search Engine: Formalization, Analysis, and Application[C]. knowledge discovery and data mining, 2018: 368-377.</li><li>Zoph B, Vasudevan V K, Shlens J, et al. Learning Transferable Architectures for Scalable Image Recognition[C]. computer vision and pattern recognition, 2018: 8697-8710.</li><li>Bello I, Pham H, Le Q V, et al. Neural Combinatorial Optimization with Reinforcement Learning[C]. international conference on learning representations, 2017.</li><li>Henderson P A, Islam R, Pineau J, et al. Deep Reinforcement Learning that Matters[C]. national conference on artificial intelligence, 2018: 3207-3214.</li><li>Lanctot M, Zambaldi V, Gruslys A, et al. A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning[C]. neural information processing systems, 2017: 4190-4203.</li><li>Zhang A, Ballas N, Pineau J, et al. A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning.[J]. arXiv: Learning, 2018.</li><li>Zhang C, Bengio S, Hardt M, et al. Understanding deep learning requires rethinking generalization[C]. international conference on learning representations, 2017.</li><li>Kawaguchi K, Kaelbling L P, Bengio Y, et al. Generalization in Deep Learning[J]. arXiv: Machine Learning, 2017.</li><li>Madry A, Makelov A, Schmidt L, et al. Towards Deep Learning Models Resistant to Adversarial Attacks[C]. international conference on learning representations, 2018. </li></ul><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2. Preliminaries"></a>2. Preliminaries</h2><h3 id="2-1-Adversarial-learning"><a href="#2-1-Adversarial-learning" class="headerlink" title="2.1. Adversarial learning"></a>2.1. Adversarial learning</h3><script type="math/tex; mode=display">\min_{\theta}\max_{x'}\mathbb{E}_{x,y\sim{P}}[J(x',y;\theta)]\quad{} \texttt{s.t.}\ \|x-x'\|\leq{\epsilon}</script><p>其中$|.|$为任意满足定义的norm。特定条件下对抗训练的弱收敛性目前已经得到了理论证明：对于数据集中的所有样本，存在一个$\epsilon$使得其周围半径为$\epsilon$的norm ball中不存在对抗样本。相比于robust optimization，分布鲁棒性优化（distributional robust optimization）通过松弛样本背后的概率分布构造不确定性集合，以下两种优化目标互为对偶</p><script type="math/tex; mode=display">\min_{\theta}\max_{Q}\mathbb{E}_{x,y\sim{Q}}[J(x,y;\theta)]-\epsilon W_c(P,Q) \\\min_{\theta}\max_{x'}\mathbb{E}_{x,y\sim{P}}[J(x',y;\theta)-\epsilon c(x,x')]</script><p>其中$W_c(.,.)$为Wasserstein距离，$c(.,.): \mathbb{R}^{n} \times \mathbb{R}^{n} \rightarrow [0, + \infty)$表示Wasserstein距离定义中概率测度支撑集上的transportation cost，是一个严格凸函数。不难看出，distributional robust optimization相当于robust optimization的松弛版本，在保障可验证的对抗样本安全边界的前提下，往往可以得到比后者更好的训练稳定性与收敛性。</p><h3 id="2-2-RL-in-continuous-control"><a href="#2-2-RL-in-continuous-control" class="headerlink" title="2.2. RL in continuous control"></a>2.2. RL in continuous control</h3><p>强化学习的最终目标，是在控制agent与环境交互的过程中学习，从而最大化期望总收益。为了求解Markov随机过程中的收益最大化问题，最大期望总收益可以分解为Bellman equation的形式。另$V \in \mathbb{R}^{|\mathcal{S}|}$为价值函数，$r \in {\mathbb{R}^{|\mathcal{S}|}}$为奖励函数，$\mathcal{P}^{\pi} \in \mathbb{R}^{|\mathcal{S}|} \times \mathbb{R}^{|\mathcal{S}|}$表示在环境中执行策略$\pi$带来的转移概率，标准Bellman operator $\mathcal{T}$是$\ell_{\infty}$ norm下的$\gamma$-压缩映射：</p><script type="math/tex; mode=display">\mathcal{T}V:=r+\gamma\mathcal{P}^{\pi}V</script><p>由于Bellman方程是一个线性递推式，可以看出求解最优策略$\pi$与求解最有价值$V$是等价的。因此按照直接求解目标的不同，强化学习方法可以大致分为基于价值（value-based）、基于策略（policy-based）、以及二者的结合actor-critic三类。</p><h4 id="2-2-1-Value-based-RL"><a href="#2-2-1-Value-based-RL" class="headerlink" title="2.2.1. Value-based RL"></a>2.2.1. Value-based RL</h4><p>基于价值求解RL问题，最具代表性的算法是Q-learning，令 $Q(s,a):=r(s,a)+\mathbb{E}_{s’,a’\sim\pi}[\sum_{t=1}^{\infty}\gamma^{t}r(s’,a’)]$ 为状态-价值函数，Q-learning的迭代形式如下：</p><script type="math/tex; mode=display">\mathcal{T}Q(s,a) := \mathbb{E}_{s,a,s'\sim\pi} [r(s,a) + \gamma \max_{a'} Q(s',a')]</script><p>由于Q-learning实际学习的目标是当前的$Q$函数所代表的greedy策略，因而是一种off-policy的算法，即当前正在学习优化的策略可以与采样策略不同。大部分基于价值的DRL算法都沿用了Q-learning的框架，深度学习兴起之后，DQN开创性地使用神经网络表示$Q$函数，训练出的RL模型可以在Atari游戏任务上达到人类水平。后续的Dueling DQN、prioritized replay、Rainbow DQN、Ape-X等方法则从不同的角度进一步提升了DQN的性能。</p><h4 id="2-2-2-Policy-based-RL"><a href="#2-2-2-Policy-based-RL" class="headerlink" title="2.2.2. Policy-based RL"></a>2.2.2. Policy-based RL</h4><p>相较于value-based RL，policy-based RL方法在过去很长一段时间的研究中都没有受到足够的重视，其代表性方法策略梯度（policy gradient）长期被认为是一种方差极大难以收敛的算法。主要原因有有二：第一，策略梯度方法直接将期望总收益作为目标进行优化，使得该算法只能采用on-policy训练，这意味着策略梯度方法无法很好地应用于更复杂的RL问题中；第二，当与神经网络结合时，策略梯度训练过程中无法像value-based方法一样从理论上保证策略提升的单调性。</p><p>为了解决上述问题，CPI提出了一种在原策略分布的一个有限范围内进行策略迭代，得到了一种较保守的策略迭代方式，然而该方法仍无法解决on-policy训练的问题。相比之下，TRPO采用重要性采样（importance sampling）来解耦合模型的训练与采样过程，并理论证明了使用了importance sampling后策略提升的下界：</p><script type="math/tex; mode=display">J(\pi_{\theta}) - J(\pi')\geq \mathbb{E}_{\pi'}[\sum_{t=0}^{\infty} \gamma^{t} \frac{\pi_{\theta}(a_{t}|s_{t})}{\pi'(a_{t}|s_{t})}A(s_{t},a_{t})]-\frac{4\gamma\max_{s,a}A^{\pi}(s,a)}{(1-\gamma)^{2}}\mathbb{E}_{s\sim{d^{\pi'}}}[D_{KL}(\pi'||\pi_{\theta})]</script><p>其中$\pi_{\theta}$为当前优化的策略，$\pi’$为采样使用的策略，$J(\pi)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}\gamma^{t}r_{t}]$为策略$\pi$下的期望总收益，$A:\mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$为优势函数（advantage function）。基于该下界不难看出，在限定$D_{KL}(\pi’||\pi_{\theta})$范围内进行策略迭代可以保证策略的单调性提升，目标函数可以写作</p><script type="math/tex; mode=display">\max_{\theta}\mathbb{E}_{\pi'}[\sum_{t=0}^{\infty}\gamma^{t}\frac{\pi_{\theta}(a_{t}|s_{t})}{\pi'(a_{t}|s_{t})}A(s_{t},a_{t})] \quad{} \texttt{s.t.}\ D_{KL}(\pi'||\pi_{\theta})\leq\epsilon</script><p>为了求解限定KL范围内的策略提升问题，TRPO引入了自然梯度下降，通过共轭梯度法求解自然梯度，配合步长的指数线性搜索来保证迭代严格满足理论性质；ACKTR采用克罗内克积（Kronecker product）来高效地近似Fisher信息矩阵，从而降低了TRPO的复杂度；PPO提出了两种替代损失函数，采用TRPO的一阶近似来进一步保障训练的稳定与高效。</p><h4 id="2-2-3-Actor-critic"><a href="#2-2-3-Actor-critic" class="headerlink" title="2.2.3. Actor-critic"></a>2.2.3. Actor-critic</h4><blockquote><p>placeholder</p></blockquote><h3 id="2-3-Adversarial-RL"><a href="#2-3-Adversarial-RL" class="headerlink" title="2.3. Adversarial RL"></a>2.3. Adversarial RL</h3><ul><li>Pinto L, Davidson J, Sukthankar R, et al. Robust adversarial reinforcement learning[C]. international conference on machine learning, 2017: 2817-2826.</li><li>Tamar A, Glassner Y, Mannor S, et al. Optimizing the CVaR via sampling[C]. national conference on artificial intelligence, 2015: 2993-2999.</li><li>Tessler C, Efroni Y, Mannor S, et al. Action Robust Reinforcement Learning and Applications in Continuous Control[C]. international conference on machine learning, 2019: 6215-6224.</li><li>Abdullah M A, Ren H, Bouammar H, et al. Wasserstein Robust Reinforcement Learning.[J]. arXiv: Learning, 2019.</li><li>Mankowitz D J, Levine N, Jeong R, et al. Robust Reinforcement Learning for Continuous Control with Model Misspecification.[J]. arXiv: Learning, 2019.</li><li>Pattanaik A, Tang Z, Liu S, et al. Robust Deep Reinforcement Learning with Adversarial Attacks[J]. adaptive agents and multi-agents systems, 2018: 2040-2042.</li><li>Packer C, Gao K, Kos J, et al. Assessing Generalization in Deep Reinforcement Learning[J]. arXiv: Learning, 2018.</li><li>Rajeswaran A, Ghotra S, Ravindran B, et al. EPOpt: Learning Robust Neural Network Policies Using Model Ensembles[J]. arXiv: Learning, 2016.</li></ul><h2 id="3-思路推进"><a href="#3-思路推进" class="headerlink" title="3. 思路推进"></a>3. 思路推进</h2><ul><li>Summarize the drawback of existing works<ul><li>opponent modeling: training instability, as reported in the MARL paper</li><li>adversarial off-policy sampling: learning with a bad sampling policy may result in slow convergence and training instability</li><li>SL regularizers: lack of interpretability</li></ul></li><li>we show that incorporating a robustness-related regularizer in the value estimation can contribute to better robustness while maintain a good performance in training environments at the same time. (Propose the framework here) Why?<br>1) Does not need to change the environment during training. This makes the algoritm more applicable to many real world environments which has no access to the environment parameters;<br>2) Similar to the intuition of the generalization in SL, the optima with high curvatures on the loss surface often correspond to bad generalization ability of the models, because a small pertubation in the parameter space can result in severe performance degration. An appropriately defined regularizer can prevent the model from overfitting in sharp minima. </li><li>Based on the intuition we propose to use a regularizer in the Bellman policy iteration scheme. The regularizer should punish the states that has high value estimations but are vulnerable to small perturbations. </li><li>Define each concept mathematically:<ul><li>Define Q_adv first, with perturbations subjected to a norm constraint. Thus Q_adv(s, a) can be represented implicitly using adversarial attack of Q(s, a) w.r.t. s</li><li>Define \pi^<em> and \pi_adv^</em> as the optimal improvement of the current policy \pi in a infinitely small norm ball</li><li>Compute the closed-form solution for the improvement of \pi under Q (<b style="color:red;">probably using a Lemma???</b>)</li><li>Define \phi(\pi) as the discrepancy between the adversarial optimal policy and the optimal policy in the \epsilon norm ball, with the discrepancy measured by KL divergence</li></ul></li><li>Propose the algorithm under the Q-learning framework</li><li>Assumptions and proof for the convergence of the Q-learning algorithm under a tabular setting (<b style="color:red;">probably using a Theorem???</b>)</li><li>Propose the algorithm under the TD3 framework</li><li>Discussion about the intuition behind the approach (<b style="color:red;">better with figures and figures and figures…</b>)</li></ul><p>References:</p><ul><li>Azar M G, Gómez V, Kappen H J. Dynamic policy programming[J]. Journal of Machine Learning Research, 2012, 13(Nov): 3207-3245.</li><li>Geist M, Scherrer B, Pietquin O. A Theory of Regularized Markov Decision Processes[J]. arXiv preprint arXiv:1901.11275, 2019.</li></ul><h2 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4. 实验结果"></a>4. 实验结果</h2><p>训练过程：</p><ul><li>正则化函数不会对训练过程与结果带来非常显著的影响，换言之，正则函数对模型的正则化提升不需要以降低模型在训练环境上的表现为代价</li><li>我们发现较小的$\epsilon$就已经足以达到满意的训练效果，虽然更大的$\epsilon$可以为模型带来更强的正则化，但过大的$\epsilon$不一定总是能够带来正面的实验效果，在一些复杂的实验环境，如Humanoid-v2下，过大的$\epsilon$可能会为训练带来带来更多的不稳定性</li><li><b style="color:red">should be waiting for the new emsemble experimental results</b>在多个不同的环境中集成训练往往会带来训练的不稳定，且会降低默认训练环境下的模型表现，这种现象在复杂环境中体现的更加明显，相比于集成训练，我们的方法可以更好地保证训练的稳定性与较好的训练结果</li></ul><p>测试环境：</p><ul><li>在所有的实验环境上，我们提出的SIR-TD3无论是在NT还是HV设定下都表现出比原本的TD3算法更强的泛化能力，尤其当测试环境与训练环境相差较大时，正则化函数为模型带来的泛化能力提升也更加明显</li><li>使用更大的扰动区间$\epsilon$有时可以为模型带来更好的正则化效果，但$\epsilon$大小与模型泛化能力之间的关系并不总是单调提升的。总体来说，我们发现较小的$\epsilon$就已经足以带来较好的正则化效果。</li></ul><p>Empirical evidence:</p><ul><li>在LunarLanderContinuous-v2环境上，我们发现模型学习到的策略更加具有可解释性。在该环境下，飞船距离着陆点较远时迅速下降，靠近着陆点时用较大的喷射力度稳定平衡就可以获得较高的reward，但这种策略同时也会在重力加速度发生变化时使得飞船更容易坠毁，遗憾的是，大部分无模型强化学习最终都会收敛到这种对环境变化缺乏泛化能力的策略。从图中可以看出，在TD3算法中加入我们提出的正则化函数后，模型前期的下降速度会比原始版本的TD3算法要慢，也就是所，在正则化函数的效果下，模型确实学习到了更加符合人类直觉的、更具有可解释性的、具有更强泛化能力的策略</li></ul><p><img src="https://chenshawn.github.io/schedule/index/interpretable.png" alt=""></p><h3 id="4-1-Training"><a href="#4-1-Training" class="headerlink" title="4.1. Training"></a>4.1. Training</h3><p>以下实验结果小数点后四舍五入只保留整数部分，符号±后面的数字是half deviation</p><div class="table-container"><table><thead><tr><th>Algo</th><th style="text-align:center">Lunar</th><th style="text-align:center">Walker2d</th><th style="text-align:center">HalfCheetah</th><th style="text-align:center">Hopper</th><th style="text-align:center">Ant</th><th style="text-align:center">Humanoid</th></tr></thead><tbody><tr><td>TD3</td><td style="text-align:center">282±8</td><td style="text-align:center">5089±65</td><td style="text-align:center">11581±62</td><td style="text-align:center">3529±163</td><td style="text-align:center">3541±78</td><td style="text-align:center">6259±251</td><td></td></tr><tr><td>SIR-TD3</td><td style="text-align:center">288±9</td><td style="text-align:center">4007±12</td><td style="text-align:center">11189±57</td><td style="text-align:center">3377±31</td><td style="text-align:center">5806±48</td><td style="text-align:center">6489±12</td><td></td></tr><tr><td>Ensemble</td><td style="text-align:center">275±8</td><td style="text-align:center">4759±480</td><td style="text-align:center">9282±77</td><td style="text-align:center">3518±188</td><td style="text-align:center">5653±172</td><td style="text-align:center">6548±417</td><td></td></tr><tr><td>Ensemble+SIR</td><td style="text-align:center">276±10</td><td style="text-align:center">4578±19</td><td style="text-align:center">11639±60</td><td style="text-align:center">3534±22</td><td style="text-align:center">5923±40</td><td style="text-align:center">5976±280</td></tr><tr><td>SAC</td><td style="text-align:center">284±9</td><td style="text-align:center">5496±51</td><td style="text-align:center">16701±47</td><td style="text-align:center">3754±6</td><td style="text-align:center">6875±414</td><td style="text-align:center">6486±4</td></tr><tr><td>SIR-SAC</td><td style="text-align:center">286±9</td><td style="text-align:center">5385±16</td><td style="text-align:center">16154±73</td><td style="text-align:center">3550±1</td><td style="text-align:center">7171±31</td><td style="text-align:center">5997±5 </td></tr></tbody></table></div><p>Note that it is unfair to directly compare the performance between the models with ensemble training and those without ensemble training, since the formers have never experienced any of the testing environments except the unmodified one used for training simulations.</p><p>Conclusions:</p><ul><li>The models with SIR have much lower variance than those without an SIR. The phenomenon is more significant in complex environments such as Walker2d-v3, Hopper-v2, and Humanoid-v2, where the SIR-TD3 models have lower variance in magnitude than their TD3 counterparts.</li><li>Compared with our proposed SIR models, ensemble training can significantly increase the model variance in training environments. The result can be concluded by comparing the performance standard deviation of the TD3 models and the Ensemble models, where the models with ensemble training have higher variance in magnitude than the TD3 models. Similarly, in most cases, the SIR-TD3 models with ensemble training also show higher variance of performance than the SIR-TD3 models.</li></ul><p>This implies that our SIR is a better solution for RL generalization than the ensemble training especially when model stability is required.</p><h3 id="4-2-Integral-area-under-curves"><a href="#4-2-Integral-area-under-curves" class="headerlink" title="4.2. Integral area under curves"></a>4.2. Integral area under curves</h3><div class="table-container"><table><thead><tr><th>epsilon</th><th style="text-align:center">Lunar</th><th style="text-align:center">Walker2d</th><th style="text-align:center">HalfCheetah</th><th style="text-align:center">Hopper</th><th style="text-align:center">Ant</th><th style="text-align:center">Humanoid</th></tr></thead><tbody><tr><td>0.00 NT</td><td style="text-align:center">29.76</td><td style="text-align:center">2781.17</td><td style="text-align:center">2487.13</td><td style="text-align:center">4389.07</td><td style="text-align:center">1708.02</td><td style="text-align:center"><strong>3136.36</strong></td></tr><tr><td>0.05 NT</td><td style="text-align:center">45.69</td><td style="text-align:center"><strong>3541.74</strong></td><td style="text-align:center"><strong>2695.17</strong></td><td style="text-align:center">4458.08</td><td style="text-align:center"><strong>2551.94</strong></td><td style="text-align:center">3015.09</td></tr><tr><td>0.10 NT</td><td style="text-align:center"><strong>54.33</strong></td><td style="text-align:center">3248.23</td><td style="text-align:center">2585.83</td><td style="text-align:center"><strong>4709.54</strong></td><td style="text-align:center">1778.62</td><td style="text-align:center">3048.04</td></tr><tr><td>0.00 HV</td><td style="text-align:center">245.56</td><td style="text-align:center">968.47</td><td style="text-align:center">8705.90</td><td style="text-align:center">920.92</td><td style="text-align:center">3556.13</td><td style="text-align:center">4389.07</td><td></td></tr><tr><td>0.05 HV</td><td style="text-align:center">255.77</td><td style="text-align:center"><strong>1797.55</strong></td><td style="text-align:center"><strong>8880.63</strong></td><td style="text-align:center"><strong>1096.20</strong></td><td style="text-align:center"><strong>5495.18</strong></td><td style="text-align:center">4458.08</td><td></td></tr><tr><td>0.10 HV</td><td style="text-align:center"><strong>264.84</strong></td><td style="text-align:center">1638.21</td><td style="text-align:center">8751.48</td><td style="text-align:center">1005.73</td><td style="text-align:center">3846.90</td><td style="text-align:center"><strong>4709.54</strong></td><td></td></tr></tbody></table></div><h3 id="4-3-Comparison-for-ensemble-training-methods"><a href="#4-3-Comparison-for-ensemble-training-methods" class="headerlink" title="4.3. Comparison for ensemble training methods"></a>4.3. Comparison for ensemble training methods</h3><div class="table-container"><table><thead><tr><th>Algo</th><th style="text-align:center">Lunar</th><th style="text-align:center">Walker2d</th><th style="text-align:center">HalfCheetah</th><th style="text-align:center">Hopper</th><th style="text-align:center">Ant</th><th style="text-align:center">Humanoid</th></tr></thead><tbody><tr><td>Ens HV</td><td style="text-align:center">270.51</td><td style="text-align:center">4526.66</td><td style="text-align:center">8979.61</td><td style="text-align:center">3070.99</td><td style="text-align:center">5511.94</td><td style="text-align:center"><strong>6290.51</strong></td><td></td></tr><tr><td>Ens+SIR HV</td><td style="text-align:center"><strong>271.16</strong></td><td style="text-align:center"><strong>4564.77</strong></td><td style="text-align:center"><strong>11322.53</strong></td><td style="text-align:center"><strong>3472.28</strong></td><td style="text-align:center"><strong>5701.74</strong></td><td style="text-align:center">5793.81</td><td></td></tr><tr><td>Ens NT</td><td style="text-align:center"><strong>28.95</strong></td><td style="text-align:center">1616.33</td><td style="text-align:center">1245.57</td><td style="text-align:center">449.80</td><td style="text-align:center">709.64</td><td style="text-align:center">1764.21</td><td></td></tr><tr><td>Ens+SIR NT</td><td style="text-align:center">17.04</td><td style="text-align:center"><strong>1810.70</strong></td><td style="text-align:center"><strong>1267.14</strong></td><td style="text-align:center"><strong>654.71</strong></td><td style="text-align:center"><strong>2289.40</strong></td><td style="text-align:center"><strong>2992.88</strong></td><td></td></tr></tbody></table></div><h3 id="4-4-Standard-variations-on-10-seeds"><a href="#4-4-Standard-variations-on-10-seeds" class="headerlink" title="4.4. Standard variations on 10 seeds"></a>4.4. Standard variations on 10 seeds</h3><p><img src="https://chenshawn.github.io/schedule/index/seed_vars.png" alt=""></p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Week Summaries</title>
      <link href="/schedule/week-report.html"/>
      <url>/schedule/week-report.html</url>
      
        <content type="html"><![CDATA[<h2 id="20190904"><a href="#20190904" class="headerlink" title="20190904"></a>20190904</h2><a id="more"></a><h3 id="Assessing-Generalization-in-Deep-Reinforcement-Learning"><a href="#Assessing-Generalization-in-Deep-Reinforcement-Learning" class="headerlink" title="Assessing Generalization in Deep Reinforcement Learning"></a><a href="https://bair.berkeley.edu/blog/2019/03/18/rl-generalization/" target="_blank" rel="noopener">Assessing Generalization in Deep Reinforcement Learning</a></h3><blockquote><p>Systematic empirical evaluation shows that vanilla deep RL algorithms generalize better than specialized deep RL algorithms designed specifically for generalization. In other words, simply training on varied environments is so far the most effective strategy for generalization.</p></blockquote><p>作者对各种深度强化学习的泛化性做了广泛调研，发现在变化的环境中训练agent是目前为止最有效的加强模型泛化性方法</p><h3 id="Quantifying-Generalization-in-Reinforcement-Learning"><a href="#Quantifying-Generalization-in-Reinforcement-Learning" class="headerlink" title="Quantifying Generalization in Reinforcement Learning"></a><a href="https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/" target="_blank" rel="noopener">Quantifying Generalization in Reinforcement Learning</a></h3><p>OpenAI的文章，新release出一个叫做CoinRun的benchmark，与之前的环境最大的区别在于，这个游戏的环境是实时生成的，i.e. agent不会两次遇到相同的环境。因此该环境要求agent不能通过死记硬背学习策略，对DRL的generalization ability提出更高的挑战</p><p><img src="https://d4mucfpksywv.cloudfront.net/research-covers/coinrun/Standard_Tile2.gif" width="45%"></p><p>作者在3层的CNN上采用PPO算法训练了256M数量级的timesteps，每个trajectory平均长度100，部分agent在封闭环境训练，部分在开放环境中训练，实验表明所有的agent都有不同程度的过拟合，但与预期一致，开放环境中训练得到的agent测试得分比封闭环境的agent强很多</p><p>此外IMPALA的CNN结构的泛化性能远强于其他的CNN结构（可能说明state的abstract embedding重要性）</p><p><img src="https://openai.com/content/images/2018/12/nat-vs-imp_generalization-1.svg" width="75%"></p><p>其他的一些结论：</p><blockquote><ul><li>Dropout and L2 regularization: Both noticeably reduce the generalization gap, though L2 regularization has a bigger impact.</li><li>Data augmentation (modified Cutout) and batch normalization: Both data augmentation and batch normalization significantly improve generalization.</li><li>Environmental stochasticity: Training with stochasticity improves generalization to a greater extent than any of the previously mentioned techniques (see the paper for details).</li></ul></blockquote><p><img src="https://openai.com/content/images/2018/12/misc_gen_500.svg" width="75%"></p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
